{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f7703a-8aa3-47c9-8461-670f297d9fb1",
   "metadata": {},
   "source": [
    "# Amazon-reviews predictions with Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e6d449-425b-4222-967e-d082d7692b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1715326141961_1103\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1715326141961_1103)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "sc: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@9d0f0fa\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val sc = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73583dd-e37b-4611-8f50-484cb7ba8bed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read data and transform to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61608eac-7522-4ac5-8f13-6ff8643f1507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K: Int = 75\n",
       "file_path_stopwords: String = ./data/stopwords.txt\n",
       "file_path_reviews: String = hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\n",
       "tokenizePattern: String = [^a-zA-Z<>^|]+\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val K = 75\n",
    "val file_path_stopwords = \"./data/stopwords.txt\"\n",
    "val file_path_reviews = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "// val file_path_reviews = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "\n",
    "val tokenizePattern = \"[^a-zA-Z<>^|]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ededeb-ae72-4a27-83a6-f3a17f4b5a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9.168583631515503 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val df = sc.read.json(file_path_reviews).select(\"category\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28fa0346-2df6-4f93-8577-13adf01fe6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source.fromFile\n",
       "stopWords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs, but, by, c, came, camera, can, cannot, cant, car, case, cause, causes, ...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source.fromFile\n",
    "\n",
    "val stopWords = fromFile(file_path_stopwords).getLines.toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195030e1-bb58-4186-b616-54d4767b8f20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "2d04b92f-8975-4983-a0d1-78eaa9a3c915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io.PrintWriter\n",
       "import org.apache.spark.rdd.RDD\n",
       "import scala.collection.immutable.TreeSet\n",
       "writeRDDToFile: (rdd: org.apache.spark.rdd.RDD[(String, List[(String, Double)])], filePath: String)Unit\n"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.collection.immutable.TreeSet\n",
    "\n",
    "def writeRDDToFile(rdd: RDD[(String, List[(String, Double)])], filePath: String) = {\n",
    "    var mergedTerms = TreeSet[String]()\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    val collectedData = rdd.sortByKey().collect()\n",
    "\n",
    "    for ((category, topk) <- collectedData) {\n",
    "        val topK = topk.map({ case (term, chiSquared) => {\n",
    "            mergedTerms += term\n",
    "            s\"$term:$chiSquared\"\n",
    "        }}).mkString(\" \")\n",
    "        writer.println(f\"<$category> $topk\")\n",
    "    }\n",
    "    \n",
    "    writer.print(mergedTerms.mkString(\" \"))\n",
    "    \n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "82aec3a0-0f39-44a4-a064-f60058d30e19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{Row, Dataset}\n",
       "import scala.collection.immutable.TreeSet\n",
       "import java.io.PrintWriter\n",
       "writeDFToFile: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], filePath: String)Unit\n"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Row, Dataset}\n",
    "import scala.collection.immutable.TreeSet\n",
    "import java.io.PrintWriter\n",
    "\n",
    "def writeDFToFile(df: Dataset[Row], filePath: String) = {\n",
    "    var mergedTerms = TreeSet[String]()\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    val collectedData = df.collect()\n",
    "\n",
    "    for (row <- collectedData) {\n",
    "      val category = row.getString(0)\n",
    "      val topk = row.getAs [Seq[Seq[String]]](\"topk\").map({\n",
    "        case Seq(token, chiSquared) => { \n",
    "            mergedTerms += token\n",
    "            s\"$token:$chiSquared\" \n",
    "        }\n",
    "      }).mkString(\" \")\n",
    "\n",
    "      writer.println(f\"<$category> $topk\")\n",
    "    }\n",
    "    writer.print(mergedTerms.mkString(\" \"))\n",
    "    \n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a17fb-63a4-46e0-9748-e37714fb5364",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Calculate Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d4d14419-d35e-4d0f-abd0-5c546e51d7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[1045] at rdd at <console>:387\n"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "f5376711-f0d9-45df-8ad7-08d8659f759f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3.3981635570526123 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "counts: scala.collection.Map[String,Long] = Map(Patio_Lawn_and_Garde -> 994, Movies_and_TV -> 4607, Electronic -> 7825, Office_Product -> 1243, Tools_and_Home_Improvement -> 1926, Kindle_Store -> 3205, Home_and_Kitche -> 4254, Digital_Music -> 836, Automotive -> 1374, Grocery_and_Gourmet_Food -> 1297, Baby -> 916, Book -> 22507, Clothing_Shoes_and_Jewelry -> 5749, Toys_and_Game -> 2253, Health_and_Personal_Care -> 2982, Sports_and_Outdoor -> 3269, Beauty -> 2023, CDs_and_Vinyl -> 3749, Musical_Instrument -> 500, Cell_Phones_and_Accessorie -> 3447, Apps_for_Android -> 2638, Pet_Supplie -> 1235)\n",
       "N: Long = 78829\n"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val counts = df.rdd.map(row => (row.getString(0), 1)).countByKey()\n",
    "val N = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "e902d564-b646-40cd-a72d-851b252cbdf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 3.675842046737671 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countsAsMap: scala.collection.immutable.Map[String,Long] = Map(Patio_Lawn_and_Garde -> 994, Movies_and_TV -> 4607, Electronic -> 7825, Office_Product -> 1243, Tools_and_Home_Improvement -> 1926, Kindle_Store -> 3205, Home_and_Kitche -> 4254, Digital_Music -> 836, Automotive -> 1374, Grocery_and_Gourmet_Food -> 1297, Baby -> 916, Book -> 22507, Clothing_Shoes_and_Jewelry -> 5749, Toys_and_Game -> 2253, Health_and_Personal_Care -> 2982, Sports_and_Outdoor -> 3269, Beauty -> 2023, CDs_and_Vinyl -> 3749, Musical_Instrument -> 500, Cell_Phones_and_Accessorie -> 3447, Apps_for_Android -> 2638, Pet_Supplie -> 1235)\n"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val countsAsMap = counts.toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "93e4b94c-ce28-4aab-94cd-55be2193dbd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 13.75816035270691 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "preprocessedRDD: org.apache.spark.rdd.RDD[(String, Array[String])] = MapPartitionsRDD[1115] at map at <console>:424\n",
       "termCategoryCounts: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[1119] at map at <console>:430\n",
       "chiSquaredValues: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[1121] at flatMapValues at <console>:434\n",
       "topTermsPerCategory: org.apache.spark.rdd.RDD[(String, List[(String, Double)])] = MapPartitionsRDD[1124] at mapValues at <console>:449\n"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val preprocessedRDD = rdd\n",
    "    .map(row => (row.getString(0), row.getString(1).toLowerCase().split(tokenizePattern).distinct))\n",
    "    .map(row => (row._1, row._2.filter(w => w.length > 1 && !stopWords.contains(w))))\n",
    "\n",
    "val termCategoryCounts = preprocessedRDD\n",
    "    .flatMapValues(terms => terms)\n",
    "    .map({ case (category, term) => ((category, term), 1) })\n",
    "    .reduceByKey(_ + _)\n",
    "    .map({ case ((category, term), count) => (term, (category, count)) })\n",
    "\n",
    "val chiSquaredValues = termCategoryCounts\n",
    "    .groupByKey()\n",
    "    .flatMapValues({ categoryCounts =>\n",
    "        val n_t = categoryCounts.map(row => row._2).sum\n",
    "        categoryCounts.map({ case (category, count) =>\n",
    "            val A = count\n",
    "            val B = n_t - A\n",
    "            val C = countsAsMap(category) - A\n",
    "            val D = N - A - B - C\n",
    "            val chiSquared = (N * math.pow((A * D) - (B * C), 2)) / ((A + B) * (A + C) * (B + D) * (C + D))\n",
    "            (category, chiSquared)\n",
    "        })\n",
    "      })\n",
    "   \n",
    "val topTermsPerCategory = chiSquaredValues\n",
    "    .map({ case (term, (category, chiSquared)) => (category, (term, chiSquared)) })\n",
    "    .groupByKey()\n",
    "    .mapValues(_.toList.sortBy(-_._2).take(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "abed58fc-7e32-4393-bb2a-c4cd009376db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 31.581246852874756 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "writeRDDToFile(topTermsPerCategory, \"./output_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43300955-1cd7-4770-b6c9-e4f6f3263fdc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Second approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55802d0a-82a9-48c0-b863-b96eb1e212cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39.51532602310181 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "filteredCategoryTerm: org.apache.spark.rdd.RDD[((String, String), Int)] = MapPartitionsRDD[83] at flatMap at <console>:45\n",
       "countTerms: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[85] at reduceByKey at <console>:49\n",
       "countCategoryTerm: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[86] at reduceByKey at <console>:52\n",
       "joinedCategoryTerm: org.apache.spark.rdd.RDD[(String, ((String, Int), Int))] = MapPartitionsRDD[90] at join at <console>:56\n",
       "chiSquaredTermCategory: org.apache.spark.rdd.RDD[(String, List[(String, Double)])] = ShuffledRDD[96] at sortByKey at <console>:70\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val filteredCategoryTerm = rdd\n",
    "    .map(row => (row.getString(0), row.getString(1).toLowerCase().split(tokenizePattern).distinct))\n",
    "    .map(row => (row._1, row._2.filter(w => w.length > 1 && !stopWords.contains(w))))\n",
    "    .flatMap(row => row._2.map(term => ((row._1, term), 1)))\n",
    "\n",
    "val countTerms = filteredCategoryTerm\n",
    "    .map(row => (row._1._2, 1))\n",
    "    .reduceByKey(_ + _)\n",
    "\n",
    "val countCategoryTerm = filteredCategoryTerm\n",
    "    .reduceByKey(_ + _)\n",
    "\n",
    "val joinedCategoryTerm = countCategoryTerm\n",
    "    .map(row => (row._1._2, (row._1._1, row._2)))\n",
    "    .join(countTerms)\n",
    "\n",
    "val chiSquaredTermCategory = joinedCategoryTerm\n",
    "    .map(row => {    \n",
    "        val A = row._2._1._2\n",
    "        val B = row._2._2 - A\n",
    "        val C = countsAsMap(row._2._1._1) - A\n",
    "        val D = N - A - B - C\n",
    "    \n",
    "        val chiSquared = (N * math.pow((A * D) - (B * C), 2)) / ((A + B) * (A + C) * (B + D) * (C + D))\n",
    "        (row._2._1._1, (row._1, chiSquared))\n",
    "    })\n",
    "    .groupByKey()\n",
    "    .map(row => (row._1, row._2.toList.sortBy(x => -x._2).take(K)))\n",
    "    .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7d2cc8a-8125-42c0-abb9-b8abae492eff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.9137146472930908 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mergedDict: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[100] at distinct at <console>:33\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val mergedDict = chiSquaredTermCategory.flatMap(row => row._2.map(term => term._1)).distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d361bcf8-edd5-4189-bae8-87c77d778d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.6810376644134521 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[101] at map at <console>:33\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val result = chiSquaredTermCategory.map(row => {\n",
    "    val key = row._1\n",
    "    val values = row._2.map { case (str, num) => s\"$str:$num\" }.mkString(\" \")\n",
    "    s\"<$key> $values\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebb670bb-04cc-45b3-964c-209444a6a06a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.0665204524993896 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "writeRDDToFile(result, mergedDict.sortBy(x => x).reduce(_ + \" \" + _), \"./output2_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ab42d-f2e5-4194-8d2f-5194f67a1b7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets/DataFrames: Spark ML and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74ef30-43da-42a8-909f-801d904cdffe",
   "metadata": {},
   "source": [
    "First, create all the necessary transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ecf763-118c-4101-82ba-ed8c66acade4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{StringIndexer, RegexTokenizer, StopWordsRemover}\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
       "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_1e5b74a2e8f4\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_4987373b4125, minTokenLength=2, gaps=true, pattern=[^a-zA-Z<>^|]+, toLowercase=true\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = StopWordsRemover: uid=stopWords_8a1a16d915c0, numStopWords=596, locale=en_US, caseSensitive=false\n",
       "countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_892c9f0924f7\n",
       "hashingTF: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_73299ee31950, binary=false, nu...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{StringIndexer, RegexTokenizer, StopWordsRemover}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "    .setInputCol(\"category\")\n",
    "    .setOutputCol(\"category_index\")\n",
    "\n",
    "val tokenizer = new RegexTokenizer()\n",
    "    .setInputCol(\"reviewText\")\n",
    "    .setOutputCol(\"raw_terms\")\n",
    "    .setMinTokenLength(2)\n",
    "    .setPattern(tokenizePattern)\n",
    "    .setToLowercase(true)\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"terms\")\n",
    "    .setStopWords(stopWords)\n",
    "\n",
    "val countVectorizer = new CountVectorizer()\n",
    "    .setInputCol(remover.getOutputCol)\n",
    "    .setOutputCol(\"raw_features\")\n",
    "    .setMinDF(1)\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"raw_features\")\n",
    "\n",
    "// Decide which frequencyCounter you want hashingTF vs cvModel?\n",
    "val idf = new IDF()\n",
    "    .setInputCol(countVectorizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35ba46-fd3a-45ba-b920-3cd4f6612b09",
   "metadata": {},
   "source": [
    "Afterward, we create the Chi^2-Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4fdaab-8640-402d-b1f6-84d68a89dc10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{ChiSqSelector, ChiSqSelectorModel}\n",
       "selector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_dde97a3c2365\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{ChiSqSelector, ChiSqSelectorModel}\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    "  .setNumTopFeatures(2000)\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(\"category_index\")\n",
    "  .setOutputCol(\"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1e633-8635-4bae-962e-d894a3d77aaf",
   "metadata": {},
   "source": [
    "Lastly, we create the pipeline to execute all the transformers and select the top K features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37fc437e-d8f4-48e7-ba40-c685356e7230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_5c607bf29a32\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover, countVectorizer, idf, indexer, selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694186d-c794-405c-a19a-6c222d4bb77b",
   "metadata": {
    "tags": []
   },
   "source": [
    "After creation of the pipeline, we can now fit it to our data, we want to transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4126e42f-ce5a-494f-88c7-10849cad7c72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50.41893196105957 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_5c607bf29a32\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52f4c09e-6668-47b6-9984-eec6719af35f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocabulary: Array[String] = Array(great, good, love, time, work, recommend, back, easy, make, bought, made, find, buy, price, put, reading, quality, people, works, quot, years, nice, characters, long, series, lot, found, author, day, bit, feel, makes, thing, perfect, fit, end, set, loved, things, thought, music, small, hard, give, year, world, size, worth, pretty, times, sound, written, light, real, big, amazon, part, bad, highly, money, excellent, purchased, happy, high, enjoyed, problem, family, interesting, wanted, character, job, review, purchase, man, watch, days, enjoy, place, home, stars, short, writing, play, cover, top, fan, full, fine, color, side, order, wonderful, amazing, point, fact, reviews, ordered, stories, favorite, easily, needed, battery, screen, water, dvd, beautifu...\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocabulary = model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary\n",
    "val selectedFeatures = model.stages.last.asInstanceOf[ChiSqSelectorModel].selectedFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f908c-5e7b-4a83-9294-c659bf2cd35e",
   "metadata": {},
   "source": [
    "Last but not least, we need to transform our data and display the format, which could be used going forward to the text classification task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d27ba7c-3c36-4dff-b9ac-cfb5e22bad1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rescaledData: org.apache.spark.sql.DataFrame = [category: string, selectedFeatures: vector]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rescaledData = model.transform(df).select(\"category\", \"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0151dbf-ead3-45eb-9073-e2c78bee88cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.2329561710357666 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.SparseVector\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.types._\n",
       "import org.apache.spark.sql.{functions=>F}\n",
       "sparseVectorToMap: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4594/1143777704@2235422f,MapType(IntegerType,DoubleType,false),List(Some(class[value[0]: vector])),Some(class[value[0]: map<int,double>]),None,true,true)\n",
       "indexToToken: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4599/148926824@5e747dce,StringType,List(Some(class[value[0]: int])),Some(class[value[0]: string]),None,true,true)\n",
       "topK: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, topk: array<array<string>>]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import org.apache.spark.ml.linalg.{SparseVector}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.{functions => F}\n",
    "\n",
    "// Define UDFs\n",
    "val sparseVectorToMap = udf((v: SparseVector) => v.indices.zip(v.values).toMap)\n",
    "val indexToToken = udf((i: Int) => vocabulary(selectedFeatures(i)))\n",
    "\n",
    "val topK = rescaledData\n",
    "    .select($\"category\", explode(sparseVectorToMap($\"selectedFeatures\")))\n",
    "    .select($\"category\", $\"key\".as(\"term\"), $\"value\".as(\"chi_squared\"))\n",
    "    .groupBy(\"category\", \"term\")\n",
    "    .agg(mean(\"chi_squared\").as(\"chi_squared\"))\n",
    "    .withColumn(\"term\", indexToToken(col(\"term\")))\n",
    "    .orderBy(desc(\"chi_squared\"), asc(\"term\"))\n",
    "    .withColumn(\"token_chisquared\", array(col(\"term\"), col(\"chi_squared\")))\n",
    "    .groupBy(\"category\")\n",
    "    .agg(slice(collect_list(\"token_chisquared\"), 1, K).as(\"topk\"))\n",
    "    .sort(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd7aa3-cb67-4d4d-8b01-4cdc2b2b9ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "writeDFToFile(topK, \"./output_ds.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bcf444-c1a7-4f67-9e8d-d2c408feb2e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "266313ae-469c-4c4e-97bd-baa311c0ea5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seed: Int = 12041500\n",
       "percentage_of_dataset: Double = 0.1\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 12041500\n",
    "val percentage_of_dataset = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bf0e4-e657-4a8b-aeca-3e9482370d29",
   "metadata": {},
   "source": [
    "The next line of code is optional and can be used to down sample the dataframe to make model training faster, in case of high load on the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88b2a8e8-5d60-4d6b-97ec-fe302f325a28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampledDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampledDF = df.sample(withReplacement = false, fraction = percentage_of_dataset, seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f02eb1-64e4-482f-8755-f27ce6e8186c",
   "metadata": {},
   "source": [
    "First, we split our data into training- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d145d3bc-45ca-451f-b9da-83c404a9bc9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = sampledDF.randomSplit(Array(0.8, 0.2), seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94727373-bffa-43c9-94cb-7510264f5949",
   "metadata": {},
   "source": [
    "Additionally, we create a Normalizer for our selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdf56e7d-c5b7-417b-9141-49520e3a2b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Normalizer\n",
       "normalizer: org.apache.spark.ml.feature.Normalizer = Normalizer: uid=normalizer_45b3a1be1511, p=2.0\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val normalizer = new Normalizer()\n",
    "  .setInputCol(selector.getOutputCol)\n",
    "  .setOutputCol(\"normFeatures\")\n",
    "  .setP(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da936a-af29-4259-85e6-2746fbcc7e47",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then, we create the classifier. In our case we use a Linear Vector Machine. However, because we deal with a multiclass problem, we wrap it in a OneVsRest-classifier to bypass the limitation of Linear Vector Machines, which can only work with binary problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa503157-648c-4263-a8f9-cdd0b9eb4fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
       "lsvc: org.apache.spark.ml.classification.LinearSVC = linearsvc_4c0a5d2d727b\n",
       "classifier: org.apache.spark.ml.classification.OneVsRest = oneVsRest_b02e036fc060\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
    "\n",
    "val lsvc = new LinearSVC()\n",
    "    .setFeaturesCol(normalizer.getOutputCol)\n",
    "    .setLabelCol(indexer.getOutputCol)\n",
    "\n",
    "val classifier = new OneVsRest()\n",
    "    .setClassifier(lsvc)\n",
    "    .setFeaturesCol(normalizer.getOutputCol)\n",
    "    .setLabelCol(indexer.getOutputCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d8dbc-1ac1-4e43-aa75-adf462a74510",
   "metadata": {},
   "source": [
    "After, setting all things up, we create the pipeline, which uses all the prior created transformers and fits the transformed data to our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35d2f6db-c9f0-4212-b65f-32cc974bd99f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_ff8d8a911179\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover, countVectorizer, idf, indexer, selector, normalizer, classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a1348-9a02-4adc-8a53-058a5b475c29",
   "metadata": {},
   "source": [
    "Now, we only need to create our MulticlassClassificationEvaluator and ParamGridBuilder, which we use for hyperparameter tuning and evaluation of our model. We evaluate our model based on the F1-Score and our hyperparameter tuning happens based on the following params:\n",
    "- Compare chi square overall top 2000 filtered features with another, heavier filtering with much less dimensionality\n",
    "- Compare different SVM settings by \n",
    "    - varying the regularization parameter (choose 3 different values), \n",
    "    - standardization of training features (2 values),\n",
    "    - and maximum number of iterations (2 values).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b6330f5-734b-46a7-9537-ba9ccd192725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "evaluater: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_179d9d6a964f, metricName=weightedFMeasure, metricLabel=0.0, beta=1.0, eps=1.0E-15\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n",
    "\n",
    "val evaluater = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(indexer.getOutputCol)\n",
    "    .setMetricName(\"weightedFMeasure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ceae8539-ffc6-4d8d-be82-2d78649d9775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinearsvc_4c0a5d2d727b-maxIter: 10,\n",
       "\tlinearsvc_4c0a5d2d727b-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_4c0a5d2d727b-maxIter: 100,\n",
       "\tlinearsvc_4c0a5d2d727b-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_4c0a5d2d727b-maxIter: 10,\n",
       "\tlinearsvc_4c0a5d2d727b-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_4c0a5d2d727b-maxIter: 100,\n",
       "\tlinearsvc_4c0a5d2d727b-standardization: true\n",
       "})\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(lsvc.maxIter, Array(10, 100))\n",
    "    .addGrid(lsvc.regParam, Array(0.01, 0.1, 0.5))\n",
    "    .addGrid(lsvc.standardization, Array(false, true))\n",
    "    .addGrid(selector.numTopFeatures, Array(20, 2000))\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb916f0-47b3-4b7f-ba42-cdef4f5d8f35",
   "metadata": {},
   "source": [
    "Now, we simply perform the grid-search on a train-validation split and evaluate the best hyperparams on our previously created evaluater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9621f352-b196-4e9c-a1ff-8a98dab38724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_786ab58c8808\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "    .setEstimator(pipeline)\n",
    "    .setEvaluator(evaluater)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setTrainRatio(0.8)\n",
    "    .setSeed(seed)\n",
    "    .setParallelism(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a50112-73e8-4ce4-9b8c-a8d4854b74fe",
   "metadata": {},
   "source": [
    "Lastly, we fit the model, with the best hyperparameters to the data and perform predictions with it. Afterward, we evaluate the model based on the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40b492a9-c8de-4fb7-bd36-39d7b1526420",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 439.81291604042053 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.TrainValidationSplitModel = TrainValidationSplitModel: uid=tvs_786ab58c8808, bestModel=pipeline_ff8d8a911179, trainRatio=0.8\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val model = trainValidationSplit.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e0cd607b-acf9-42fa-88bd-7e26984069c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [category: string, reviewText: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9061ed4-6a74-4a93-afe5-d49809765612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score = 0.46804112569563594\n"
     ]
    }
   ],
   "source": [
    "println(s\"F1-Score = ${evaluater.evaluate(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bdf06e1d-95e8-4072-9597-ab693e0c5b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best binary classifier parameters:\n",
      "\tmaxIter: 10\n",
      "\tregParam: 0.0\n",
      "\tstandardization: false\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{OneVsRestModel, LinearSVCModel}\n",
       "bestModel: org.apache.spark.ml.PipelineModel = pipeline_ff8d8a911179\n",
       "bestClassifier: org.apache.spark.ml.classification.OneVsRestModel = OneVsRestModel: uid=oneVsRest_b02e036fc060, classifier=linearsvc_4c0a5d2d727b, numClasses=22, numFeatures=2000\n",
       "bestBinaryClassifierModel: org.apache.spark.ml.classification.LinearSVCModel = LinearSVCModel: uid=linearsvc_4c0a5d2d727b, numClasses=2, numFeatures=2000\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{OneVsRestModel, LinearSVCModel}\n",
    "\n",
    "val bestModel = model.bestModel.asInstanceOf[PipelineModel]\n",
    "val bestClassifier = bestModel.stages.last.asInstanceOf[OneVsRestModel]\n",
    "val bestBinaryClassifierModel = bestClassifier.models.head.asInstanceOf[LinearSVCModel]\n",
    "\n",
    "// Print the parameters of the best binary classifier model\n",
    "println(s\"Best binary classifier parameters:\\n\" +\n",
    "  s\"\\tmaxIter: ${bestBinaryClassifierModel.getMaxIter}\\n\" +\n",
    "  s\"\\tregParam: ${bestBinaryClassifierModel.getRegParam}\\n\" +\n",
    "  s\"\\tstandardization: ${bestBinaryClassifierModel.getStandardization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac59eb8-046f-4768-b223-c4fad0dc1690",
   "metadata": {},
   "source": [
    "val Array(training, test) = sampledDF.randomSplit(Array(0.8, 0.2), seed = seed)### Another approach\n",
    "Coming back later, when server isn't at full capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9304590-9e3a-4da6-ba48-ac2a821d58f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val Array(training, test) = sampledDF.randomSplit(Array(0.8, 0.2), seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1bf5041-867e-4020-be32-026426b1b359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipeline_data: org.apache.spark.ml.Pipeline = pipeline_3ff51004a9e5\n",
       "pipeline_classifier: org.apache.spark.ml.Pipeline = pipeline_d83ad444a85b\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline_data = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover, countVectorizer, idf, indexer))\n",
    "val pipeline_classifier = new Pipeline()\n",
    "    .setStages(Array(selector, normalizer, classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "91a42256-5f8d-4177-90f6-1adb8389b79a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_data: org.apache.spark.ml.PipelineModel = pipeline_3ff51004a9e5\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model_data = pipeline_data.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8a1299f6-f59f-4b78-a0ef-299e3f4e4e18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformed_training: org.apache.spark.sql.DataFrame = [category: string, reviewText: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transformed_training = model_data.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d1c697ac-f90d-419f-a42b-87e55ee78528",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_cache: transformed_training.type = [category: string, reviewText: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training_cache = transformed_training.cache() //vs persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e4690b28-70b5-469e-b0be-e04732844309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_cache: org.apache.spark.sql.DataFrame = [category: string, reviewText: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_training.write.save(\"transformed_training.parquet\")\n",
    "val training_cache = spark.read.load(\"transformed_training.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aa47b700-7462-47e4-84b7-5f5798710be8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_132389348f6e\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "    .setEstimator(pipeline_classifier)\n",
    "    .setEvaluator(evaluater)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setTrainRatio(0.8)\n",
    "    .setSeed(seed)\n",
    "    .setParallelism(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb11ed-c73a-4bd2-9553-ed5ab2f9778a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "val model_classifier = trainValidationSplit.fit(training_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b76405b0-1575-4f35-9102-9ad4dad72937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_test: org.apache.spark.sql.DataFrame = [category: string, reviewText: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_test = model_data.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cbff44-6ef8-4507-9f6d-2244ba0fe582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val predictions = model_classifier.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979a47f-f225-405b-820d-62e73421bb29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "println(s\"F1-Score = ${evaluater.evaluate(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b27c73-9738-451a-a181-b2f25328cbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
