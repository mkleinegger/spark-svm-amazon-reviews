{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "530addd5-5a67-40a5-b584-393c28f5bad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reviewsDF: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsDF = spark.read.options(Map(\"header\"->\"true\")).format(\"json\").load(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\").select(\"category\",\"reviewText\")\n",
    "reviewsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "19022931-913d-4fda-b55f-7f977d356664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            category|               words|\n",
      "+--------------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|[this, was, a, gi...|\n",
      "|Patio_Lawn_and_Garde|[this, is, a, ver...|\n",
      "|Patio_Lawn_and_Garde|[the, metal, base...|\n",
      "|Patio_Lawn_and_Garde|[for, the, most, ...|\n",
      "|Patio_Lawn_and_Garde|[this, hose, is, ...|\n",
      "|Patio_Lawn_and_Garde|[this, tool, work...|\n",
      "|Patio_Lawn_and_Garde|[this, product, i...|\n",
      "|Patio_Lawn_and_Garde|[i, was, excited,...|\n",
      "|Patio_Lawn_and_Garde|[i, purchased, th...|\n",
      "|Patio_Lawn_and_Garde|[never, used, a, ...|\n",
      "|Patio_Lawn_and_Garde|[good, price, goo...|\n",
      "|Patio_Lawn_and_Garde|[i, have, owned, ...|\n",
      "|Patio_Lawn_and_Garde|[i, had, won, a, ...|\n",
      "|Patio_Lawn_and_Garde|[the, birds, ate,...|\n",
      "|Patio_Lawn_and_Garde|[bought, last, su...|\n",
      "|Patio_Lawn_and_Garde|[i, knew, i, had,...|\n",
      "|Patio_Lawn_and_Garde|[i, was, a, littl...|\n",
      "|Patio_Lawn_and_Garde|[i, have, used, t...|\n",
      "|Patio_Lawn_and_Garde|[i, actually, do,...|\n",
      "|Patio_Lawn_and_Garde|[just, what, i, e...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Tokenizer\n",
       "import org.apache.spark.ml.util.DefaultParamsWritable\n",
       "import org.apache.spark.sql.functions.udf\n",
       "defined class CustomTokenizer\n",
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_e7128560c2b3\n",
       "tokenized: org.apache.spark.sql.DataFrame = [category: string, words: array<string>]\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Tokenize and Case Folding\n",
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "import org.apache.spark.ml.util.DefaultParamsWritable\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "class CustomTokenizer extends Tokenizer with DefaultParamsWritable {\n",
    "\n",
    "  // use splitting pattern from exercise 1\n",
    "  override protected def createTransformFunc: String => Seq[String] = { input =>\n",
    "    input.toLowerCase.split(\"[^a-zA-Z<>^|]+\").toSeq\n",
    "  }\n",
    "}\n",
    "\n",
    "val tokenizer = new CustomTokenizer()\n",
    ".setInputCol(\"reviewText\")\n",
    ".setOutputCol(\"words\")\n",
    "\n",
    "val tokenized = tokenizer.transform(reviewsDF).select(\"category\",\"words\")\n",
    "\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e0e470da-f77d-47b3-ac08-9b6e3f961b61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            category|            filtered|\n",
      "+--------------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|[gift, husband, m...|\n",
      "|Patio_Lawn_and_Garde|[nice, spreader, ...|\n",
      "|Patio_Lawn_and_Garde|[metal, base, hos...|\n",
      "|Patio_Lawn_and_Garde|[part, works, pre...|\n",
      "|Patio_Lawn_and_Garde|[hose, supposed, ...|\n",
      "|Patio_Lawn_and_Garde|[tool, works, cut...|\n",
      "|Patio_Lawn_and_Garde|[typical, usable,...|\n",
      "|Patio_Lawn_and_Garde|[excited, ditch, ...|\n",
      "|Patio_Lawn_and_Garde|[purchased, leaf,...|\n",
      "|Patio_Lawn_and_Garde|[manual, lawnmowe...|\n",
      "|Patio_Lawn_and_Garde|[good, price, goo...|\n",
      "|Patio_Lawn_and_Garde|[owned, flowtron,...|\n",
      "|Patio_Lawn_and_Garde|[similar, family,...|\n",
      "|Patio_Lawn_and_Garde|[birds, ate, blue...|\n",
      "|Patio_Lawn_and_Garde|[bought, summer, ...|\n",
      "|Patio_Lawn_and_Garde|[knew, mouse, bas...|\n",
      "|Patio_Lawn_and_Garde|[worried, reading...|\n",
      "|Patio_Lawn_and_Garde|[brand, long, tim...|\n",
      "|Patio_Lawn_and_Garde|[current, model, ...|\n",
      "|Patio_Lawn_and_Garde|[expected, works,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "defined class CustomStopWordsRemover\n",
       "stopWordsFile: String = stopwords.txt\n",
       "remover: CustomStopWordsRemover = StopWordsRemover: uid=stopWords_af1d7f26a8fb, numStopWords=596, locale=en_US, caseSensitive=false\n",
       "filtered: org.apache.spark.sql.DataFrame = [category: string, filtered: array<string>]\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "class CustomStopWordsRemover(stopWordsFile: String) extends StopWordsRemover {\n",
    "    // load and set custom stop words\n",
    "    val customStopWords: Array[String] = scala.io.Source.fromFile(stopWordsFile).getLines.toArray\n",
    "    setStopWords(customStopWords)    \n",
    "}\n",
    "\n",
    "\n",
    "val stopWordsFile = \"stopwords.txt\"\n",
    "val remover = new CustomStopWordsRemover(stopWordsFile)\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "val filtered = remover.transform(tokenized).select(\"category\", \"filtered\")\n",
    "filtered.show()\n",
    "\n",
    "\n",
    "// val query3 = filtered.filter(array_contains($\"filtered\", \"ever\"))\n",
    "// query3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7d12c16a-b963-48fa-99b5-68b8e70b2e25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+\n",
      "|  category|     token|   A|\n",
      "+----------+----------+----+\n",
      "|      Book|      good|4886|\n",
      "|      Book|     great|4834|\n",
      "|      Book|   reading|3893|\n",
      "|      Book|      love|3812|\n",
      "|      Book|      time|3755|\n",
      "|      Book|    author|3210|\n",
      "|      Book|characters|3142|\n",
      "|      Book|   written|2576|\n",
      "|      Book| recommend|2459|\n",
      "|      Book|    series|2409|\n",
      "|      Book|    people|2343|\n",
      "|      Book|      find|2256|\n",
      "|Electronic|     great|2223|\n",
      "|      Book|      make|2194|\n",
      "|      Book|     found|2104|\n",
      "|      Book|       put|1944|\n",
      "|      Book|      work|1941|\n",
      "|      Book|     loved|1925|\n",
      "|      Book|       end|1925|\n",
      "|      Book|     world|1923|\n",
      "+----------+----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenFreqByCategory: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, token: string ... 1 more field]\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenFreqByCategory = filtered\n",
    ".withColumn(\"token\", explode(array_distinct($\"filtered\")))\n",
    ".groupBy(\"category\", \"token\")\n",
    ".count()\n",
    ".withColumnRenamed(\"count\", \"A\").orderBy(desc(\"A\"))\n",
    "\n",
    "tokenFreqByCategory.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8822a35f-3b51-4d3c-89b0-4a19c3899710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.ChiSqSelector\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.stat.ChiSquareTest\n",
       "import org.apache.spark.sql.functions._\n",
       "tokenFreqByCategory: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, token: string ... 1 more field]\n",
       "t_total_number_of_occurrences: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [token: string, total_number_of_occurrences: bigint]\n",
       "n_docs_by_cat: org.apache.spark.sql.DataFrame = [category: string, n_docs_by_cat: bigint]\n",
       "n_of_docs: org.apache.spark.sql.DataFrame = [N: bigint]\n",
       "crossjoin_n_info: org.apache.spark.sql.DataFrame = [category: string, n_docs_by_cat: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.ChiSqSelector\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Step 1: for token t number of occurrens within each category -> A\n",
    "val tokenFreqByCategory = filtered\n",
    " .withColumn(\"token\", explode(array_distinct($\"filtered\")))\n",
    ".groupBy(\"category\", \"token\")\n",
    ".count()\n",
    ".withColumnRenamed(\"count\", \"A\").orderBy(desc(\"A\"))\n",
    "\n",
    "//tokenFreqByCategory.show()\n",
    "\n",
    "// Step 2: for token t total number of occurrencs across all categories -> B = this - A\n",
    "val t_total_number_of_occurrences = tokenFreqByCategory\n",
    "  .groupBy(\"token\")\n",
    "  .agg(sum(\"A\").alias(\"total_number_of_occurrences\")).orderBy(desc(\"total_number_of_occurrences\"))\n",
    "//t_total_number_of_occurrences.show()\n",
    "\n",
    "// Step 3: number of reviews by category\n",
    "val n_docs_by_cat = filtered.groupBy(\"category\").agg(count(\"*\").as(\"n_docs_by_cat\")) // C = this - A\n",
    "\n",
    "// Step 4: total number of reviews\n",
    "val n_of_docs = n_docs_by_cat.agg(sum(\"n_docs_by_cat\").alias(\"N\")) // N\n",
    "// join both dataframes\n",
    "val crossjoin_n_info = n_docs_by_cat.crossJoin(n_of_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ebc37969-5ca9-4d2a-a86f-5f0a3b89cf55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------------+\n",
      "|            category|      token|        chisquared|\n",
      "+--------------------+-----------+------------------+\n",
      "|       CDs_and_Vinyl|      music|13083.770814228063|\n",
      "|       Movies_and_TV|        dvd| 8746.742991979037|\n",
      "|       CDs_and_Vinyl|     albums| 6705.824908271496|\n",
      "|       CDs_and_Vinyl|     tracks|6516.9526511813565|\n",
      "|                Book|    reading| 6184.609280406393|\n",
      "|                Book|     author| 6181.072186631432|\n",
      "|       CDs_and_Vinyl|     lyrics|   5856.6404116392|\n",
      "|       CDs_and_Vinyl|     listen| 5637.042824641422|\n",
      "|       Movies_and_TV|     acting|  5158.79879531417|\n",
      "|      Office_Product| cartridges| 5038.569370197361|\n",
      "|       Movies_and_TV|     movies| 5030.696393981536|\n",
      "|       CDs_and_Vinyl|       band| 4883.454486644896|\n",
      "|       CDs_and_Vinyl|       rock|  4836.65107719276|\n",
      "|                Book| characters| 4818.141989312432|\n",
      "|       CDs_and_Vinyl|     vocals| 4785.881671742037|\n",
      "|Clothing_Shoes_an...|comfortable| 4674.489581798549|\n",
      "|       CDs_and_Vinyl|      track| 4579.551601051995|\n",
      "|         Pet_Supplie|        cat|4345.4105161240595|\n",
      "|       Movies_and_TV|      watch|4310.1744762011085|\n",
      "|                Book|    written|  4300.44930410557|\n",
      "+--------------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "join: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, token: string ... 1 more field]\n"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val join = tokenFreqByCategory\n",
    ".join(t_total_number_of_occurrences, (\"token\"))\n",
    ".join(crossjoin_n_info, (\"category\"))\n",
    ".withColumn(\"B\", $\"total_number_of_occurrences\" - $\"A\")\n",
    ".withColumn(\"C\", $\"n_docs_by_cat\" - $\"A\")\n",
    ".withColumn(\"D\", $\"N\" - $\"A\" - $\"B\" - $\"C\")\n",
    ".withColumn(\"D\", $\"N\" - $\"A\" - $\"B\" - $\"C\")\n",
    ".withColumn(\"chisquared\",\n",
    "  ($\"N\" * pow($\"A\" * $\"D\" - $\"B\" * $\"C\", 2)) /\n",
    "    (($\"A\" + $\"B\") * ($\"A\" + $\"C\") * ($\"B\" + $\"D\") * ($\"C\" + $\"D\"))\n",
    ")\n",
    ".select(\"category\", \"token\",\"chisquared\").orderBy(desc(\"chisquared\"))\n",
    "\n",
    "join.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
