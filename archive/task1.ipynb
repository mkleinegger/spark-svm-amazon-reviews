{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53ba899",
   "metadata": {},
   "source": [
    "Datasets/DataFrames Spark: Calculate Chi Squared values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "530addd5-5a67-40a5-b584-393c28f5bad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1715326141961_0171\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1715326141961_0171)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reviewsDF: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsDF = spark.read.options(Map(\"header\"->\"true\")).format(\"json\").load(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\").select(\"category\",\"reviewText\")\n",
    "reviewsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19022931-913d-4fda-b55f-7f977d356664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Tokenizer\n",
       "import org.apache.spark.ml.util.DefaultParamsWritable\n",
       "defined class CustomTokenizer\n",
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_c15664cdb2b5\n",
       "tokenized: org.apache.spark.sql.DataFrame = [category: string, words: array<string>]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Tokenize and Case Folding\n",
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "import org.apache.spark.ml.util.DefaultParamsWritable\n",
    "//import org.apache.spark.sql.functions.udf\n",
    "\n",
    "class CustomTokenizer extends Tokenizer with DefaultParamsWritable {\n",
    "\n",
    "  // use splitting pattern from exercise 1\n",
    "  override protected def createTransformFunc: String => Seq[String] = { input =>\n",
    "    input.toLowerCase.split(\"[^a-zA-Z<>^|]+\").toSeq\n",
    "  }\n",
    "}\n",
    "\n",
    "val tokenizer = new CustomTokenizer()\n",
    ".setInputCol(\"reviewText\")\n",
    ".setOutputCol(\"words\")\n",
    "\n",
    "val tokenized = tokenizer.transform(reviewsDF).select(\"category\",\"words\")\n",
    "\n",
    "//tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0e470da-f77d-47b3-ac08-9b6e3f961b61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StopWordsRemover\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "defined class CustomStopWordsRemover\n",
       "stopWordsFile: String = stopwords.txt\n",
       "remover: CustomStopWordsRemover = StopWordsRemover: uid=stopWords_83208a65f616, numStopWords=596, locale=en_US, caseSensitive=false\n",
       "filtered: org.apache.spark.sql.DataFrame = [category: string, filtered: array<string>]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "class CustomStopWordsRemover(stopWordsFile: String) extends StopWordsRemover {\n",
    "    // load and set custom stop words\n",
    "    val customStopWords: Array[String] = scala.io.Source.fromFile(stopWordsFile).getLines.toArray\n",
    "    setStopWords(customStopWords)    \n",
    "}\n",
    "\n",
    "\n",
    "val stopWordsFile = \"../data/stopwords.txt\"\n",
    "val remover = new CustomStopWordsRemover(stopWordsFile)\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "val filtered = remover.transform(tokenized).select(\"category\", \"filtered\")\n",
    "//filtered.show()\n",
    "\n",
    "\n",
    "// val query3 = filtered.filter(array_contains($\"filtered\", \"ever\"))\n",
    "// query3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d12c16a-b963-48fa-99b5-68b8e70b2e25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenFreqByCategory: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, token: string ... 1 more field]\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenFreqByCategory = filtered\n",
    ".withColumn(\"token\", explode(array_distinct($\"filtered\")))\n",
    ".groupBy(\"category\", \"token\")\n",
    ".count()\n",
    ".withColumnRenamed(\"count\", \"A\").orderBy(desc(\"A\"))\n",
    "\n",
    "//tokenFreqByCategory.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8822a35f-3b51-4d3c-89b0-4a19c3899710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.ChiSqSelector\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.stat.ChiSquareTest\n",
       "import org.apache.spark.sql.functions._\n",
       "tokenFreqByCategory: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, token: string ... 1 more field]\n",
       "t_total_number_of_occurrences: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [token: string, total_number_of_occurrences: bigint]\n",
       "n_docs_by_cat: org.apache.spark.sql.DataFrame = [category: string, n_docs_by_cat: bigint]\n",
       "n_of_docs: org.apache.spark.sql.DataFrame = [N: bigint]\n",
       "crossjoin_n_info: org.apache.spark.sql.DataFrame = [category: string, n_docs_by_cat: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.ChiSqSelector\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Step 1: for token t number of occurrens within each category -> A\n",
    "val tokenFreqByCategory = filtered\n",
    " .withColumn(\"token\", explode(array_distinct($\"filtered\")))\n",
    ".groupBy(\"category\", \"token\")\n",
    ".count()\n",
    ".withColumnRenamed(\"count\", \"A\").orderBy(desc(\"A\"))\n",
    "\n",
    "//tokenFreqByCategory.show()\n",
    "\n",
    "// Step 2: for token t total number of occurrencs across all categories -> B = this - A\n",
    "val t_total_number_of_occurrences = tokenFreqByCategory\n",
    "  .groupBy(\"token\")\n",
    "  .agg(sum(\"A\").alias(\"total_number_of_occurrences\")).orderBy(desc(\"total_number_of_occurrences\"))\n",
    "//t_total_number_of_occurrences.show()\n",
    "\n",
    "// Step 3: number of reviews by category\n",
    "val n_docs_by_cat = filtered.groupBy(\"category\").agg(count(\"*\").as(\"n_docs_by_cat\")) // C = this - A\n",
    "\n",
    "// Step 4: total number of reviews\n",
    "val n_of_docs = n_docs_by_cat.agg(sum(\"n_docs_by_cat\").alias(\"N\")) // N\n",
    "// join both dataframes\n",
    "val crossjoin_n_info = n_docs_by_cat.crossJoin(n_of_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebc37969-5ca9-4d2a-a86f-5f0a3b89cf55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSquaredValues: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, token: string ... 1 more field]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chiSquaredValues = tokenFreqByCategory\n",
    ".join(t_total_number_of_occurrences, (\"token\"))\n",
    ".join(crossjoin_n_info, (\"category\"))\n",
    ".withColumn(\"B\", $\"total_number_of_occurrences\" - $\"A\")\n",
    ".withColumn(\"C\", $\"n_docs_by_cat\" - $\"A\")\n",
    ".withColumn(\"D\", $\"N\" - $\"A\" - $\"B\" - $\"C\")\n",
    ".withColumn(\"D\", $\"N\" - $\"A\" - $\"B\" - $\"C\")\n",
    ".withColumn(\"chisquared\",\n",
    "  ($\"N\" * pow($\"A\" * $\"D\" - $\"B\" * $\"C\", 2)) /\n",
    "    (($\"A\" + $\"B\") * ($\"A\" + $\"C\") * ($\"B\" + $\"D\") * ($\"C\" + $\"D\"))\n",
    ")\n",
    ".select(\"category\", \"token\",\"chisquared\").orderBy(desc(\"chisquared\"))\n",
    "\n",
    "// chiSquaredValues.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c6ea5ad4-88ab-4eb1-a183-ba8f6af11bef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+----+\n",
      "|  category|     token|        chisquared|rank|\n",
      "+----------+----------+------------------+----+\n",
      "|Automotive|       oem|1068.8583585543724|   1|\n",
      "|Automotive|     honda|1035.2233546903822|   2|\n",
      "|Automotive|    engine| 763.2772560433446|   3|\n",
      "|Automotive|   vehicle| 667.9866189155996|   4|\n",
      "|Automotive|headlights| 661.4206245337347|   5|\n",
      "|Automotive|   exhaust| 627.5001547880793|   6|\n",
      "|Automotive|      jeep| 613.5399313513781|   7|\n",
      "|Automotive| installed| 556.7738877280015|   8|\n",
      "|Automotive|      tire| 547.3958796305628|   9|\n",
      "|Automotive| headlight| 522.1912239048576|  10|\n",
      "|Automotive|    toyota| 469.6296601687105|  11|\n",
      "|Automotive|   factory|  415.447649158483|  12|\n",
      "|Automotive|    dealer| 413.4005822828253|  13|\n",
      "|Automotive|dealership| 397.0329781505319|  14|\n",
      "|Automotive|   muffler| 394.6383917024621|  15|\n",
      "|Automotive|      ford| 381.6024077213795|  16|\n",
      "|Automotive|     wiper|   376.08092581619|  17|\n",
      "|Automotive|    nissan| 370.5218255560827|  18|\n",
      "|Automotive|      mats|370.32124504772554|  19|\n",
      "|Automotive|       fit| 345.0426805309335|  20|\n",
      "+----------+----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@434e325d\n",
       "top75ByCategory: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, token: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Top 75\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "val windowSpec = Window.partitionBy(\"category\").orderBy(desc(\"chisquared\"))\n",
    "\n",
    "val top75ByCategory = chiSquaredValues\n",
    ".withColumn(\"rank\", row_number().over(windowSpec))\n",
    ".filter(col(\"rank\") <= 75)\n",
    "//.withColumn(\"value_str\", col(\"chisquared\").cast(\"string\"))\n",
    "\n",
    "\n",
    "top75ByCategory.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91df68-58ab-45c6-854e-d8dc41e56586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              output|\n",
      "+--------------------+\n",
      "|Automotive:oem,10...|\n",
      "|Baby:diaper,2429....|\n",
      "|Book:reading,6184...|\n",
      "|Clothing_Shoes_an...|\n",
      "|Health_and_Person...|\n",
      "|Kindle_Store:auth...|\n",
      "|Movies_and_TV:dvd...|\n",
      "|Apps_for_Android:...|\n",
      "|Beauty:lotion,231...|\n",
      "|CDs_and_Vinyl:mus...|\n",
      "|Cell_Phones_and_A...|\n",
      "|Digital_Music:mus...|\n",
      "|Electronic:cable,...|\n",
      "|Grocery_and_Gourm...|\n",
      "|Home_and_Kitche:v...|\n",
      "|Musical_Instrumen...|\n",
      "|Office_Product:ca...|\n",
      "|Patio_Lawn_and_Ga...|\n",
      "|Pet_Supplie:cat,4...|\n",
      "|Sports_and_Outdoo...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// category, [{token, chi_squared}]\n",
    "\n",
    "val output = top75ByCategory\n",
    ".groupBy(\"category\")\n",
    ".agg(collect_list(concat_ws( \",\", $\"token\", $\"chisquared\")) as \"top_tokens\")\n",
    ".withColumn(\"output\", concat_ws(\":\", $\"category\", $\"top_tokens\"))\n",
    ".select(\"output\")\n",
    "\n",
    "// Show the result\n",
    "output.show()\n",
    "\n",
    "output\n",
    ".select(\"output\")\n",
    ".write\n",
    ".format(\"text\") // Change format to \"text\" for TXT file\n",
    ".option(\"header\", \"false\") // If you want to include headers, set this to true\n",
    ".mode(\"overwrite\") // Overwrite the output if it already exists, or use \"append\" or \"ignore\"\n",
    ".save(\"output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fec5493c-7d5f-404d-8d7e-1a17dbba3674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "68: error: org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row] does not take parameters",
     "output_type": "error",
     "traceback": [
      "<console>:68: error: org.apache.spark.sql.DataFrameWriter[org.apache.spark.sql.Row] does not take parameters",
      "       .write()",
      "             ^",
      ""
     ]
    }
   ],
   "source": [
    "output\n",
    ".select(\"output\")\n",
    ".write\n",
    ".format(\"text\") // Change format to \"text\" for TXT file\n",
    ".option(\"header\", \"false\") // If you want to include headers, set this to true\n",
    ".mode(\"overwrite\") // Overwrite the output if it already exists, or use \"append\" or \"ignore\"\n",
    ".save(\"file:///home/dic24/e12239877/Exercise_2/output.txt\")\n",
    " //\"file:///path/to/output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "defe0cb2-c9dd-47f5-ae66-3e6833aa7b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted.",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)",
      "  at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)",
      "  at org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)",
      "  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)",
      "  ... 51 elided",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1352.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1352.0 (TID 1052) (zookeeper03.os.hpc.tuwien.ac.at executor 2): java.io.IOException: Mkdirs failed to create file:/home/dic24/e12239877/Exercise_2/output.txt/_temporary/0/_temporary/attempt_202405130935223960805438915708656_1904_m_000000_3 (exists=false, cwd=file:/hdfs/nm-local-dir/usercache/e12239877/appcache/application_1715326141961_0171/container_1715326141961_0171_01_000003)",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:724)",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:709)",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1233)",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1109)",
      "\tat org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "\tat java.lang.Thread.run(Thread.java:750)",
      "",
      "Driver stacktrace:",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)",
      "  at scala.Option.foreach(Option.scala:407)",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)",
      "  ... 87 more",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/home/dic24/e12239877/Exercise_2/output.txt/_temporary/0/_temporary/attempt_202405130935223960805438915708656_1904_m_000000_3 (exists=false, cwd=file:/hdfs/nm-local-dir/usercache/e12239877/appcache/application_1715326141961_0171/container_1715326141961_0171_01_000003)",
      "  at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:724)",
      "  at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:709)",
      "  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1233)",
      "  at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1109)",
      "  at org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113)",
      "  at org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)",
      "  at org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:131)",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
      "  ... 1 more",
      ""
     ]
    }
   ],
   "source": [
    "output.rdd.saveAsTextFile(\"file:///home/dic24/e12239877/Exercise_2/output.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
