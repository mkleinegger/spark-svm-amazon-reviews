{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f7703a-8aa3-47c9-8461-670f297d9fb1",
   "metadata": {},
   "source": [
    "# Amazon-reviews predictions with Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6d449-425b-4222-967e-d082d7692b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val conf = new SparkConf()\n",
    "      .setMaster(\"yarn\")\n",
    "      .set(\"spark.executor.memory\", \"4g\")\n",
    "      .set(\"spark.driver.memory\", \"4g\")\n",
    "      .set(\"spark.driver.maxResultSize\", \"2g\")\n",
    "      .set(\"spark.executor.instances\", \"5\")\n",
    "      .set(\"spark.executor.cores\", \"4\")\n",
    "      .set(\"spark.default.parallelism\", \"20\")\n",
    "\n",
    "// Initialize SparkSession\n",
    "val sc = SparkSession.builder.config(conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73583dd-e37b-4611-8f50-484cb7ba8bed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read data and transform to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61608eac-7522-4ac5-8f13-6ff8643f1507",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val K = 75\n",
    "val file_path_stopwords = \"../data/stopwords.txt\"\n",
    "val file_path_reviews = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "// val file_path_reviews = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "\n",
    "val tokenizePattern = \"[^a-zA-Z<>^|]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ededeb-ae72-4a27-83a6-f3a17f4b5a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "val df = sc.read.json(file_path_reviews).select(\"category\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa0346-2df6-4f93-8577-13adf01fe6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scala.io.Source.fromFile\n",
    "\n",
    "val stopWords = fromFile(file_path_stopwords).getLines.toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195030e1-bb58-4186-b616-54d4767b8f20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04b92f-8975-4983-a0d1-78eaa9a3c915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import java.io.PrintWriter\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.collection.immutable.TreeSet\n",
    "\n",
    "def writeRDDToFile(rdd: RDD[(String, Seq[(String, Double)])], filePath: String): Unit = {\n",
    "    var mergedTerms = TreeSet[String]()\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    val collectedData = rdd.sortByKey().collect()\n",
    "\n",
    "    for ((category, topk) <- collectedData) {\n",
    "        val topKStr = topk.map { case (term, chiSquared) => \n",
    "            mergedTerms += term\n",
    "            s\"$term:$chiSquared\"\n",
    "        }.mkString(\" \")\n",
    "        writer.println(s\"<$category> $topKStr\")\n",
    "    }\n",
    "    \n",
    "    writer.print(mergedTerms.mkString(\" \"))\n",
    "    \n",
    "    writer.close()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aec3a0-0f39-44a4-a064-f60058d30e19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import java.io.PrintWriter\n",
    "\n",
    "def writeArrToFile(arr: Array[String], filePath: String) = {\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    writer.println(arr.mkString(\" \"))\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.PrintWriter\n",
    "\n",
    "def evaluateGridSearch(paramMaps: Array[org.apache.spark.ml.param.ParamMap], validationMetrics: Array[Double], filePath: String) = {\n",
    "    val paramsAndMetrics = paramMaps.zip(validationMetrics)\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    writer.println(\"maxIter, NumTopFeatures, regParam, standardization, f1-score\")\n",
    "\n",
    "    paramsAndMetrics.foreach { case (paramMap, metric) =>\n",
    "        val maxIter = paramMap.get(lscv.maxIter).head\n",
    "        val NumTopFeatures = paramMap.get(selector.numTopFeatures).head\n",
    "        val regParam = paramMap.get(lscv.regParam).head\n",
    "        val standardization = paramMap.get(lscv.standardization).head\n",
    "\n",
    "        writer.println(s\"$maxIter, $NumTopFeatures, $regParam, $standardization, ${metric}\")\n",
    "    }\n",
    "\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a17fb-63a4-46e0-9748-e37714fb5364",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate Chi-Square\n",
    "This approaches, first calculates the different number of documents per category and afterward calculates the chi-squared values per term per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d14419-d35e-4d0f-abd0-5c546e51d7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee8bb4e-b8b1-4c96-bd35-e629458ba6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "def preprocessing(row: Row): Seq[((String, Option[String]), Int)] = {\n",
    "  val category = row.getString(0)\n",
    "  val reviewText = row.getString(1)\n",
    "\n",
    "  val terms = reviewText\n",
    "    .toLowerCase()\n",
    "    .split(tokenizePattern)\n",
    "    .filter(token => token.length > 1 && !stopWords.contains(token))\n",
    "    .toSet\n",
    "\n",
    "  val counts = Seq(((category, None), 1)) ++ terms.map(token => ((category, Some(token)), 1))\n",
    "  counts.toSeq\n",
    "}\n",
    "\n",
    "def tokenToKey(row: ((String, Option[String]), Int)): (Option[String], (String, Int)) = {\n",
    "  val ((category, token), count) = row\n",
    "  (token, (category, count))\n",
    "}\n",
    "\n",
    "def tokenSum(row: (Option[String], Iterable[(String, Int)])): Iterable[(String, (Option[String], Int, Int))] = {\n",
    "  val (token, values) = row\n",
    "  val counts = values.groupBy(_._1).mapValues(_.map(_._2).sum)\n",
    "  val n_t = counts.values.sum\n",
    "\n",
    "  counts.map { case (category, count) => (category, (token, count, n_t)) }\n",
    "}\n",
    "\n",
    "def chiSquared(row: (String, Iterable[(Option[String], Int, Int)])): (String, Seq[(String, Double)]) = {\n",
    "  val (category, values) = row\n",
    "  val counts = values.map { case (token, count, n_t) => token -> (count, n_t) }.toMap\n",
    "  val (n_c, n) = counts.getOrElse(None, (0, counts.values.map(_._2).sum))\n",
    "\n",
    "  val results = counts\n",
    "    .collect {\n",
    "      case (Some(token), (a, n_t)) =>\n",
    "        val b = n_t - a\n",
    "        val c = n_c - a\n",
    "        val d = n - a - b - c\n",
    "        val chiSquaredValue = n.toDouble * math.pow(a * d - b * c, 2) / ((a + b).toDouble * (a + c) * (b + d) * (c + d))\n",
    "      \n",
    "        (token, chiSquaredValue)\n",
    "    }\n",
    "    .toSeq\n",
    "    .sortBy(-_._2)\n",
    "    .take(K)\n",
    "\n",
    "  (category, results)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a0ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "val topTermsPerCategory = rdd\n",
    "  .flatMap(preprocessing)\n",
    "  .reduceByKey(_ + _)\n",
    "  .map(tokenToKey)\n",
    "  .groupByKey()\n",
    "  .flatMap(tokenSum)\n",
    "  .groupByKey()\n",
    "  .map(chiSquared)\n",
    "  .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f313f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "writeRDDToFile(topTermsPerCategory, \"../output_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ab42d-f2e5-4194-8d2f-5194f67a1b7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets/DataFrames: Spark ML and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74ef30-43da-42a8-909f-801d904cdffe",
   "metadata": {},
   "source": [
    "First, create all the necessary transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ecf763-118c-4101-82ba-ed8c66acade4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{StringIndexer, RegexTokenizer, StopWordsRemover}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "    .setInputCol(\"category\")\n",
    "    .setOutputCol(\"category_index\")\n",
    "\n",
    "val tokenizer = new RegexTokenizer()\n",
    "    .setInputCol(\"reviewText\")\n",
    "    .setOutputCol(\"raw_terms\")\n",
    "    .setMinTokenLength(2)\n",
    "    .setPattern(tokenizePattern)\n",
    "    .setToLowercase(true)\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"terms\")\n",
    "    .setStopWords(stopWords)\n",
    "\n",
    "val countVectorizer = new CountVectorizer()\n",
    "    .setInputCol(remover.getOutputCol)\n",
    "    .setOutputCol(\"raw_features\")\n",
    "    .setMinDF(1)\n",
    "\n",
    "val idf = new IDF()\n",
    "    .setInputCol(countVectorizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35ba46-fd3a-45ba-b920-3cd4f6612b09",
   "metadata": {},
   "source": [
    "Afterward, we create the Chi^2-Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4fdaab-8640-402d-b1f6-84d68a89dc10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{ChiSqSelector, ChiSqSelectorModel}\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    "  .setNumTopFeatures(2000)\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(\"category_index\")\n",
    "  .setOutputCol(\"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1e633-8635-4bae-962e-d894a3d77aaf",
   "metadata": {},
   "source": [
    "Lastly, we create the pipeline to execute all the transformers and select the top K features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc437e-d8f4-48e7-ba40-c685356e7230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover, countVectorizer, idf, indexer, selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694186d-c794-405c-a19a-6c222d4bb77b",
   "metadata": {
    "tags": []
   },
   "source": [
    "After creation of the pipeline, we can now fit it to our data, we want to transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126e42f-ce5a-494f-88c7-10849cad7c72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "val model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588eed2",
   "metadata": {},
   "source": [
    "Afterward, we can extract the vocabulary and selected features to map them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4c09e-6668-47b6-9984-eec6719af35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val vocabulary = model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary\n",
    "val selectedFeatures = model.stages.last.asInstanceOf[ChiSqSelectorModel].selectedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb593f4-51fa-4785-8171-c3eb50e07127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scala.util.Sorting.quickSort\n",
    "\n",
    "val top2000terms = selectedFeatures.map(i => vocabulary(i))\n",
    "quickSort(top2000terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec7c6b-842d-4724-a335-af29cd8b6ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "writeArrToFile(top2000terms, \"../output_ds.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bcf444-c1a7-4f67-9e8d-d2c408feb2e3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "328d693b-4527-4d5e-8561-7dd0e545733c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seed: Int = 12041500\n",
       "fraction: Double = 0.5\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 12041500\n",
    "val fraction = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a1dc12e-783e-4360-9994-dda57528aae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampledDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampledDF = df.sample(true, fraction, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "345d9a87-6bbb-403c-bc97-408bde156f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = sampledDF.randomSplit(Array(0.8, 0.2), seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "393926a9-7bd3-4107-a4ae-86311222ec1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Normalizer\n",
       "normalizer: org.apache.spark.ml.feature.Normalizer = Normalizer: uid=normalizer_a52da129e251, p=2.0\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val normalizer = new Normalizer()\n",
    "  .setInputCol(selector.getOutputCol)\n",
    "  .setOutputCol(\"normFeatures\")\n",
    "  .setP(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4385f0fe-d932-4559-80fc-ae6139617624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
       "lsvc: org.apache.spark.ml.classification.LinearSVC = linearsvc_680b806ceee2\n",
       "classifier: org.apache.spark.ml.classification.OneVsRest = oneVsRest_ddde733371fe\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
    "\n",
    "val lsvc = new LinearSVC()\n",
    "\n",
    "val classifier = new OneVsRest()\n",
    "    .setClassifier(lsvc)\n",
    "    .setFeaturesCol(normalizer.getOutputCol)\n",
    "    .setLabelCol(indexer.getOutputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6dfa441-a570-4c73-a07e-f240edf73078",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_classifier: org.apache.spark.ml.Pipeline = pipeline_145c10c57836\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_classifier = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover, countVectorizer, idf, indexer, selector, normalizer, classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2442cc79-160b-4021-b147-aab19e6d37c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinearsvc_680b806ceee2-maxIter: 10,\n",
       "\tchiSqSelector_154469e462f0-numTopFeatures: 20,\n",
       "\tlinearsvc_680b806ceee2-regParam: 0.001,\n",
       "\tlinearsvc_680b806ceee2-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_680b806ceee2-maxIter: 50,\n",
       "\tchiSqSelector_154469e462f0-numTopFeatures: 20,\n",
       "\tlinearsvc_680b806ceee2-regParam: 0.001,\n",
       "\tlinearsvc_680b806ceee2-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_680b806ceee2-maxIter: 10,\n",
       "\tchiSqSelector_154469e462f0-numTopFeatures: 20,\n",
       "\tlinearsvc_680b806ceee2-regParam: 0.01,\n",
       "\tlinearsvc_680b806ceee2-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_680b806ceee2-maxIter: 50,\n",
       "\tchiSqSelector_154469e462f0-numTopFeatures: 20,\n",
       "\tlinearsvc_680b806ceee2-regParam: 0.01,\n",
       "\tlinearsvc_680b806ceee2-s...\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(selector.numTopFeatures, Array(20, 2000))\n",
    "    .addGrid(lsvc.maxIter, Array(10, 50))\n",
    "    .addGrid(lsvc.regParam, Array(0.001, 0.01, 0.1))\n",
    "    .addGrid(lsvc.standardization, Array(false, true))\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36f916d1-33f1-498a-a46e-5f04af08770a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "evaluater: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_18e861264f98, metricName=f1, metricLabel=0.0, beta=1.0, eps=1.0E-15\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n",
    "\n",
    "val evaluater = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(indexer.getOutputCol)\n",
    "    .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0048506-85b2-4c26-9fca-1ab4ba442002",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_5af76543b683\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "    .setEstimator(pipeline_classifier)\n",
    "    .setEvaluator(evaluater)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setTrainRatio(0.8)\n",
    "    .setSeed(seed)\n",
    "    .setParallelism(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7da6e-5361-4199-946d-cc7f7fd65218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "val model = trainValidationSplit.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2786a0-e648-478d-a285-d3fea1fc6afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674a355-60f2-4a3b-9332-7b0bc91c0382",
   "metadata": {},
   "source": [
    "Lastly, we sample and therefore reduce the size of the data to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "626391fa-fedc-4365-9789-04a59b3d904a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score = 0.6685228303780189\n"
     ]
    }
   ],
   "source": [
    "println(s\"F1-Score = ${evaluater.evaluate(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30fc946a-e68d-4470-a24d-977051b7aa68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best binary classifier parameters:\n",
      "\tmaxIter: 10\n",
      "\tregParam: 0.001\n",
      "\tstandardization: true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{OneVsRestModel, LinearSVCModel}\n",
       "bestModel: org.apache.spark.ml.PipelineModel = pipeline_482b12d36bfa\n",
       "bestClassifier: org.apache.spark.ml.classification.OneVsRestModel = OneVsRestModel: uid=oneVsRest_bb12bdfb41f3, classifier=linearsvc_cad71bb5a8e8, numClasses=22, numFeatures=2000\n",
       "bestBinaryClassifierModel: org.apache.spark.ml.classification.LinearSVCModel = LinearSVCModel: uid=linearsvc_cad71bb5a8e8, numClasses=2, numFeatures=2000\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{OneVsRestModel, LinearSVCModel}\n",
    "\n",
    "val bestModel = model.bestModel.asInstanceOf[PipelineModel]\n",
    "val bestClassifier = bestModel.stages.last.asInstanceOf[OneVsRestModel]\n",
    "val bestBinaryClassifierModel = bestClassifier.models.head.asInstanceOf[LinearSVCModel]\n",
    "\n",
    "println(s\"Best binary classifier parameters:\\n\" +\n",
    "  s\"\\tmaxIter: ${bestBinaryClassifierModel.getMaxIter}\\n\" +\n",
    "  s\"\\tregParam: ${bestBinaryClassifierModel.getRegParam}\\n\" +\n",
    "  s\"\\tstandardization: ${bestBinaryClassifierModel.getStandardization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311376e3",
   "metadata": {},
   "source": [
    "At this point, we store for each parameter combination of the grid together with the performance metrics into a csv file for further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbaf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "val paramMaps = model.getEstimatorParamMaps\n",
    "val validationMetrics = model.validationMetrics\n",
    "val paramsAndMetrics = paramMaps.zip(validationMetrics)\n",
    "\n",
    "// evaluateGridSearch(paramMaps, validationMetrics, \"../grid_search_evaluation.csv\")\n",
    "evaluateGridSearch(paramMaps, validationMetrics, \"Exercise_2/data/grid_search_evaluation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
