{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f7703a-8aa3-47c9-8461-670f297d9fb1",
   "metadata": {},
   "source": [
    "# Amazon-reviews predictions with Spark ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df47f3",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate on how to apply Spark to process large text corpora in the domain of amazon reviews and making predictions using Apache Spark ML outling four steps. At first we will\n",
    " 1. Read the data, transform it to RDD\n",
    " 2. Preprocess with RDDs and calculate Chi Squared values for each token\n",
    " 3. Construct a Spark ML pipeline for preprocessing and feature extraction\n",
    " 4. Train and validate a Text Classification Model\n",
    " \n",
    "\n",
    " At first, we need create a SparkSession using the following Spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e6d449-425b-4222-967e-d082d7692b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1715326141961_2701\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1715326141961_2701)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.SparkConf\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@71347310\n",
       "sc: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@507ceebf\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val conf = new SparkConf()\n",
    "      .setMaster(\"yarn\")\n",
    "      .set(\"spark.executor.memory\", \"4g\")\n",
    "      .set(\"spark.driver.memory\", \"4g\")\n",
    "      .set(\"spark.driver.maxResultSize\", \"2g\")\n",
    "      .set(\"spark.executor.instances\", \"5\")\n",
    "      .set(\"spark.executor.cores\", \"4\")\n",
    "      .set(\"spark.default.parallelism\", \"20\")\n",
    "\n",
    "// Initialize SparkSession\n",
    "val sc = SparkSession.builder.config(conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73583dd-e37b-4611-8f50-484cb7ba8bed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read data and transform to RDD\n",
    "\n",
    "Secondly, we will establish global variables, import the Amazon reviews dataset, and load the stopwords. Additionally, we will define helper functions that are  intended to write files to the local filesystem.\n",
    "\n",
    "Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61608eac-7522-4ac5-8f13-6ff8643f1507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K: Int = 75\n",
       "file_path_stopwords: String = ../data/stopwords.txt\n",
       "file_path_reviews: String = hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\n",
       "tokenizePattern: String = [^a-zA-Z<>^|]+\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val K = 75\n",
    "val file_path_stopwords = \"../data/stopwords.txt\"\n",
    "val file_path_reviews = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "// val file_path_reviews = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "\n",
    "val tokenizePattern = \"[^a-zA-Z<>^|]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006bcb1",
   "metadata": {},
   "source": [
    "Load the amazon review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ededeb-ae72-4a27-83a6-f3a17f4b5a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 7.644007444381714 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val df = sc.read.json(file_path_reviews).select(\"category\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cf89cd",
   "metadata": {},
   "source": [
    "Load the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28fa0346-2df6-4f93-8577-13adf01fe6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source.fromFile\n",
       "stopWords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs, but, by, c, came, camera, can, cannot, cant, car, case, cause, causes, ...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source.fromFile\n",
    "\n",
    "val stopWords = fromFile(file_path_stopwords).getLines.toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195030e1-bb58-4186-b616-54d4767b8f20",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ecf18",
   "metadata": {},
   "source": [
    "This helper function is designed to write an RDD to the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d04b92f-8975-4983-a0d1-78eaa9a3c915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io.PrintWriter\n",
       "import org.apache.spark.rdd.RDD\n",
       "import scala.collection.immutable.TreeSet\n",
       "writeRDDToFile: (rdd: org.apache.spark.rdd.RDD[(String, Seq[(String, Double)])], filePath: String)Unit\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.collection.immutable.TreeSet\n",
    "\n",
    "def writeRDDToFile(rdd: RDD[(String, Seq[(String, Double)])], filePath: String): Unit = {\n",
    "    var mergedTerms = TreeSet[String]()\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    val collectedData = rdd.sortByKey().collect()\n",
    "\n",
    "    for ((category, topk) <- collectedData) {\n",
    "        val topKStr = topk.map { case (term, chiSquared) => \n",
    "            mergedTerms += term\n",
    "            s\"$term:$chiSquared\"\n",
    "        }.mkString(\" \")\n",
    "        writer.println(s\"<$category> $topKStr\")\n",
    "    }\n",
    "    \n",
    "    writer.print(mergedTerms.mkString(\" \"))\n",
    "    \n",
    "    writer.close()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870f720b",
   "metadata": {},
   "source": [
    "The helper function designed to write an String Array to the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82aec3a0-0f39-44a4-a064-f60058d30e19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io.PrintWriter\n",
       "writeArrToFile: (arr: Array[String], filePath: String)Unit\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "\n",
    "def writeArrToFile(arr: Array[String], filePath: String) = {\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    writer.println(arr.mkString(\" \"))\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a17fb-63a4-46e0-9748-e37714fb5364",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate Chi-Square\n",
    "This approaches, first calculates the different number of documents per category and afterward calculates the chi-squared values per term per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d14419-d35e-4d0f-abd0-5c546e51d7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[8] at rdd at <console>:32\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43932df0",
   "metadata": {},
   "source": [
    "In the following cell, all neccessary function are defined that are used to evaluate the chi-squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ee8bb4e-b8b1-4c96-bd35-e629458ba6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.8895168304443359 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "import org.apache.spark.sql.Row\n",
       "preprocessing: (row: org.apache.spark.sql.Row)Seq[((String, Option[String]), Int)]\n",
       "tokenToKey: (row: ((String, Option[String]), Int))(Option[String], (String, Int))\n",
       "tokenSum: (row: (Option[String], Iterable[(String, Int)]))Iterable[(String, (Option[String], Int, Int))]\n",
       "chiSquared: (row: (String, Iterable[(Option[String], Int, Int)]))(String, Seq[(String, Double)])\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "def preprocessing(row: Row): Seq[((String, Option[String]), Int)] = {\n",
    "  val category = row.getString(0)\n",
    "  val reviewText = row.getString(1)\n",
    "\n",
    "  val terms = reviewText\n",
    "    .toLowerCase()\n",
    "    .split(tokenizePattern)\n",
    "    .filter(token => token.length > 1 && !stopWords.contains(token))\n",
    "    .toSet\n",
    "\n",
    "  val counts = Seq(((category, None), 1)) ++ terms.map(token => ((category, Some(token)), 1))\n",
    "  counts.toSeq\n",
    "}\n",
    "\n",
    "def tokenToKey(row: ((String, Option[String]), Int)): (Option[String], (String, Int)) = {\n",
    "  val ((category, token), count) = row\n",
    "  (token, (category, count))\n",
    "}\n",
    "\n",
    "def tokenSum(row: (Option[String], Iterable[(String, Int)])): Iterable[(String, (Option[String], Int, Int))] = {\n",
    "  val (token, values) = row\n",
    "  val counts = values.groupBy(_._1).mapValues(_.map(_._2).sum)\n",
    "  val n_t = counts.values.sum\n",
    "\n",
    "  counts.map { case (category, count) => (category, (token, count, n_t)) }\n",
    "}\n",
    "\n",
    "def chiSquared(row: (String, Iterable[(Option[String], Int, Int)])): (String, Seq[(String, Double)]) = {\n",
    "  val (category, values) = row\n",
    "  val counts = values.map { case (token, count, n_t) => token -> (count, n_t) }.toMap\n",
    "  val (n_c, n) = counts.getOrElse(None, (0, counts.values.map(_._2).sum))\n",
    "\n",
    "  val results = counts\n",
    "    .collect {\n",
    "      case (Some(token), (a, n_t)) =>\n",
    "        val b = n_t - a\n",
    "        val c = n_c - a\n",
    "        val d = n - a - b - c\n",
    "        val chiSquaredValue = n.toDouble * math.pow(a * d - b * c, 2) / ((a + b).toDouble * (a + c) * (b + d) * (c + d))\n",
    "      \n",
    "        (token, chiSquaredValue)\n",
    "    }\n",
    "    .toSeq\n",
    "    .sortBy(-_._2)\n",
    "    .take(K)\n",
    "\n",
    "  (category, results)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7335f36",
   "metadata": {},
   "source": [
    "The previously defined functions are now applied by chaining various generic RDD functions and applying the necessary transformations and actions for chi-squared calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "363a0ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 33.28363609313965 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topTermsPerCategory: org.apache.spark.rdd.RDD[(String, Seq[(String, Double)])] = ShuffledRDD[18] at sortByKey at <console>:46\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val topTermsPerCategory = rdd\n",
    "  .flatMap(preprocessing)\n",
    "  .reduceByKey(_ + _)\n",
    "  .map(tokenToKey)\n",
    "  .groupByKey()\n",
    "  .flatMap(tokenSum)\n",
    "  .groupByKey()\n",
    "  .map(chiSquared)\n",
    "  .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e4f313f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.2687962055206299 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "writeRDDToFile(topTermsPerCategory, \"../output_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ab42d-f2e5-4194-8d2f-5194f67a1b7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets/DataFrames: Spark ML and Pipelines\n",
    "\n",
    "In this section, we will create an initial pipeline to delve into the process. We will apply this pipeline to the Amazon review dataset to retrieve the top 2000 selected features by ChiSqSelector. These features will be stored and later used for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74ef30-43da-42a8-909f-801d904cdffe",
   "metadata": {},
   "source": [
    "First, create all the necessary transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ecf763-118c-4101-82ba-ed8c66acade4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{StringIndexer, RegexTokenizer, StopWordsRemover}\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
       "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_72d2940458eb\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_d6afe954f67f, minTokenLength=2, gaps=true, pattern=[^a-zA-Z<>^|]+, toLowercase=true\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = StopWordsRemover: uid=stopWords_e61df3ed420d, numStopWords=596, locale=en_US, caseSensitive=false\n",
       "countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_2ffc01fba4d9\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_c827c0fd494c\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{StringIndexer, RegexTokenizer, StopWordsRemover}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "    .setInputCol(\"category\")\n",
    "    .setOutputCol(\"category_index\")\n",
    "\n",
    "val tokenizer = new RegexTokenizer()\n",
    "    .setInputCol(\"reviewText\")\n",
    "    .setOutputCol(\"raw_terms\")\n",
    "    .setMinTokenLength(2)\n",
    "    .setPattern(tokenizePattern)\n",
    "    .setToLowercase(true)\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"terms\")\n",
    "    .setStopWords(stopWords)\n",
    "\n",
    "val countVectorizer = new CountVectorizer()\n",
    "    .setInputCol(remover.getOutputCol)\n",
    "    .setOutputCol(\"raw_features\")\n",
    "    .setMinDF(1)\n",
    "\n",
    "val idf = new IDF()\n",
    "    .setInputCol(countVectorizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35ba46-fd3a-45ba-b920-3cd4f6612b09",
   "metadata": {},
   "source": [
    "Afterward, we create the Chi^2-Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a4fdaab-8640-402d-b1f6-84d68a89dc10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{ChiSqSelector, ChiSqSelectorModel}\n",
       "selector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_e102c3526f1a\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{ChiSqSelector, ChiSqSelectorModel}\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    "  .setNumTopFeatures(2000)\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(\"category_index\")\n",
    "  .setOutputCol(\"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1e633-8635-4bae-962e-d894a3d77aaf",
   "metadata": {},
   "source": [
    "Lastly, we create the pipeline to execute all the transformers and select the top K features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37fc437e-d8f4-48e7-ba40-c685356e7230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_3f2c1deaf497\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover, countVectorizer, idf, indexer, selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694186d-c794-405c-a19a-6c222d4bb77b",
   "metadata": {
    "tags": []
   },
   "source": [
    "After creation of the pipeline, we can now fit it to our data, we want to transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4126e42f-ce5a-494f-88c7-10849cad7c72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 50.5444974899292 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_3f2c1deaf497\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588eed2",
   "metadata": {},
   "source": [
    "Afterward, we can extract the vocabulary and selected features to map them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f4c09e-6668-47b6-9984-eec6719af35f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocabulary: Array[String] = Array(great, good, love, time, work, recommend, back, easy, make, bought, made, find, buy, price, put, reading, quality, people, works, quot, years, nice, characters, long, series, lot, found, author, day, bit, feel, makes, thing, perfect, fit, end, set, loved, things, thought, music, small, hard, give, year, world, size, worth, pretty, times, sound, written, light, real, big, amazon, part, bad, highly, money, excellent, purchased, happy, high, enjoyed, problem, family, interesting, wanted, character, job, review, purchase, man, watch, days, enjoy, place, home, stars, short, writing, play, cover, top, fan, full, fine, color, side, order, wonderful, amazing, point, fact, reviews, ordered, stories, favorite, easily, needed, battery, screen, water, dvd, beautifu...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocabulary = model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary\n",
    "val selectedFeatures = model.stages.last.asInstanceOf[ChiSqSelectorModel].selectedFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc55b6",
   "metadata": {},
   "source": [
    "Sort the terms in ascending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb593f4-51fa-4785-8171-c3eb50e07127",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Sorting.quickSort\n",
       "top2000terms: Array[String] = Array(access, accessories, account, acid, acoustic, act, acted, acting, action, actions, actor, actors, adapter, adapters, addicted, addicting, addictive, adjust, adjustable, adjustment, admit, adorable, ads, adult, adults, adventure, adventures, advertised, advice, age, ages, agree, air, albums, alive, alpha, amazing, amazon, america, american, amp, amusing, analysis, ancient, android, angle, animals, animated, animation, anime, answers, antenna, appeal, apple, applied, apply, applying, approach, apps, arch, arm, arrangements, arrived, art, artist, artists, asleep, aspects, assemble, assembled, assembly, asus, atmosphere, attach, attached, attention, attractive, audience, audio, author, authors, auto, automatically, awar...\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Sorting.quickSort\n",
    "\n",
    "val top2000terms = selectedFeatures.map(i => vocabulary(i))\n",
    "quickSort(top2000terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a2dc87",
   "metadata": {},
   "source": [
    "Store the terms to local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5ec7c6b-842d-4724-a335-af29cd8b6ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.2079153060913086 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "writeArrToFile(top2000terms, \"../output_ds.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bcf444-c1a7-4f67-9e8d-d2c408feb2e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "328d693b-4527-4d5e-8561-7dd0e545733c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seed: Int = 12041500\n",
       "fraction: Double = 0.1\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 12041500\n",
    "val fraction = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419322a5-3a8f-47a3-a010-42a12f161fca",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "First, we pre process the data by performing the whole tokenization and tdidf-calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b53074fe-67ed-4af2-a419-ded3246f97c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipelinePreprocessing: org.apache.spark.ml.Pipeline = pipeline_a9b6b4373fe7\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipelinePreprocessing = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f076ac",
   "metadata": {},
   "source": [
    "Subsample the dataset to make model training easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5da3869-3ff8-4396-89fd-b58c322de1ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampledDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampledDF = df.sample(withReplacement = false, fraction = fraction, seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78363cc0",
   "metadata": {},
   "source": [
    "Create train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4ec2e4-24e9-4739-9fa5-1b548cd1c905",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = sampledDF.randomSplit(Array(0.8, 0.2), seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4891769b",
   "metadata": {},
   "source": [
    "Fit the preprocessing pipeline (tokenization, stopword removal) on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a49d3989-9eb1-4ba6-a918-c60386b73f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocessingModel: org.apache.spark.ml.PipelineModel = pipeline_a9b6b4373fe7\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val preprocessingModel = pipelinePreprocessing.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c4f4a6",
   "metadata": {},
   "source": [
    "Apply preprocessing pipeline on training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79f1f819-8b7c-4dcc-9f14-2c6da5021467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocessedTrainining: org.apache.spark.sql.DataFrame = [terms: array<string>, category: string]\n",
       "preprocessedTest: org.apache.spark.sql.DataFrame = [terms: array<string>, category: string]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val preprocessedTrainining = preprocessingModel.transform(training).select(remover.getOutputCol, \"category\")\n",
    "val preprocessedTest = preprocessingModel.transform(test).select(remover.getOutputCol, \"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10eee0",
   "metadata": {},
   "source": [
    "This code persists the preprocessed training and test datasets to both memory and disk storage levels to optimize their availability for subsequent computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35a0585c-ce6d-42bd-ac99-2ec451eb92e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.storage.StorageLevel\n",
       "persistedProcessedTraining: preprocessedTrainining.type = [terms: array<string>, category: string]\n",
       "persistedProcessedTest: preprocessedTest.type = [terms: array<string>, category: string]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val persistedProcessedTraining = preprocessedTrainining.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "val persistedProcessedTest = preprocessedTest.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12d5ed-4d77-428f-8dac-d907b0c1700a",
   "metadata": {},
   "source": [
    "### Training classifier\n",
    "\n",
    "Now that the data is split into train and test sets and already persisted, this section will implement the feature extraction pipeline as well as the grid search. Following this, the grid search will be applied to the training set and validated on the validation set, where each individual model's parameters will be stored locally for further investigation. Finally, the best-performing model from the grid search will be applied to the test set, and the persisted training and test data will be unpersisted.\n",
    "\n",
    "Define L2 normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62177b92-a3dc-437f-beb6-ecf2ab6fe765",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Normalizer\n",
       "normalizer: org.apache.spark.ml.feature.Normalizer = Normalizer: uid=normalizer_55c2eb37dc15, p=2.0\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val normalizer = new Normalizer()\n",
    "  .setInputCol(selector.getOutputCol)\n",
    "  .setOutputCol(\"normFeatures\")\n",
    "  .setP(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd85a26",
   "metadata": {},
   "source": [
    "Define estimator and evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee6d8df1-aef0-4e3c-9131-84d8f9830e87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
       "lsvc: org.apache.spark.ml.classification.LinearSVC = linearsvc_55ffc87a2423\n",
       "classifier: org.apache.spark.ml.classification.OneVsRest = oneVsRest_117e1f890a1e\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
    "\n",
    "val lsvc = new LinearSVC()\n",
    "\n",
    "val classifier = new OneVsRest()\n",
    "    .setClassifier(lsvc)\n",
    "    .setFeaturesCol(normalizer.getOutputCol)\n",
    "    .setLabelCol(indexer.getOutputCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d0187",
   "metadata": {},
   "source": [
    "Create pipeline for feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffa2cead-793d-445e-b66f-3c5884ef6d57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineClassifier: org.apache.spark.ml.Pipeline = pipeline_d324b5769e4e\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelineClassifier = new Pipeline()\n",
    "    .setStages(Array(countVectorizer, idf, indexer, selector, normalizer, classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99c873",
   "metadata": {},
   "source": [
    "Define parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bb9825c-52d6-44bd-a1b4-f74457e008e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinearsvc_55ffc87a2423-maxIter: 10,\n",
       "\tchiSqSelector_e102c3526f1a-numTopFeatures: 20,\n",
       "\tlinearsvc_55ffc87a2423-regParam: 0.001,\n",
       "\tlinearsvc_55ffc87a2423-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_55ffc87a2423-maxIter: 50,\n",
       "\tchiSqSelector_e102c3526f1a-numTopFeatures: 20,\n",
       "\tlinearsvc_55ffc87a2423-regParam: 0.001,\n",
       "\tlinearsvc_55ffc87a2423-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_55ffc87a2423-maxIter: 10,\n",
       "\tchiSqSelector_e102c3526f1a-numTopFeatures: 20,\n",
       "\tlinearsvc_55ffc87a2423-regParam: 0.001,\n",
       "\tlinearsvc_55ffc87a2423-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_55ffc87a2423-maxIter: 50,\n",
       "\tchiSqSelector_e102c3526f1a-numTopFeatures: 20,\n",
       "\tlinearsvc_55ffc87a2423-regParam: 0.001,\n",
       "\tlinearsvc_55ffc87a2423-...\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(lsvc.maxIter, Array(10, 50))\n",
    "    .addGrid(lsvc.regParam, Array(0.001, 0.01, 0.1))\n",
    "    .addGrid(lsvc.standardization, Array(false, true))\n",
    "    .addGrid(selector.numTopFeatures, Array(20, 2000))\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7506a98c",
   "metadata": {},
   "source": [
    "Define evaluator and set metric to F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ef24d7c-c24c-4a3d-810d-ef1c1d411878",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "evaluater: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_fb9c3f850193, metricName=f1, metricLabel=0.0, beta=1.0, eps=1.0E-15\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n",
    "\n",
    "val evaluater = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(indexer.getOutputCol)\n",
    "    .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c994165",
   "metadata": {},
   "source": [
    "Define Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87fc1da1-ecd5-4c7c-ac33-9ac884a00085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_5803524faddd\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "    .setEstimator(pipelineClassifier)\n",
    "    .setEvaluator(evaluater)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setTrainRatio(0.8)\n",
    "    .setSeed(seed)\n",
    "    .setParallelism(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be442e1",
   "metadata": {},
   "source": [
    "Perform Grid Search on presisted training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b41670cb-0b90-4b7b-8586-a1f35a81002c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1773.4690883159637 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.TrainValidationSplitModel = TrainValidationSplitModel: uid=tvs_5803524faddd, bestModel=pipeline_d324b5769e4e, trainRatio=0.8\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val model = trainValidationSplit.fit(persistedProcessedTraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d0798-35b0-47ed-89e1-4b199ecfbe4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0904a6a7-46f4-49b5-a1e7-df95b5fe3e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [terms: array<string>, category: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(persistedProcessedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6f3076e-7b2f-4366-804f-218bff0fe2a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score = 0.47262102005903917\n"
     ]
    }
   ],
   "source": [
    "println(s\"F1-Score = ${evaluater.evaluate(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "70022b1d-f556-4081-9d5e-de07b4a4f62b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best binary classifier parameters with a F1-Score = 0.47262102005903917:\n",
      "  LVC = maxIter: 50, regParam: 0.001, standardization: false\n",
      "  ChiSqSelector = topNumFeatures: 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{OneVsRestModel, LinearSVCModel}\n",
       "bestModel: org.apache.spark.ml.PipelineModel = pipeline_d324b5769e4e\n",
       "bestClassifier: org.apache.spark.ml.classification.OneVsRestModel = OneVsRestModel: uid=oneVsRest_117e1f890a1e, classifier=linearsvc_55ffc87a2423, numClasses=22, numFeatures=2000\n",
       "bestBinaryClassifierModel: org.apache.spark.ml.classification.LinearSVCModel = LinearSVCModel: uid=linearsvc_55ffc87a2423, numClasses=2, numFeatures=2000\n",
       "bestSelector: org.apache.spark.ml.feature.ChiSqSelectorModel = ChiSqSelectorModel: uid=chiSqSelector_e102c3526f1a, numSelectedFeatures=2000\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{OneVsRestModel, LinearSVCModel}\n",
    "\n",
    "val bestModel = model.bestModel.asInstanceOf[PipelineModel]\n",
    "val bestClassifier = bestModel.stages.last.asInstanceOf[OneVsRestModel]\n",
    "val bestBinaryClassifierModel = bestClassifier.models.head.asInstanceOf[LinearSVCModel]\n",
    "val bestSelector = bestModel.stages(3).asInstanceOf[ChiSqSelectorModel]\n",
    "\n",
    "println(s\"Best binary classifier parameters with a F1-Score = ${evaluater.evaluate(predictions)}:\\n\" +\n",
    "  s\"  LVC = maxIter: ${bestBinaryClassifierModel.getMaxIter}, regParam: ${bestBinaryClassifierModel.getRegParam}, standardization: ${bestBinaryClassifierModel.getStandardization}\\n\" +\n",
    "  s\"  ChiSqSelector = topNumFeatures: ${bestSelector.getNumTopFeatures}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31807cb8",
   "metadata": {},
   "source": [
    "#### Evaluate Grid Search Models \n",
    "\n",
    "The following helper function merges the parameter map and validation metrics of the trainValidationSplit model, then writes them to the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9907e4a4-a096-4e17-a254-48ccc29a5846",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io.PrintWriter\n",
       "evaluateGridSearch: (paramMaps: Array[org.apache.spark.ml.param.ParamMap], validationMetrics: Array[Double], filePath: String)Unit\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "\n",
    "def evaluateGridSearch(paramMaps: Array[org.apache.spark.ml.param.ParamMap], validationMetrics: Array[Double], filePath: String) = {\n",
    "    val paramsAndMetrics = paramMaps.zip(validationMetrics)\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    writer.println(\"maxIter, NumTopFeatures, regParam, standardization, f1-score\")\n",
    "\n",
    "    paramsAndMetrics.foreach { case (paramMap, metric) =>\n",
    "        val maxIter = paramMap.get(lsvc.maxIter).head\n",
    "        val NumTopFeatures = paramMap.get(selector.numTopFeatures).head\n",
    "        val regParam = paramMap.get(lsvc.regParam).head\n",
    "        val standardization = paramMap.get(lsvc.standardization).head\n",
    "\n",
    "        writer.println(s\"$maxIter,$NumTopFeatures,$regParam,$standardization,${metric}\")\n",
    "    }\n",
    "\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bed2c5",
   "metadata": {},
   "source": [
    "Retrieve the parameter map of the estimator and validation metrics and write the results to the local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d57133c6-cc45-469f-a410-1a9d9980f75c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramMaps: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinearsvc_55ffc87a2423-maxIter: 10,\n",
       "\tchiSqSelector_e102c3526f1a-numTopFeatures: 20,\n",
       "\tlinearsvc_55ffc87a2423-regParam: 0.001,\n",
       "\tlinearsvc_55ffc87a2423-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_55ffc87a2423-maxIter: 50,\n",
       "\tchiSqSelector_e102c3526f1a-numTopFeatures: 20,\n",
       "\tlinearsvc_55ffc87a2423-regParam: 0.001,\n",
       "\tlinearsvc_55ffc87a2423-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_55ffc87a2423-maxIter: 10,\n",
       "\tchiSqSelector_e102c3526f1a-numTopFeatures: 20,\n",
       "\tlinearsvc_55ffc87a2423-regParam: 0.001,\n",
       "\tlinearsvc_55ffc87a2423-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_55ffc87a2423-maxIter: 50,\n",
       "\tchiSqSelector_e102c3526f1a-numTopFeatures: 20,\n",
       "\tlinearsvc_55ffc87a2423-regParam: 0.001,\n",
       "\tlinearsvc_55ffc87a2423-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_55ffc87a2423-...\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paramMaps = model.getEstimatorParamMaps\n",
    "val validationMetrics = model.validationMetrics\n",
    "\n",
    "evaluateGridSearch(paramMaps, validationMetrics, \"../grid_search_evaluation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c46b88a-e1cf-4732-bce3-9ca07d4f1561",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68fd8ced-c736-4f83-a399-61846800ca91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bestModelPipeline: org.apache.spark.ml.Pipeline = pipeline_244a9c59d8c7\n",
       "bestModelFull: org.apache.spark.ml.PipelineModel = pipeline_244a9c59d8c7\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bestModelPipeline = new Pipeline().setStages(bestModel.stages)\n",
    "val bestModelFull = bestModelPipeline.fit(persistedProcessedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86440995-26c3-455e-a04a-1e0069c326ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score full model = 0.47262102005903917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [terms: array<string>, category: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = bestModelFull.transform(persistedProcessedTest)\n",
    "println(s\"F1-Score full model = ${evaluater.evaluate(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a51eb-a8ee-4f22-9947-95308727c453",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b269cbaf-53de-48fc-8d18-b3212af18310",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: persistedProcessedTest.type = [terms: array<string>, category: string]\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistedProcessedTraining.unpersist()\n",
    "persistedProcessedTest.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
