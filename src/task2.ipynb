{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1715326141961_1483\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1715326141961_1483)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.feature.{StringIndexer, HashingTF, CountVectorizer, IDF, RegexTokenizer, StopWordsRemover, ChiSqSelector, Normalizer}\n",
       "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.util.DefaultParamsWritable\n",
       "import org.apache.spark.sql.functions.udf\n",
       "import org.apache.spark.ml.linalg.SparseVector\n",
       "import scala.io.Source.fromFile\n",
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.linalg.{Vector, SparseVector, DenseVector}\n",
       "import o...\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.ml.feature.{StringIndexer,HashingTF,CountVectorizer,IDF,RegexTokenizer,StopWordsRemover,ChiSqSelector,Normalizer}\n",
    "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.util.DefaultParamsWritable\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import scala.io.Source.fromFile\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.linalg.{Vector, SparseVector, DenseVector}\n",
    "import org.apache.spark.sql.functions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "sc: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@f41e81e\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val sc = SparkSession.builder\n",
    ".appName(\"SVM Text Classification\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_stopwords: String = Exercise_2/data/stopwords.txt\n",
       "k: Int = 75\n",
       "seed: Int = 42\n",
       "split_pattern: String = [^a-zA-Z<>^|]+\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "System.getProperty(\"user.dir\")\n",
    "\n",
    "// for execution using jupyter hub\n",
    "//val path_to_stopwords = \"../data/stopwords.txt\"\n",
    "\n",
    "// for execution us vs code\n",
    "val path_to_stopwords = \"Exercise_2/data/stopwords.txt\"\n",
    "\n",
    "val k = 75\n",
    "val seed = 42\n",
    "val split_pattern = \"[^a-zA-Z<>^|]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewsDF: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsDF = sc\n",
    ".read.json(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\")\n",
    ".select(\"category\",\"reviewText\")\n",
    ".groupBy(\"category\")\n",
    ".agg(concat_ws(\" \", collect_list(\"reviewText\")).alias(\"reviewText\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs, but, by, c, came, camera, can, cannot, cant, car, case, cause, causes, cd, certain, certainly, changes,...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopwords = fromFile(path_to_stopwords).getLines.toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_4660990c2e9b, minTokenLength=1, gaps=true, pattern=[^a-zA-Z<>^|]+, toLowercase=true\n",
       "stopWordsFile: String = Exercise_2/data/stopwords.txt\n",
       "stopwords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behi...\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer()\n",
    ".setInputCol(\"reviewText\")\n",
    ".setOutputCol(\"words\")\n",
    ".setPattern(split_pattern)\n",
    "\n",
    "val stopWordsFile = path_to_stopwords\n",
    "val stopwords = scala.io.Source.fromFile(stopWordsFile).getLines().toArray\n",
    "val remover = new StopWordsRemover()\n",
    ".setInputCol(tokenizer.getOutputCol)\n",
    ".setOutputCol(\"tokens\")\n",
    ".setStopWords(stopwords)\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    ".setInputCol(\"category\")\n",
    ".setOutputCol(\"label\")\n",
    "\n",
    "val countVectorizer = new CountVectorizer()\n",
    ".setInputCol(\"tokens\")\n",
    ".setOutputCol(\"rawFeatures\")\n",
    "\n",
    "val idf = new IDF()\n",
    ".setInputCol(\"rawFeatures\")\n",
    ".setOutputCol(\"features\")\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    ".setNumTopFeatures(75)\n",
    ".setFeaturesCol(\"features\")\n",
    ".setLabelCol(\"label\")\n",
    ".setOutputCol(\"selectedFeatures\")\n",
    "\n",
    "val preprocessing = new Pipeline().setStages(Array(tokenizer, remover, indexer, countVectorizer, idf, selector))\n",
    "val preprocessing_model = preprocessing.fit(reviewsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [category: string, selectedFeatures: vector]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = preprocessing_model.transform(reviewsDF).select(\"category\",\"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode selected tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countVectorizerModel: org.apache.spark.ml.feature.CountVectorizerModel = CountVectorizerModel: uid=cntVec_9974e5761bdb, vocabularySize=96129\n",
       "vocabulary: Array[String] = Array(great, good, love, time, work, recommend, back, easy, make, bought, made, find, buy, price, put, reading, quality, people, works, quot, years, nice, characters, long, series, lot, found, author, day, bit, feel, makes, thing, perfect, fit, end, set, loved, things, thought, music, small, hard, give, year, world, size, worth, pretty, times, sound, written, light, real, big, amazon, part, bad, highly, money, excellent, purchased, happy, high, enjoyed, problem, family, interesting, wanted, character, job, review, purchase, man, watch, days, enjoy, place, home, stars, short, writing, play, cover, top, fan, full, fine, co...\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Extract model and get the vocabulary\n",
    "val countVectorizerModel = preprocessing_model.stages(3).asInstanceOf[CountVectorizerModel]\n",
    "val vocabulary = countVectorizerModel.vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4973/1996645485@19228148,StringType,List(Some(class[value[0]: vector])),Some(class[value[0]: string]),None,true,true)\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// UDF to map output\n",
    "val outputUDF = udf { (features: SparseVector) =>\n",
    "  // features.indices, features.values\n",
    "  \n",
    "  val words = features.indices.map(vocabulary)\n",
    "  val values = features.values\n",
    "  val sortedIndicesValues = words.zip(values).sortBy(-_._2) // Sort by values in descending order\n",
    "  val outputString = sortedIndicesValues.map { case (word, value) => f\"$word:${\"%.2f\".format(value)}\" }.mkString(\", \")\n",
    "  outputString\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            category|           top_terms|\n",
      "+--------------------+--------------------+\n",
      "|    Apps_for_Android|made:19.83, makes...|\n",
      "|          Automotive|loved:6.85, autho...|\n",
      "|                Baby|loved:29.35, easy...|\n",
      "|              Beauty|written:13.84, bu...|\n",
      "|                Book|great:119.40, bac...|\n",
      "|       CDs_and_Vinyl|made:24.47, find:...|\n",
      "|Cell_Phones_and_A...|love:32.09, easy:...|\n",
      "|Clothing_Shoes_an...|good:63.97, time:...|\n",
      "|       Digital_Music|find:3.16, made:2...|\n",
      "|          Electronic|love:47.34, bad:3...|\n",
      "|Grocery_and_Gourm...|written:11.74, wo...|\n",
      "|Health_and_Person...|written:19.71, bu...|\n",
      "|     Home_and_Kitche|easy:22.20, good:...|\n",
      "|        Kindle_Store|great:10.58, love...|\n",
      "|       Movies_and_TV|great:14.09, made...|\n",
      "|  Musical_Instrument|made:4.82, world:...|\n",
      "|      Office_Product|day:10.71, size:1...|\n",
      "|Patio_Lawn_and_Garde|easy:6.00, writte...|\n",
      "|         Pet_Supplie|good:5.42, easy:5...|\n",
      "|  Sports_and_Outdoor|world:19.30, good...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.sql.DataFrame = [category: string, top_terms: string]\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = df\n",
    ".withColumn(\"top_terms\", outputUDF(col(\"selectedFeatures\")))\n",
    ".orderBy(asc(\"category\"))\n",
    ".select(\"category\",\"top_terms\")\n",
    "\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Output File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{Row, Dataset}\n",
       "import scala.collection.immutable.TreeSet\n",
       "import java.io.PrintWriter\n",
       "writeDFToFile: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], filePath: String)Unit\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Row, Dataset}\n",
    "import scala.collection.immutable.TreeSet\n",
    "import java.io.PrintWriter\n",
    "\n",
    "\n",
    "def writeDFToFile(df: Dataset[Row], filePath: String) = {\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    val collectedData = df.collect()\n",
    "\n",
    "    for (row <- collectedData) {\n",
    "        val category = row.getString(0)\n",
    "        val top_terms = row.getString(1)  // Explicitly specify the type as Vector\n",
    "        \n",
    "        writer.println(f\"<$category> $top_terms\")\n",
    "    }\n",
    "    \n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDFToFile(result, \"Exercise_2/data/output_ds.txt\") //Exercise_2/data/output_ds.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
