{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1715326141961_1521\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1715326141961_1521)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.ml.feature.{StringIndexer, HashingTF, CountVectorizer, IDF, RegexTokenizer, StopWordsRemover, ChiSqSelector, Normalizer}\n",
       "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.util.DefaultParamsWritable\n",
       "import org.apache.spark.sql.functions.udf\n",
       "import scala.io.Source.fromFile\n",
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.ml.linalg.{Vector, SparseVector, DenseVector}\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.ml.feature.{StringIndexer,HashingTF,CountVectorizer,IDF,RegexTokenizer,StopWordsRemover,ChiSqSelector,Normalizer}\n",
    "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.util.DefaultParamsWritable\n",
    "import org.apache.spark.sql.functions.udf\n",
    "import scala.io.Source.fromFile\n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.linalg.{Vector, SparseVector, DenseVector}\n",
    "import org.apache.spark.sql.functions._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "sc: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3832c56c\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val sc = SparkSession.builder\n",
    ".appName(\"SVM Text Classification\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_stopwords: String = Exercise_2/data/stopwords.txt\n",
       "k: Int = 75\n",
       "seed: Int = 42\n",
       "split_pattern: String = [^a-zA-Z<>^|]+\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "System.getProperty(\"user.dir\")\n",
    "\n",
    "// for execution using jupyter hub\n",
    "//val path_to_stopwords = \"../data/stopwords.txt\"\n",
    "\n",
    "// for execution us vs code\n",
    "val path_to_stopwords = \"Exercise_2/data/stopwords.txt\"\n",
    "\n",
    "val k = 75\n",
    "val seed = 42\n",
    "val split_pattern = \"[^a-zA-Z<>^|]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewsDF: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsDF = sc\n",
    ".read.json(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\")\n",
    ".select(\"category\",\"reviewText\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs, but, by, c, came, camera, can, cannot, cant, car, case, cause, causes, cd, certain, certainly, changes,...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopwords = fromFile(path_to_stopwords).getLines.toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_241035bd8a25, minTokenLength=1, gaps=true, pattern=[^a-zA-Z<>^|]+, toLowercase=true\n",
       "stopWordsFile: String = Exercise_2/data/stopwords.txt\n",
       "stopwords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behi...\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer()\n",
    ".setInputCol(\"reviewText\")\n",
    ".setOutputCol(\"words\")\n",
    ".setPattern(split_pattern)\n",
    "\n",
    "val stopWordsFile = path_to_stopwords\n",
    "val stopwords = scala.io.Source.fromFile(stopWordsFile).getLines().toArray\n",
    "val remover = new StopWordsRemover()\n",
    ".setInputCol(tokenizer.getOutputCol)\n",
    ".setOutputCol(\"tokens\")\n",
    ".setStopWords(stopwords)\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    ".setInputCol(\"category\")\n",
    ".setOutputCol(\"label\")\n",
    "\n",
    "val countVectorizer = new CountVectorizer()\n",
    ".setInputCol(\"tokens\")\n",
    ".setOutputCol(\"rawFeatures\")\n",
    "\n",
    "val idf = new IDF()\n",
    ".setInputCol(\"rawFeatures\")\n",
    ".setOutputCol(\"features\")\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    ".setNumTopFeatures(75)\n",
    ".setFeaturesCol(\"features\")\n",
    ".setLabelCol(\"label\")\n",
    ".setOutputCol(\"selectedFeatures\")\n",
    "\n",
    "val preprocessing = new Pipeline().setStages(Array(tokenizer, remover, indexer, countVectorizer, idf, selector))\n",
    "val preprocessing_model = preprocessing.fit(reviewsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [category: string, selectedFeatures: vector]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = preprocessing_model.transform(reviewsDF).select(\"category\",\"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            category|    selectedFeatures|\n",
      "+--------------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|(75,[2,3,7,8,35],...|\n",
      "|Patio_Lawn_and_Garde|(75,[0,1,3,21,39]...|\n",
      "|Patio_Lawn_and_Garde|(75,[4,10],[2.443...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode selected tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countVectorizerModel: org.apache.spark.ml.feature.CountVectorizerModel = CountVectorizerModel: uid=cntVec_62a354bf03d6, vocabularySize=96129\n",
       "vocabulary: Array[String] = Array(great, good, love, time, work, recommend, back, easy, make, bought, made, find, buy, price, put, reading, quality, people, works, quot, years, nice, characters, long, series, lot, found, author, day, bit, feel, makes, thing, perfect, fit, end, set, loved, things, thought, music, small, hard, give, year, world, size, worth, pretty, times, sound, written, light, real, big, amazon, part, bad, highly, money, excellent, purchased, happy, high, enjoyed, problem, family, interesting, wanted, character, job, review, purchase, man, watch, days, enjoy, place, home, stars, short, writing, play, cover, top, fan, full, fine, co...\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Extract model and get the vocabulary\n",
    "val countVectorizerModel = preprocessing_model.stages(3).asInstanceOf[CountVectorizerModel]\n",
    "val vocabulary = countVectorizerModel.vocabulary\n",
    "\n",
    "// UDF to map output\n",
    "val encodeSelectedFeaturesUDF = udf { (features: SparseVector) =>\n",
    "  // features.indices, features.values\n",
    "  \n",
    "  val words = features.indices.map(vocabulary)\n",
    "  words\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Chi Square Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenFreqByCategory: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, token: string ... 1 more field]\n",
       "t_total_number_of_occurrences: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [token: string, total_number_of_occurrences: bigint]\n",
       "n_docs_by_cat: org.apache.spark.sql.DataFrame = [category: string, n_docs_by_cat: bigint]\n",
       "n_of_docs: org.apache.spark.sql.DataFrame = [N: bigint]\n",
       "crossjoin_n_info: org.apache.spark.sql.DataFrame = [category: string, n_docs_by_cat: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 1: for token t number of occurrens within each category -> A\n",
    "val tokenFreqByCategory = df\n",
    ".withColumn(\"tokens\", encodeSelectedFeaturesUDF(col(\"selectedFeatures\")))\n",
    ".withColumn(\"token\", explode(col(\"tokens\")))\n",
    ".groupBy(\"category\", \"token\")\n",
    ".count()\n",
    ".withColumnRenamed(\"count\", \"A\").orderBy(desc(\"A\"))\n",
    "\n",
    "// Step 2: for token t total number of occurrencs across all categories -> B = this - A\n",
    "val t_total_number_of_occurrences = tokenFreqByCategory\n",
    ".groupBy(\"token\")\n",
    ".agg(sum(\"A\").alias(\"total_number_of_occurrences\")).orderBy(desc(\"total_number_of_occurrences\"))\n",
    "\n",
    "// Step 3: number of reviews by category\n",
    "val n_docs_by_cat = df\n",
    ".groupBy(\"category\")\n",
    ".agg(count(\"*\")\n",
    ".as(\"n_docs_by_cat\")) // C = this - A\n",
    "\n",
    "// Step 4: total number of reviews\n",
    "val n_of_docs = n_docs_by_cat.agg(sum(\"n_docs_by_cat\").alias(\"N\"))\n",
    "\n",
    "// join both dataframes\n",
    "val crossjoin_n_info = n_docs_by_cat.crossJoin(n_of_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calucate Chi Squared Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSquaredValues: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, token: string ... 1 more field]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chiSquaredValues = tokenFreqByCategory\n",
    ".join(t_total_number_of_occurrences, (\"token\"))\n",
    ".join(crossjoin_n_info, (\"category\"))\n",
    ".withColumn(\"B\", $\"total_number_of_occurrences\" - $\"A\")\n",
    ".withColumn(\"C\", $\"n_docs_by_cat\" - $\"A\")\n",
    ".withColumn(\"D\", $\"N\" - $\"A\" - $\"B\" - $\"C\")\n",
    ".withColumn(\"D\", $\"N\" - $\"A\" - $\"B\" - $\"C\")\n",
    ".withColumn(\"chisquared\",\n",
    "  ($\"N\" * pow($\"A\" * $\"D\" - $\"B\" * $\"C\", 2)) /\n",
    "    (($\"A\" + $\"B\") * ($\"A\" + $\"C\") * ($\"B\" + $\"D\") * ($\"C\" + $\"D\"))\n",
    ")\n",
    ".select(\"category\", \"token\",\"chisquared\").orderBy(desc(\"chisquared\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------------------+----+\n",
      "|        category|     token|        chisquared|rank|\n",
      "+----------------+----------+------------------+----+\n",
      "|Apps_for_Android|       man| 2158.369406820129|   1|\n",
      "|Apps_for_Android|    bought| 174.4587211734982|   2|\n",
      "|Apps_for_Android|     price| 149.5959088208227|   3|\n",
      "|Apps_for_Android|   quality|144.19395657092497|   4|\n",
      "|Apps_for_Android|       lot|135.34205354169052|   5|\n",
      "|Apps_for_Android|     makes|126.10402357270085|   6|\n",
      "|Apps_for_Android|     years|124.35784441658687|   7|\n",
      "|Apps_for_Android|   reading|121.65120689569069|   8|\n",
      "|Apps_for_Android|      made|120.91456375779855|   9|\n",
      "|Apps_for_Android|characters|118.09788512184659|  10|\n",
      "|Apps_for_Android|      long|115.05716592780018|  11|\n",
      "|Apps_for_Android|      size|114.84060961362133|  12|\n",
      "|Apps_for_Android|      back| 99.12491188526246|  13|\n",
      "|Apps_for_Android|     thing| 95.24822201431748|  14|\n",
      "|Apps_for_Android|      hard| 93.34366680110551|  15|\n",
      "|Apps_for_Android|       put| 91.54071641565497|  16|\n",
      "|Apps_for_Android|    author| 85.36280524819963|  17|\n",
      "|Apps_for_Android|      part| 83.61050330354685|  18|\n",
      "|Apps_for_Android|    things| 81.52586411099587|  19|\n",
      "|Apps_for_Android|      feel| 78.42844012421912|  20|\n",
      "+----------------+----------+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.expressions.Window\n",
       "windowSpec: org.apache.spark.sql.expressions.WindowSpec = org.apache.spark.sql.expressions.WindowSpec@38217c4a\n",
       "top75ByCategory: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, token: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Top 75\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "val windowSpec = Window.partitionBy(\"category\").orderBy(desc(\"chisquared\"))\n",
    "\n",
    "val top75ByCategory = chiSquaredValues\n",
    ".withColumn(\"rank\", row_number().over(windowSpec))\n",
    ".filter(col(\"rank\") <= 75)\n",
    "//.withColumn(\"value_str\", col(\"chisquared\").cast(\"string\"))\n",
    "\n",
    "\n",
    "top75ByCategory.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            category|          top_tokens|\n",
      "+--------------------+--------------------+\n",
      "|    Apps_for_Android|[man:2158.3694068...|\n",
      "|          Automotive|[makes:345.042680...|\n",
      "|                Baby|[easy:111.8953956...|\n",
      "|              Beauty|[reading:117.5479...|\n",
      "|                Book|[reading:6184.609...|\n",
      "|       CDs_and_Vinyl|[loved:13083.7708...|\n",
      "|Cell_Phones_and_A...|[watch:446.765539...|\n",
      "|Clothing_Shoes_an...|[hard:3312.666743...|\n",
      "|       Digital_Music|[loved:1420.97017...|\n",
      "|          Electronic|[works:1800.79675...|\n",
      "|Grocery_and_Gourm...|[sound:118.998168...|\n",
      "|Health_and_Person...|[lot:153.96737881...|\n",
      "|     Home_and_Kitche|[characters:246.5...|\n",
      "|        Kindle_Store|[lot:1079.2803955...|\n",
      "|       Movies_and_TV|[interesting:4310...|\n",
      "|  Musical_Instrument|[world:491.957905...|\n",
      "|      Office_Product|[price:111.146121...|\n",
      "|Patio_Lawn_and_Garde|[works:66.7466270...|\n",
      "|         Pet_Supplie|[reading:60.28768...|\n",
      "|  Sports_and_Outdoor|[makes:340.676944...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "output: org.apache.spark.sql.DataFrame = [category: string, top_tokens: array<string>]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val output = top75ByCategory\n",
    ".groupBy(\"category\")\n",
    ".agg(collect_list(concat_ws( \":\", $\"token\", $\"chisquared\")) as \"top_tokens\")\n",
    "\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{Row, Dataset}\n",
       "import scala.collection.immutable.TreeSet\n",
       "import java.io.PrintWriter\n",
       "writeDFToFile: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], filePath: String)Unit\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Row, Dataset}\n",
    "import scala.collection.immutable.TreeSet\n",
    "import java.io.PrintWriter\n",
    "\n",
    "\n",
    "def writeDFToFile(df: Dataset[Row], filePath: String) = {\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    val collectedData = df.collect()\n",
    "\n",
    "    for (row <- collectedData) {\n",
    "        val category = row.getString(0)\n",
    "        val top_terms = row.getSeq(1).mkString(\", \")  // Explicitly specify the type as Vector\n",
    "        \n",
    "        writer.println(f\"<$category> $top_terms\")\n",
    "    }\n",
    "    \n",
    "    writer.close()\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeDFToFile(output, \"Exercise_2/data/output_ds_group_last.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
