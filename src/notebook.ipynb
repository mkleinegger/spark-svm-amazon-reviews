{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Text Processing and Classification using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "import re\n",
    "\n",
    "spark: SparkSession = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/stopwords.txt') as f:\n",
    "    stopwords = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.11 ms, sys: 2.1 ms, total: 4.2 ms\n",
      "Wall time: 788 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "K = 75\n",
    "FILE_PATH = 'hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json' # devset\n",
    "# FILE_PATH = 'hdfs:///user/dic24_shared/amazon-reviews/full/reviewscombined.json' # full dataset\n",
    "\n",
    "rdd = spark.read.json(FILE_PATH).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 µs, sys: 0 ns, total: 23 µs\n",
      "Wall time: 26.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def preprocessing(row):\n",
    "    category = row['category']\n",
    "\n",
    "    # lower text\n",
    "    # tokenises each line by using whitespaces, tabs, digits, and the characters ()[]{}.!?,;:+=-_\"'`~#@&*%€$§\\/ as delimiters\n",
    "    tokens = re.split(r'[^a-zA-Z<>^|]+', row['reviewText'].lower())\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = filter(lambda token: len(token) > 1 and (token not in stopwords), tokens)\n",
    "\n",
    "    # remove duplicates\n",
    "    tokens = set(tokens)\n",
    "\n",
    "    # count all documents in category\n",
    "    yield (category, None), 1\n",
    "\n",
    "    # count all documents in category containing token\n",
    "    for token in tokens:\n",
    "        yield (category, token), 1\n",
    "\n",
    "\n",
    "def token_to_key(row):\n",
    "    (category, token), count = row\n",
    "    return token, (category, count)\n",
    "\n",
    "\n",
    "def token_sum(row):\n",
    "    token, values = row\n",
    "    counts = {category: count for category, count in values}\n",
    "    n_t = sum(counts.values())\n",
    "\n",
    "    for category, count in counts.items():\n",
    "        yield category, (token, count, n_t)\n",
    "\n",
    "\n",
    "def chi_squared(row):\n",
    "    category, values = row\n",
    "\n",
    "    # dictionary of tokens with their counts and total number of documents\n",
    "    counts = {token: (count, n_t) for token, count, n_t in values}\n",
    "    \n",
    "    # total number of documents in category and dataset\n",
    "    n_c, n = counts.pop(None)\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for token, (a, n_t) in counts.items():\n",
    "        b = n_t - a\n",
    "        c = n_c - a\n",
    "        d = n - a - b - c\n",
    "\n",
    "        chi_squared = n * ((a * d - b * c) ** 2) / ((a + b) * (a + c) * (b + d) * (c + d))\n",
    "        result.append((chi_squared, token))\n",
    "\n",
    "    return category, sorted(result, key=lambda x: (-x[0], x[1]))[:K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.1 ms, sys: 13.8 ms, total: 46.9 ms\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topk = rdd.flatMap(preprocessing) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .map(token_to_key) \\\n",
    "    .groupByKey() \\\n",
    "    .flatMap(token_sum) \\\n",
    "    .groupByKey() \\\n",
    "    .map(chi_squared) \\\n",
    "    .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 1.62 ms, total: 12.8 ms\n",
      "Wall time: 740 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('../output_rdd.txt', 'w') as f:\n",
    "    tokens = set()\n",
    "\n",
    "    for category, values in topk.toLocalIterator():\n",
    "        tokens.update(map(lambda x: x[1], values))\n",
    "        value_strings = [f'{value[1]}:{value[0]}' for value in values]\n",
    "        print(' '.join([f'<{category}>'] + value_strings), file=f)\n",
    "\n",
    "    print(' '.join(sorted(tokens)), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
