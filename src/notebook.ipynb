{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f7703a-8aa3-47c9-8461-670f297d9fb1",
   "metadata": {},
   "source": [
    "# Amazon-reviews predictions with Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06e6d449-425b-4222-967e-d082d7692b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.SparkConf\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@3dc15ebc\n",
       "sc: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@53422aa9\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "val conf = new SparkConf()\n",
    "      .setMaster(\"yarn\")\n",
    "      .set(\"spark.executor.memory\", \"4g\")\n",
    "      .set(\"spark.driver.memory\", \"4g\")\n",
    "      .set(\"spark.driver.maxResultSize\", \"2g\")\n",
    "      .set(\"spark.executor.instances\", \"5\")\n",
    "      .set(\"spark.executor.cores\", \"4\")\n",
    "      .set(\"spark.default.parallelism\", \"20\")\n",
    "\n",
    "// Initialize SparkSession\n",
    "val sc = SparkSession.builder.config(conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73583dd-e37b-4611-8f50-484cb7ba8bed",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read data and transform to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61608eac-7522-4ac5-8f13-6ff8643f1507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K: Int = 75\n",
       "file_path_stopwords: String = ../data/stopwords.txt\n",
       "file_path_reviews: String = hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\n",
       "tokenizePattern: String = [^a-zA-Z<>^|]+\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val K = 75\n",
    "val file_path_stopwords = \"../data/stopwords.txt\"\n",
    "val file_path_reviews = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "// val file_path_reviews = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "\n",
    "val tokenizePattern = \"[^a-zA-Z<>^|]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "17ededeb-ae72-4a27-83a6-f3a17f4b5a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.6782345771789551 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val df = sc.read.json(file_path_reviews).select(\"category\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28fa0346-2df6-4f93-8577-13adf01fe6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source.fromFile\n",
       "stopWords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs, but, by, c, came, camera, can, cannot, cant, car, case, cause, causes, ...\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source.fromFile\n",
    "\n",
    "val stopWords = fromFile(file_path_stopwords).getLines.toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195030e1-bb58-4186-b616-54d4767b8f20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d04b92f-8975-4983-a0d1-78eaa9a3c915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io.PrintWriter\n",
       "import org.apache.spark.rdd.RDD\n",
       "import scala.collection.immutable.TreeSet\n",
       "writeRDDToFile: (rdd: org.apache.spark.rdd.RDD[(String, Seq[(String, Double)])], filePath: String)Unit\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "import org.apache.spark.rdd.RDD\n",
    "import scala.collection.immutable.TreeSet\n",
    "\n",
    "def writeRDDToFile(rdd: RDD[(String, Seq[(String, Double)])], filePath: String): Unit = {\n",
    "    var mergedTerms = TreeSet[String]()\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    val collectedData = rdd.sortByKey().collect()\n",
    "\n",
    "    for ((category, topk) <- collectedData) {\n",
    "        val topKStr = topk.map { case (term, chiSquared) => \n",
    "            mergedTerms += term\n",
    "            s\"$term:$chiSquared\"\n",
    "        }.mkString(\" \")\n",
    "        writer.println(s\"<$category> $topKStr\")\n",
    "    }\n",
    "    \n",
    "    writer.print(mergedTerms.mkString(\" \"))\n",
    "    \n",
    "    writer.close()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82aec3a0-0f39-44a4-a064-f60058d30e19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{Row, Dataset}\n",
       "import scala.collection.immutable.TreeSet\n",
       "import java.io.PrintWriter\n",
       "writeDFToFile: (df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], filePath: String)Unit\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{Row, Dataset}\n",
    "import scala.collection.immutable.TreeSet\n",
    "import java.io.PrintWriter\n",
    "\n",
    "def writeDFToFile(df: Dataset[Row], filePath: String) = {\n",
    "    var mergedTerms = TreeSet[String]()\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    \n",
    "    val collectedData = df.collect()\n",
    "\n",
    "    for (row <- collectedData) {\n",
    "      val category = row.getString(0)\n",
    "      val topk = row.getAs [Seq[Seq[String]]](\"topk\").map({\n",
    "        case Seq(token, chiSquared) => { \n",
    "            mergedTerms += token\n",
    "            s\"$token:$chiSquared\" \n",
    "        }\n",
    "      }).mkString(\" \")\n",
    "\n",
    "      writer.println(f\"<$category> $topk\")\n",
    "    }\n",
    "    writer.print(mergedTerms.mkString(\" \"))\n",
    "    \n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a17fb-63a4-46e0-9748-e37714fb5364",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Calculate Chi-Square\n",
    "This approaches, first calculates the different number of documents per category and afterward calculates the chi-squared values per term per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d14419-d35e-4d0f-abd0-5c546e51d7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[8] at rdd at <console>:34\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5376711-f0d9-45df-8ad7-08d8659f759f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 4.964831113815308 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "counts: scala.collection.Map[String,Long] = Map(Patio_Lawn_and_Garde -> 994, Movies_and_TV -> 4607, Electronic -> 7825, Office_Product -> 1243, Tools_and_Home_Improvement -> 1926, Kindle_Store -> 3205, Home_and_Kitche -> 4254, Digital_Music -> 836, Automotive -> 1374, Grocery_and_Gourmet_Food -> 1297, Baby -> 916, Book -> 22507, Clothing_Shoes_and_Jewelry -> 5749, Toys_and_Game -> 2253, Health_and_Personal_Care -> 2982, Sports_and_Outdoor -> 3269, Beauty -> 2023, CDs_and_Vinyl -> 3749, Musical_Instrument -> 500, Cell_Phones_and_Accessorie -> 3447, Apps_for_Android -> 2638, Pet_Supplie -> 1235)\n",
       "N: Long = 78829\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val counts = df.rdd.map(row => (row.getString(0), 1)).countByKey()\n",
    "val N = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e902d564-b646-40cd-a72d-851b252cbdf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.35018062591552734 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countsAsMap: scala.collection.immutable.Map[String,Long] = Map(Patio_Lawn_and_Garde -> 994, Movies_and_TV -> 4607, Electronic -> 7825, Office_Product -> 1243, Tools_and_Home_Improvement -> 1926, Kindle_Store -> 3205, Home_and_Kitche -> 4254, Digital_Music -> 836, Automotive -> 1374, Grocery_and_Gourmet_Food -> 1297, Baby -> 916, Book -> 22507, Clothing_Shoes_and_Jewelry -> 5749, Toys_and_Game -> 2253, Health_and_Personal_Care -> 2982, Sports_and_Outdoor -> 3269, Beauty -> 2023, CDs_and_Vinyl -> 3749, Musical_Instrument -> 500, Cell_Phones_and_Accessorie -> 3447, Apps_for_Android -> 2638, Pet_Supplie -> 1235)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val countsAsMap = counts.toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e4b94c-ce28-4aab-94cd-55be2193dbd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.2302906513214111 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "preprocessedRDD: org.apache.spark.rdd.RDD[(String, Array[String])] = MapPartitionsRDD[20] at map at <console>:41\n",
       "termCategoryCounts: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[24] at map at <console>:47\n",
       "chiSquaredValues: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[26] at flatMapValues at <console>:51\n",
       "topTermsPerCategory: org.apache.spark.rdd.RDD[(String, Seq[(String, Double)])] = MapPartitionsRDD[29] at mapValues at <console>:66\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val preprocessedRDD = rdd\n",
    "    .map(row => (row.getString(0), row.getString(1).toLowerCase().split(tokenizePattern).distinct))\n",
    "    .map(row => (row._1, row._2.filter(w => w.length > 1 && !stopWords.contains(w))))\n",
    "\n",
    "val termCategoryCounts = preprocessedRDD\n",
    "    .flatMapValues(terms => terms)\n",
    "    .map({ case (category, term) => ((category, term), 1) })\n",
    "    .reduceByKey(_ + _)\n",
    "    .map({ case ((category, term), count) => (term, (category, count)) })\n",
    "\n",
    "val chiSquaredValues = termCategoryCounts\n",
    "    .groupByKey()\n",
    "    .flatMapValues({ categoryCounts =>\n",
    "        val n_t = categoryCounts.map(row => row._2).sum\n",
    "        categoryCounts.map({ case (category, count) =>\n",
    "            val A = count\n",
    "            val B = n_t - A\n",
    "            val C = countsAsMap(category) - A\n",
    "            val D = N - A - B - C\n",
    "            val chiSquared = (N * math.pow((A * D) - (B * C), 2)) / ((A + B) * (A + C) * (B + D) * (C + D))\n",
    "            (category, chiSquared)\n",
    "        })\n",
    "      })\n",
    "   \n",
    "val topTermsPerCategory = chiSquaredValues\n",
    "    .map({ case (term, (category, chiSquared)) => (category, (term, chiSquared)) })\n",
    "    .groupByKey()\n",
    "    .mapValues(_.toSeq.sortBy(-_._2).take(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abed58fc-7e32-4393-bb2a-c4cd009376db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27.114131927490234 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "writeRDDToFile(topTermsPerCategory, \"../output_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e679884",
   "metadata": {},
   "source": [
    "### Second approach\n",
    "In this approach we calculate the number of documents per category \"on the fly\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ee8bb4e-b8b1-4c96-bd35-e629458ba6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.7137079238891602 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "preprocessing: (row: org.apache.spark.sql.Row)Seq[((String, Option[String]), Int)]\n",
       "tokenToKey: (row: ((String, Option[String]), Int))(Option[String], (String, Int))\n",
       "tokenSum: (row: (Option[String], Iterable[(String, Int)]))Iterable[(String, (Option[String], Int, Int))]\n",
       "chiSquared: (row: (String, Iterable[(Option[String], Int, Int)]))(String, Seq[(String, Double)])\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "def preprocessing(row: Row): Seq[((String, Option[String]), Int)] = {\n",
    "  val category = row.getString(0)\n",
    "  val reviewText = row.getString(1)\n",
    "\n",
    "  val terms = reviewText\n",
    "    .toLowerCase()\n",
    "    .split(tokenizePattern)\n",
    "    .filter(token => token.length > 1 && !stopWords.contains(token))\n",
    "    .toSet\n",
    "\n",
    "  val counts = Seq(((category, None), 1)) ++ terms.map(token => ((category, Some(token)), 1))\n",
    "  counts.toSeq\n",
    "}\n",
    "\n",
    "def tokenToKey(row: ((String, Option[String]), Int)): (Option[String], (String, Int)) = {\n",
    "  val ((category, token), count) = row\n",
    "  (token, (category, count))\n",
    "}\n",
    "\n",
    "def tokenSum(row: (Option[String], Iterable[(String, Int)])): Iterable[(String, (Option[String], Int, Int))] = {\n",
    "  val (token, values) = row\n",
    "  val counts = values.groupBy(_._1).mapValues(_.map(_._2).sum)\n",
    "  val n_t = counts.values.sum\n",
    "\n",
    "  counts.map { case (category, count) => (category, (token, count, n_t)) }\n",
    "}\n",
    "\n",
    "def chiSquared(row: (String, Iterable[(Option[String], Int, Int)])): (String, Seq[(String, Double)]) = {\n",
    "  val (category, values) = row\n",
    "  val counts = values.map { case (token, count, n_t) => token -> (count, n_t) }.toMap\n",
    "  val (n_c, n) = counts.getOrElse(None, (0, counts.values.map(_._2).sum))\n",
    "\n",
    "  val results = counts\n",
    "    .collect {\n",
    "      case (Some(token), (a, n_t)) =>\n",
    "        val b = n_t - a\n",
    "        val c = n_c - a\n",
    "        val d = n - a - b - c\n",
    "        val chiSquaredValue = n.toDouble * math.pow(a * d - b * c, 2) / ((a + b).toDouble * (a + c) * (b + d) * (c + d))\n",
    "      \n",
    "        (token, chiSquaredValue)\n",
    "    }\n",
    "    .toSeq\n",
    "    .sortBy(-_._2)\n",
    "    .take(K)\n",
    "\n",
    "  (category, results)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363a0ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 27.783408880233765 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topTermsPerCategory: org.apache.spark.rdd.RDD[(String, Seq[(String, Double)])] = ShuffledRDD[42] at sortByKey at <console>:47\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val topTermsPerCategory = rdd\n",
    "  .flatMap(preprocessing)\n",
    "  .reduceByKey(_ + _)\n",
    "  .map(tokenToKey)\n",
    "  .groupByKey()\n",
    "  .flatMap(tokenSum)\n",
    "  .groupByKey()\n",
    "  .map(chiSquared)\n",
    "  .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e4f313f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.1373136043548584 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "writeRDDToFile(topTermsPerCategory, \"../output_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ab42d-f2e5-4194-8d2f-5194f67a1b7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets/DataFrames: Spark ML and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74ef30-43da-42a8-909f-801d904cdffe",
   "metadata": {},
   "source": [
    "First, create all the necessary transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4ecf763-118c-4101-82ba-ed8c66acade4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{StringIndexer, RegexTokenizer, StopWordsRemover}\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
       "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_083ba66f52ea\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_fe9487f58d14, minTokenLength=2, gaps=true, pattern=[^a-zA-Z<>^|]+, toLowercase=true\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = StopWordsRemover: uid=stopWords_e02c4cdc50a1, numStopWords=596, locale=en_US, caseSensitive=false\n",
       "countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_c0a25880bfff\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_9dba73983742\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{StringIndexer, RegexTokenizer, StopWordsRemover}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "    .setInputCol(\"category\")\n",
    "    .setOutputCol(\"category_index\")\n",
    "\n",
    "val tokenizer = new RegexTokenizer()\n",
    "    .setInputCol(\"reviewText\")\n",
    "    .setOutputCol(\"raw_terms\")\n",
    "    .setMinTokenLength(2)\n",
    "    .setPattern(tokenizePattern)\n",
    "    .setToLowercase(true)\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"terms\")\n",
    "    .setStopWords(stopWords)\n",
    "\n",
    "val countVectorizer = new CountVectorizer()\n",
    "    .setInputCol(remover.getOutputCol)\n",
    "    .setOutputCol(\"raw_features\")\n",
    "    .setMinDF(1)\n",
    "\n",
    "// Decide which frequencyCounter you want hashingTF vs cvModel?\n",
    "// val hashingTF = new HashingTF()\n",
    "//    .setInputCol(tokenizer.getOutputCol)\n",
    "//    .setOutputCol(\"raw_features\")\n",
    "\n",
    "val idf = new IDF()\n",
    "    .setInputCol(countVectorizer.getOutputCol)\n",
    "    .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35ba46-fd3a-45ba-b920-3cd4f6612b09",
   "metadata": {},
   "source": [
    "Afterward, we create the Chi^2-Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a4fdaab-8640-402d-b1f6-84d68a89dc10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{ChiSqSelector, ChiSqSelectorModel}\n",
       "selector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_93618e654f32\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{ChiSqSelector, ChiSqSelectorModel}\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    "  .setNumTopFeatures(2000)\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(\"category_index\")\n",
    "  .setOutputCol(\"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1e633-8635-4bae-962e-d894a3d77aaf",
   "metadata": {},
   "source": [
    "Lastly, we create the pipeline to execute all the transformers and select the top K features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37fc437e-d8f4-48e7-ba40-c685356e7230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_1040765eebac\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover, countVectorizer, idf, indexer, selector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694186d-c794-405c-a19a-6c222d4bb77b",
   "metadata": {
    "tags": []
   },
   "source": [
    "After creation of the pipeline, we can now fit it to our data, we want to transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4126e42f-ce5a-494f-88c7-10849cad7c72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 46.24801969528198 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_bbd84c8d218d\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588eed2",
   "metadata": {},
   "source": [
    "Afterward, we can extract the vocabulary and selected features to map them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52f4c09e-6668-47b6-9984-eec6719af35f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vocabulary: Array[String] = Array(great, good, love, time, work, recommend, back, easy, make, bought, made, find, buy, price, put, reading, quality, people, works, quot, years, nice, characters, long, series, lot, found, author, day, bit, feel, makes, thing, perfect, fit, end, set, loved, things, thought, music, small, hard, give, year, world, size, worth, pretty, times, sound, written, light, real, big, amazon, part, bad, highly, money, excellent, purchased, happy, high, enjoyed, problem, family, interesting, wanted, character, job, review, purchase, man, watch, days, enjoy, place, home, stars, short, writing, play, cover, top, fan, full, fine, color, side, order, wonderful, amazing, point, fact, reviews, ordered, stories, favorite, easily, needed, battery, screen, water, dvd, beautifu...\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vocabulary = model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary\n",
    "val selectedFeatures = model.stages.last.asInstanceOf[ChiSqSelectorModel].selectedFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f908c-5e7b-4a83-9294-c659bf2cd35e",
   "metadata": {},
   "source": [
    "Last but not least, we need to transform our data and display the format, which could be used going forward to the text classification task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d27ba7c-3c36-4dff-b9ac-cfb5e22bad1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rescaledData: org.apache.spark.sql.DataFrame = [category: string, selectedFeatures: vector]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rescaledData = model.transform(df).select(\"category\", \"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeada3f2",
   "metadata": {},
   "source": [
    "We can now use this transformed data to extract the top 2000 features overall and group them by category and select the top 75 of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0151dbf-ead3-45eb-9073-e2c78bee88cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.7742371559143066 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.SparseVector\n",
       "import org.apache.spark.sql.expressions.Window\n",
       "import org.apache.spark.sql.types._\n",
       "sparseVectorToMap: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4915/1919411326@37b7350d,MapType(IntegerType,DoubleType,false),List(Some(class[value[0]: vector])),Some(class[value[0]: map<int,double>]),None,true,true)\n",
       "indexToToken: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$4920/1716803550@509cae9c,StringType,List(Some(class[value[0]: int])),Some(class[value[0]: string]),None,true,true)\n",
       "topK: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, topk: array<array<string>>]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import org.apache.spark.ml.linalg.{SparseVector}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "// Define UDFs\n",
    "val sparseVectorToMap = udf((v: SparseVector) => v.indices.zip(v.values).toMap)\n",
    "val indexToToken = udf((i: Int) => vocabulary(selectedFeatures(i)))\n",
    "\n",
    "val topK = rescaledData\n",
    "    .select($\"category\", explode(sparseVectorToMap($\"selectedFeatures\")))\n",
    "    .select($\"category\", $\"key\".as(\"term\"), $\"value\".as(\"chi_squared\"))\n",
    "    .groupBy(\"category\", \"term\")\n",
    "    .agg(mean(\"chi_squared\").as(\"chi_squared\"))\n",
    "    .withColumn(\"term\", indexToToken(col(\"term\")))\n",
    "    .orderBy(desc(\"chi_squared\"), asc(\"term\"))\n",
    "    .withColumn(\"token_chisquared\", array(col(\"term\"), col(\"chi_squared\")))\n",
    "    .groupBy(\"category\")\n",
    "    .agg(slice(collect_list(\"token_chisquared\"), 1, K).as(\"topk\"))\n",
    "    .sort(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7cd7aa3-cb67-4d4d-8b01-4cdc2b2b9ecd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 12.263301610946655 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "writeDFToFile(topK, \"../output_ds.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bcf444-c1a7-4f67-9e8d-d2c408feb2e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "328d693b-4527-4d5e-8561-7dd0e545733c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seed: Int = 12041500\n",
       "percentage_of_dataset: Int = 1\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 12041500\n",
    "val percentage_of_dataset = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a8402-a82f-4dc6-8309-3984266e59bf",
   "metadata": {},
   "source": [
    "First, lets create two pipelines. One to perfom pre-processing and one to train the classifier. The reason for this is that we don't want to perform all pre-processing steps for each of the runs in the grid-search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690f63f-0fab-4c3b-ac53-15440a078e23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Pre-processing\n",
    "First, we pre process the data by performing the whole tokenization and tdidf-calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1bf5041-867e-4020-be32-026426b1b359",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipeline_preprocessing: org.apache.spark.ml.Pipeline = pipeline_fc877683f3e5\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline_preprocessing = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover, countVectorizer, idf, indexer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674a355-60f2-4a3b-9332-7b0bc91c0382",
   "metadata": {},
   "source": [
    "Lastly, we sample and therefore reduce the size of the data to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58821c61-cb72-4699-8ced-1fa388625904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampled_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampled_df = df.sample(withReplacement = false, fraction = percentage_of_dataset, seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9304590-9e3a-4da6-ba48-ac2a821d58f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = sampled_df.randomSplit(Array(0.8, 0.2), seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91a42256-5f8d-4177-90f6-1adb8389b79a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocessing: org.apache.spark.ml.PipelineModel = pipeline_fc877683f3e5\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val preprocessing = pipeline_preprocessing.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "476481e8-a582-4daa-910f-f98674360204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocessed_trainining: org.apache.spark.sql.DataFrame = [features: vector, category: string ... 1 more field]\n",
       "preprocessed_test: org.apache.spark.sql.DataFrame = [features: vector, category: string ... 1 more field]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val preprocessed_trainining = preprocessing.transform(training).select(idf.getOutputCol, \"category\", indexer.getOutputCol)\n",
    "val preprocessed_test = preprocessing.transform(test).select(idf.getOutputCol, \"category\", indexer.getOutputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46bcb0a5-2f8c-4d11-9ddd-29d1ea1f6df8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessed_trainining.write.mode(\"overwrite\").parquet(\"training_data.parquet\")\n",
    "preprocessed_test.write.mode(\"overwrite\").parquet(\"test_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd53c2b-608f-4857-81ed-de55df87a662",
   "metadata": {},
   "source": [
    "Reload them to have them easily accessible in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "656525b5-84f7-436f-909d-9b0b5dbbad14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainDF: org.apache.spark.sql.DataFrame = [features: vector, category: string ... 1 more field]\n",
       "testDF: org.apache.spark.sql.DataFrame = [features: vector, category: string ... 1 more field]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainDF = sc.read.parquet(\"training_data.parquet\")\n",
    "val testDF = sc.read.parquet(\"training_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9630fe-bcc9-47eb-9708-5076bf80de65",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b1617-3f36-4aad-a863-c46e9617a604",
   "metadata": {},
   "source": [
    "Additionally, we create a Normalizer for our selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c60b170-4264-428e-bdcd-69c69f779acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Normalizer\n",
       "normalizer: org.apache.spark.ml.feature.Normalizer = Normalizer: uid=normalizer_f7479c2eda16, p=2.0\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val normalizer = new Normalizer()\n",
    "  .setInputCol(selector.getOutputCol)\n",
    "  .setOutputCol(\"normFeatures\")\n",
    "  .setP(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e315b-f927-4598-9c98-55426a80ef8a",
   "metadata": {},
   "source": [
    "Then, we create the classifier. In our case we use a Linear Vector Machine. However, because we deal with a multiclass problem, we wrap it in a OneVsRest-classifier to bypass the limitation of Linear Vector Machines, which can only work with binary problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c73d1434-b36d-4f93-bc07-b4fbcb87de55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
       "lsvc: org.apache.spark.ml.classification.LinearSVC = linearsvc_44ca9d66f970\n",
       "classifier: org.apache.spark.ml.classification.OneVsRest = oneVsRest_ada2f6f879d9\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
    "\n",
    "val lsvc = new LinearSVC()\n",
    "\n",
    "val classifier = new OneVsRest()\n",
    "    .setClassifier(lsvc)\n",
    "    .setFeaturesCol(normalizer.getOutputCol)\n",
    "    .setLabelCol(indexer.getOutputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c8080b8-7091-47dd-ae8d-aaa675005731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline_classifier: org.apache.spark.ml.Pipeline = pipeline_abd62de89b22\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline_classifier = new Pipeline()\n",
    "    .setStages(Array(selector, normalizer, classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc6b93-324e-43c7-a371-1cb59f2584b4",
   "metadata": {},
   "source": [
    "Now, we only need to create our MulticlassClassificationEvaluator and ParamGridBuilder, which we use for hyperparameter tuning and evaluation of our model. We evaluate our model based on the F1-Score and our hyperparameter tuning happens based on the following params:\n",
    "- Compare chi square overall top 2000 filtered features with another, heavier filtering with much less dimensionality\n",
    "- Compare different SVM settings by \n",
    "    - varying the regularization parameter (choose 3 different values), \n",
    "    - standardization of training features (2 values),\n",
    "    - and maximum number of iterations (2 values).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f710ce43-370d-4142-a020-2523d9aebb1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinearsvc_44ca9d66f970-maxIter: 10,\n",
       "\tchiSqSelector_93618e654f32-numTopFeatures: 20,\n",
       "\tlinearsvc_44ca9d66f970-regParam: 0.001,\n",
       "\tlinearsvc_44ca9d66f970-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_44ca9d66f970-maxIter: 10,\n",
       "\tchiSqSelector_93618e654f32-numTopFeatures: 20,\n",
       "\tlinearsvc_44ca9d66f970-regParam: 0.001,\n",
       "\tlinearsvc_44ca9d66f970-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_44ca9d66f970-maxIter: 50,\n",
       "\tchiSqSelector_93618e654f32-numTopFeatures: 20,\n",
       "\tlinearsvc_44ca9d66f970-regParam: 0.001,\n",
       "\tlinearsvc_44ca9d66f970-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_44ca9d66f970-maxIter: 50,\n",
       "\tchiSqSelector_93618e654f32-numTopFeatures: 20,\n",
       "\tlinearsvc_44ca9d66f970-regParam: 0.001,\n",
       "\tlinearsvc_44ca9d66f970-...\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(lsvc.maxIter, Array(10, 50))\n",
    "    .addGrid(lsvc.regParam, Array(0.001, 0.01, 0.1))\n",
    "    .addGrid(lsvc.standardization, Array(false, true))\n",
    "    .addGrid(selector.numTopFeatures, Array(20, 2000))\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44646dc3-786f-4df5-b10d-04a444acadd4",
   "metadata": {},
   "source": [
    "Now, we simply perform the grid-search on a train-validation split and evaluate the best hyperparams on our previously created evaluater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13cb0660-a422-4783-8539-262aa714abb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "evaluater: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_b83a1950d040, metricName=f1, metricLabel=0.0, beta=1.0, eps=1.0E-15\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n",
    "\n",
    "val evaluater = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(indexer.getOutputCol)\n",
    "    .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4363132-cf81-44b2-951d-3853e21069c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_6c0eb8532277\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "    .setEstimator(pipeline_classifier)\n",
    "    .setEvaluator(evaluater)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setTrainRatio(0.8)\n",
    "    .setSeed(seed)\n",
    "    .setParallelism(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fcf9f-bc4c-4c05-8e7e-a672c3c02572",
   "metadata": {},
   "source": [
    "Lastly, we fit the model, with the best hyperparameters to the data and perform predictions with it. Afterward, we evaluate the model based on the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45094b4f-75e8-4bda-98be-58a565f8b97b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1180.5153167247772 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.tuning.TrainValidationSplitModel = TrainValidationSplitModel: uid=tvs_6c0eb8532277, bestModel=pipeline_482b12d36bfa, trainRatio=0.8\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val model = trainValidationSplit.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb7f69f4-d7cf-48b3-8259-43b603b39dff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, category: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "626391fa-fedc-4365-9789-04a59b3d904a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score = 0.6685228303780189\n"
     ]
    }
   ],
   "source": [
    "println(s\"F1-Score = ${evaluater.evaluate(predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30fc946a-e68d-4470-a24d-977051b7aa68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best binary classifier parameters:\n",
      "\tmaxIter: 10\n",
      "\tregParam: 0.001\n",
      "\tstandardization: true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{OneVsRestModel, LinearSVCModel}\n",
       "bestModel: org.apache.spark.ml.PipelineModel = pipeline_482b12d36bfa\n",
       "bestClassifier: org.apache.spark.ml.classification.OneVsRestModel = OneVsRestModel: uid=oneVsRest_bb12bdfb41f3, classifier=linearsvc_cad71bb5a8e8, numClasses=22, numFeatures=2000\n",
       "bestBinaryClassifierModel: org.apache.spark.ml.classification.LinearSVCModel = LinearSVCModel: uid=linearsvc_cad71bb5a8e8, numClasses=2, numFeatures=2000\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{OneVsRestModel, LinearSVCModel}\n",
    "\n",
    "val bestModel = model.bestModel.asInstanceOf[PipelineModel]\n",
    "val bestClassifier = bestModel.stages.last.asInstanceOf[OneVsRestModel]\n",
    "val bestBinaryClassifierModel = bestClassifier.models.head.asInstanceOf[LinearSVCModel]\n",
    "\n",
    "println(s\"Best binary classifier parameters:\\n\" +\n",
    "  s\"\\tmaxIter: ${bestBinaryClassifierModel.getMaxIter}\\n\" +\n",
    "  s\"\\tregParam: ${bestBinaryClassifierModel.getRegParam}\\n\" +\n",
    "  s\"\\tstandardization: ${bestBinaryClassifierModel.getStandardization}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe163373-38ab-4060-a9d1-f9686f17eeda",
   "metadata": {},
   "source": [
    "### Try-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "774bd9b6-a7a2-4dc4-94a2-73ff5da924c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Normalizer\n",
       "normalizer: org.apache.spark.ml.feature.Normalizer = Normalizer: uid=normalizer_143d6f976863, p=2.0\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val normalizer = new Normalizer()\n",
    "  .setInputCol(selector.getOutputCol)\n",
    "  .setOutputCol(\"normFeatures\")\n",
    "  .setP(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "82270c09-7b74-47aa-a461-515a48644d37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
       "lsvc: org.apache.spark.ml.classification.LinearSVC = linearsvc_02ce1607396d\n",
       "classifier: org.apache.spark.ml.classification.OneVsRest = oneVsRest_d857996ac75b\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
    "\n",
    "val lsvc = new LinearSVC()\n",
    "val classifier = new OneVsRest()\n",
    "    .setClassifier(lsvc)\n",
    "    .setFeaturesCol(normalizer.getOutputCol)\n",
    "    .setLabelCol(indexer.getOutputCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c1834b6-19fc-4168-ad90-d84343a1b07a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.ml.feature.{ChiSqSelector, ChiSqSelectorModel}\n",
       "pre_processing_classifier: (df: org.apache.spark.sql.DataFrame, numFeatures: Integer)Unit\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.ml.feature.{ChiSqSelector, ChiSqSelectorModel}\n",
    "\n",
    "def pre_processing_classifier(df: DataFrame, numFeatures: Integer) = {\n",
    "    val selector = new ChiSqSelector()\n",
    "      .setNumTopFeatures(numFeatures)\n",
    "      .setFeaturesCol(idf.getOutputCol)\n",
    "      .setLabelCol(\"category_index\")\n",
    "      .setOutputCol(\"selectedFeatures\")\n",
    "    \n",
    "    val pipeline_preprocessing = new Pipeline()\n",
    "        .setStages(Array(tokenizer, remover, countVectorizer, idf, indexer, selector, normalizer))\n",
    "\n",
    "    val Array(training, test) = df.randomSplit(Array(0.8, 0.2), seed = seed)\n",
    "    val preprocessing = pipeline_preprocessing.fit(training)\n",
    "\n",
    "    val preprocessed_trainining = preprocessing.transform(training).select(normalizer.getOutputCol, \"category\", indexer.getOutputCol)\n",
    "    val preprocessed_test = preprocessing.transform(test).select(normalizer.getOutputCol, \"category\", indexer.getOutputCol)\n",
    "\n",
    "    preprocessed_trainining.write.mode(\"overwrite\").parquet(\"training_data.parquet\")\n",
    "    preprocessed_test.write.mode(\"overwrite\").parquet(\"test_data.parquet\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dee089f5-440a-4de9-8780-1257ea0cccb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinearsvc_02ce1607396d-maxIter: 10,\n",
       "\tlinearsvc_02ce1607396d-regParam: 0.001,\n",
       "\tlinearsvc_02ce1607396d-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_02ce1607396d-maxIter: 10,\n",
       "\tlinearsvc_02ce1607396d-regParam: 0.01,\n",
       "\tlinearsvc_02ce1607396d-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_02ce1607396d-maxIter: 10,\n",
       "\tlinearsvc_02ce1607396d-regParam: 0.1,\n",
       "\tlinearsvc_02ce1607396d-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_02ce1607396d-maxIter: 10,\n",
       "\tlinearsvc_02ce1607396d-regParam: 0.001,\n",
       "\tlinearsvc_02ce1607396d-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_02ce1607396d-maxIter: 10,\n",
       "\tlinearsvc_02ce1607396d-regParam: 0.01,\n",
       "\tlinearsvc_02ce1607396d-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_02ce1607396d-maxIter: 10,\n",
       "\tl...\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(lsvc.maxIter, Array(10, 50))\n",
    "    .addGrid(lsvc.regParam, Array(0.001, 0.01, 0.1))\n",
    "    .addGrid(lsvc.standardization, Array(false, true))\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8ca302d2-234d-4e9e-aba1-825a2b754cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "evaluater: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_a5c41df99b6f, metricName=f1, metricLabel=0.0, beta=1.0, eps=1.0E-15\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n",
    "\n",
    "val evaluater = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(indexer.getOutputCol)\n",
    "    .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "223ec48b-2a60-469c-af15-3d88b306e02a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid-search for 20-topFeatures selected by ChiSquared\n",
      "F1-Score = 0.1663626942224923\n",
      "Starting grid-search for 2000-topFeatures selected by ChiSquared\n",
      "F1-Score = 0.6665776668536328\n",
      "Time: 832.5181884765625 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.classification.{OneVsRestModel, LinearSVCModel}\n",
       "numTopFeatures: Array[Int] = Array(20, 2000)\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n",
    "import org.apache.spark.ml.classification.{OneVsRestModel, LinearSVCModel}\n",
    "\n",
    "val numTopFeatures = Array(20, 2000)\n",
    "for (n <- numTopFeatures) {\n",
    "    println(s\"Starting grid-search for ${n}-topFeatures selected by ChiSquared\")\n",
    "    pre_processing_classifier(df, n)\n",
    "    val trainDF = sc.read.parquet(\"training_data.parquet\")\n",
    "    val testDF = sc.read.parquet(\"training_data.parquet\")\n",
    "    \n",
    "    val trainValidationSplit = new TrainValidationSplit()\n",
    "        .setEstimator(classifier)\n",
    "        .setEvaluator(evaluater)\n",
    "        .setEstimatorParamMaps(paramGrid)\n",
    "        .setTrainRatio(0.8)\n",
    "        .setSeed(seed)\n",
    "        .setParallelism(20)\n",
    "    \n",
    "    val model = trainValidationSplit.fit(trainDF)\n",
    "    val predictions = model.transform(testDF)\n",
    "    println(s\"F1-Score = ${evaluater.evaluate(predictions)}\")\n",
    "\n",
    "    //val bestModel = model.bestModel.asInstanceOf[PipelineModel]\n",
    "    //val bestClassifier = bestModel.stages.last.asInstanceOf[OneVsRestModel]\n",
    "    //val bestBinaryClassifierModel = bestClassifier.models.head.asInstanceOf[LinearSVCModel]\n",
    "\n",
    "    //println(s\"Best binary classifier parameters:\\n\" +\n",
    "    //  s\"\\tmaxIter: ${bestBinaryClassifierModel.getMaxIter}\\n\" +\n",
    "    //  s\"\\tregParam: ${bestBinaryClassifierModel.getRegParam}\\n\" +\n",
    "    //  s\"\\tstandardization: ${bestBinaryClassifierModel.getStandardization}\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136ee45c-2b79-4d40-91fc-f02691994905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
