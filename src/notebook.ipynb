{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Text Processing and Classification using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "24/05/26 16:45:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4051. Attempting port 4052.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4052. Attempting port 4053.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4053. Attempting port 4054.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4054. Attempting port 4055.\n",
      "24/05/26 16:46:00 WARN Utils: Service 'SparkUI' could not bind on port 4055. Attempting port 4056.\n",
      "24/05/26 16:46:02 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import MapType, IntegerType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF, ChiSqSelector, Normalizer, StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from multiprocessing import cpu_count\n",
    "from operator import add\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "spark: SparkSession = SparkSession.builder \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.memory\", \"7g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"7g\") \\\n",
    "    .config(\"spark.executor.instances\", 5) \\\n",
    "    .config(\"spark.executor.cores\", 4) \\\n",
    "    .config(\"spark.default.parallelism\", cpu_count()) \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/stopwords.txt') as f:\n",
    "    stopwords = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 75\n",
    "FILE_PATH = 'hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json' # devset\n",
    "# FILE_PATH = 'hdfs:///user/dic24_shared/amazon-reviews/full/reviewscombined.json' # full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1) RDDs\n",
    "\n",
    "Repeat the steps of Assignment 1, i.e. calculation of chi-square values and output of the sorted top terms per category, as well as the joined dictionary, using RDDs and transformations. Write the output to a file `output_rdd.txt`. Compare the generated `output_rdd.txt` with your generated `output.txt` from Assignment 1 and describe your observations briefly in the submission report (see Part 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd = spark.read.json(FILE_PATH).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 0 ns, total: 6 µs\n",
      "Wall time: 10.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def preprocessing(row):\n",
    "    category = row['category']\n",
    "\n",
    "    # lower text\n",
    "    # tokenises each line by using whitespaces, tabs, digits, and the characters ()[]{}.!?,;:+=-_\"'`~#@&*%€$§\\/ as delimiters\n",
    "    tokens = re.split(r'[^a-zA-Z<>^|]+', row['reviewText'].lower())\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = filter(lambda token: len(token) > 1 and (token not in stopwords), tokens)\n",
    "\n",
    "    # remove duplicates\n",
    "    tokens = set(tokens)\n",
    "\n",
    "    # count all documents in category\n",
    "    yield (category, None), 1\n",
    "\n",
    "    # count all documents in category containing token\n",
    "    for token in tokens:\n",
    "        yield (category, token), 1\n",
    "\n",
    "\n",
    "def token_to_key(row):\n",
    "    (category, token), count = row\n",
    "    return token, (category, count)\n",
    "\n",
    "\n",
    "def token_sum(row):\n",
    "    token, values = row\n",
    "    counts = {category: count for category, count in values}\n",
    "    n_t = sum(counts.values())\n",
    "\n",
    "    for category, count in counts.items():\n",
    "        yield category, (token, count, n_t)\n",
    "\n",
    "\n",
    "def chi_squared(row):\n",
    "    category, values = row\n",
    "\n",
    "    # dictionary of tokens with their counts and total number of documents\n",
    "    counts = {token: (count, n_t) for token, count, n_t in values}\n",
    "    \n",
    "    # total number of documents in category and dataset\n",
    "    n_c, n = counts.pop(None)\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for token, (a, n_t) in counts.items():\n",
    "        b = n_t - a\n",
    "        c = n_c - a\n",
    "        d = n - a - b - c\n",
    "\n",
    "        chi_squared = n * ((a * d - b * c) ** 2) / ((a + b) * (a + c) * (b + d) * (c + d))\n",
    "        result.append((chi_squared, token))\n",
    "\n",
    "    return category, sorted(result, key=lambda x: (-x[0], x[1]))[:K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.3 ms, sys: 11.9 ms, total: 60.1 ms\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topk = rdd.flatMap(preprocessing) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .map(token_to_key) \\\n",
    "    .groupByKey() \\\n",
    "    .flatMap(token_sum) \\\n",
    "    .groupByKey() \\\n",
    "    .map(chi_squared) \\\n",
    "    .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 ms, sys: 2.09 ms, total: 16.4 ms\n",
      "Wall time: 854 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('../output_rdd.txt', 'w') as f:\n",
    "    tokens = set()\n",
    "\n",
    "    for category, values in topk.toLocalIterator():\n",
    "        tokens.update(map(lambda x: x[1], values))\n",
    "        value_strings = [f'{value[1]}:{value[0]}' for value in values]\n",
    "        print(' '.join([f'<{category}>'] + value_strings), file=f)\n",
    "\n",
    "    print(' '.join(sorted(tokens)), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2) Datasets/DataFrames: Spark ML and Pipelines\n",
    "\n",
    "Convert the review texts to a classic vector space representation with TFIDF-weighted features based on the Spark DataFrame/Dataset API by building a transformation [pipeline](https://spark.apache.org/docs/latest/ml-pipeline.html). The primary goal of this part is the preparation of the pipeline for Part 3 (see below). Note: although parts of this pipeline will be very similar to Assignment 1 or Part 1 above, do not expect to obtain identical results or have access to all intermediate outputs to compare the individual steps.\n",
    "\n",
    "Use built-in functions for [tokenization](https://spark.apache.org/docs/latest/ml-features.html#tokenizer) to unigrams at whitespaces, tabs, digits, and the delimiter characters ()\\[\\]{}.!?,;:+=-\\_\"'\\`~#@&\\*%€$§\\\\/, casefolding, [stopword removal](https://spark.apache.org/docs/latest/ml-features.html#stopwordsremover), [TF-IDF calculation](https://spark.apache.org/docs/latest/ml-features.html#tf-idf), and [chi square selection](https://spark.apache.org/docs/latest/ml-features.html#chisqselector) ) (using 2000 top terms overall). Write the terms selected this way to a file `output_ds.txt` and compare them with the terms selected in Assignment 1. Describe your observations briefly in the submission report (see Part 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol='reviewText', outputCol='rawTokens', pattern=r'[^a-zA-Z<>^|]+', toLowercase=True, minTokenLength=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordRemover = StopWordsRemover(inputCol='rawTokens', outputCol='tokens', stopWords=list(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "countVectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='category', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    stopwordRemover,\n",
    "    # hashingTF,\n",
    "    countVectorizer,\n",
    "    idf,\n",
    "    indexer,\n",
    "    selector,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 14:58:58 WARN DAGScheduler: Broadcasting large task binary with size 1059.7 KiB\n",
      "24/05/25 14:59:09 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/25 14:59:10 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "24/05/25 14:59:18 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = model.stages[2].vocabulary\n",
    "selectedFeatures = model.stages[-1].selectedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared = model.transform(df).select('category', 'selectedFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_vector_to_map = F.udf(lambda v: {i: float(v[i]) for i in v.indices.tolist()}, returnType=MapType(IntegerType(), DoubleType()))\n",
    "index_to_token = F.udf(lambda i: vocabulary[selectedFeatures[i]], returnType='string')\n",
    "\n",
    "topk = chi_squared.select('category', F.explode(sparse_vector_to_map('selectedFeatures')).alias('token', 'chi_squared'))\n",
    "topk = topk.groupBy('category', 'token').agg(F.mean('chi_squared').alias('chi_squared'))\n",
    "topk = topk.withColumn('token', index_to_token('token'))\n",
    "topk = topk.withColumn('rank', F.row_number().over(Window.partitionBy('category').orderBy(F.desc('chi_squared'), F.asc('token'))))\n",
    "topk = topk.filter(F.col('rank') <= K)\n",
    "topk = topk.withColumn('token_chisq', F.array('token', 'chi_squared'))\n",
    "topk = topk.groupBy('category').agg(F.collect_list('token_chisq').alias('topk'))\n",
    "topk = topk.sort('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 14:59:47 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "24/05/25 14:59:58 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/05/25 14:59:59 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/05/25 15:00:00 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n",
      "24/05/25 15:00:00 WARN DAGScheduler: Broadcasting large task binary with size 3.6 MiB\n"
     ]
    }
   ],
   "source": [
    "with open('../output_ds.txt', 'w') as f:\n",
    "    tokens = set()\n",
    "\n",
    "    for row in topk.toLocalIterator():\n",
    "        tokens.update(map(lambda x: x[0], row['topk']))\n",
    "        value_strings = [f'{value[0]}:{value[1]}' for value in row['topk']]\n",
    "        print(' '.join([f'<{row[\"category\"]}>'] + value_strings), file=f)\n",
    "\n",
    "    print(' '.join(sorted(tokens)), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3) Text Classification\n",
    "\n",
    "In this part, you will train a text classifier from the features extracted in Part 2. The goal is to learn a model that can predict the product category from a review's text.\n",
    "\n",
    "To this end, extend the pipeline from Part 2 such that a **Support Vector Machine** classifier is trained. Since we are dealing with multi-class problems, make sure to put a strategy in place that allows binary classifiers to be applicable. Apply vector length normalization before feeding the feature vectors into the classifier (use [`Normalizer`](https://spark.apache.org/docs/latest/mllib-feature-extraction.html#normalizer) with L2 norm).\n",
    "\n",
    "Follow best practices for machine learning experiment design and investigate the effects of parameter settings using the functions provided by Spark:\n",
    "\n",
    "*   Split the review data into training, validation, and test set.\n",
    "    \n",
    "*   Make experiments reproducible.\n",
    "    \n",
    "*   Use a grid search for parameter optimization:\n",
    "    \n",
    "    *   Compare chi square overall top 2000 filtered features with another, heavier filtering with much less dimensionality (see Spark ML documentation for options).\n",
    "        \n",
    "    *   Compare different SVM settings by varying the regularization parameter (choose 3 different values), standardization of training features (2 values), and maximum number of iterations (2 values).\n",
    "        \n",
    "*   Use the [`MulticlassClassificationEvaluator`](https://spark.apache.org/docs/3.5.1/ml-tuning.html#model-selection-aka-hyperparameter-tuning) to estimate performance of your trained classifiers on the test set, using F1 measure as criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Gridsearch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    stopwordRemover,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenization_pipeline.fit(df).transform(df).select('category', 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tokenized.randomSplit([0.8, 0.2], seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train.write.mode('overwrite').parquet('train.parquet')\n",
    "test.write.mode('overwrite').parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train = spark.read.parquet('train.parquet')\n",
    "test = spark.read.parquet('test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol='selectedFeatures', outputCol='normFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearSVC()\n",
    "ovrClassifier = OneVsRest(classifier=classifier, featuresCol='normFeatures', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    indexer,\n",
    "    countVectorizer,\n",
    "    idf,\n",
    "    selector,\n",
    "    normalizer,\n",
    "    ovrClassifier,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(selector.numTopFeatures, [20, 2000])\n",
    "grid = grid.addGrid(classifier.regParam, [0, 0.01, 0.1])\n",
    "grid = grid.addGrid(classifier.standardization, [True, False])\n",
    "grid = grid.addGrid(classifier.maxIter, [10, 100])\n",
    "grid = grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                           estimatorParamMaps=grid,\n",
    "                           evaluator=MulticlassClassificationEvaluator(),\n",
    "                           trainRatio=0.8,\n",
    "                           seed=42,\n",
    "                           parallelism=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = model.bestModel\n",
    "numTopFeatures = bestModel.stages[-3].getNumTopFeatures()\n",
    "regParam = bestModel.stages[-1].getRegParam()\n",
    "standardization = bestModel.stages[-1].getStandardization()\n",
    "maxIter = bestModel.stages[-1].getMaxIter()\n",
    "{'numTopFeatures': numTopFeatures, 'regParam': regParam, 'standardization': standardization, 'maxIter': maxIter}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Gridsearch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persist_read(df, path):\n",
    "    df.write.mode('overwrite').save(path)\n",
    "    return spark.read.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train, validation, test = df.randomSplit([0.6, 0.2, 0.2], seed=SEED)\n",
    "train = persist_read(train, 'train.parquet')\n",
    "validation = persist_read(validation, 'validation.parquet')\n",
    "test = persist_read(test, 'test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol='selectedFeatures', outputCol='normFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    stopwordRemover,\n",
    "    countVectorizer,\n",
    "    idf,\n",
    "    indexer,\n",
    "    selector,\n",
    "    normalizer,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_grid = ParamGridBuilder()\n",
    "pp_grid = pp_grid.addGrid(selector.numTopFeatures, [20, 2000])\n",
    "pp_grid = pp_grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 16:02:26 WARN DAGScheduler: Broadcasting large task binary with size 1992.4 KiB\n",
      "24/05/26 16:02:26 WARN DAGScheduler: Broadcasting large task binary with size 1994.6 KiB\n",
      "24/05/26 16:02:28 WARN DAGScheduler: Broadcasting large task binary with size 1997.6 KiB\n",
      "24/05/26 16:02:42 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/05/26 16:02:44 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/05/26 16:02:54 WARN DAGScheduler: Broadcasting large task binary with size 1992.4 KiB\n",
      "24/05/26 16:02:54 WARN DAGScheduler: Broadcasting large task binary with size 1994.6 KiB\n",
      "24/05/26 16:02:56 WARN DAGScheduler: Broadcasting large task binary with size 1997.5 KiB\n",
      "24/05/26 16:03:05 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "24/05/26 16:03:07 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preprocessed = []\n",
    "columns = ['category', 'label', 'normFeatures']\n",
    "\n",
    "for i, preprocessing_params in enumerate(pp_grid):\n",
    "    pp_model = pp_pipeline.fit(train)\n",
    "    pp_train = pp_model.transform(train)\n",
    "    pp_validation = pp_model.transform(validation)\n",
    "    pp_train = persist_read(pp_train.select(columns), f'pp_train_{i}.parquet')\n",
    "    pp_validation = persist_read(pp_validation.select(columns), f'pp_validation_{i}.parquet')\n",
    "    \n",
    "    preprocessed.append((preprocessing_params, pp_train, pp_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LinearSVC()\n",
    "ovrClassifier = OneVsRest(classifier=classifier, featuresCol='normFeatures', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = Pipeline(stages=[\n",
    "    ovrClassifier,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid = ParamGridBuilder()\n",
    "model_grid = model_grid.addGrid(classifier.regParam, [0, 0.01, 0.1])\n",
    "model_grid = model_grid.addGrid(classifier.standardization, [True, False])\n",
    "model_grid = model_grid.addGrid(classifier.maxIter, [10, 100])\n",
    "model_grid = model_grid.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "param_grid = itertools.product(preprocessed, model_grid)\n",
    "\n",
    "def train_model(pp, model_params):\n",
    "    pp_params, pp_train, pp_validation = pp\n",
    "    model = model_pipeline.fit(pp_train, model_params)\n",
    "    prediction = model.transform(pp_validation)\n",
    "    f1 = evaluator.evaluate(prediction)\n",
    "    current_params = pp_params | model_params\n",
    "\n",
    "    print(f'F1: {f1}, Params: {current_params}')\n",
    "    \n",
    "    return f1, current_params\n",
    "\n",
    "with ThreadPool() as pool:\n",
    "    results = pool.starmap(train_model, param_grid)\n",
    "    \n",
    "f1, best_params = max(results, key=lambda x: x[0])\n",
    "f1, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = Pipeline(stages=pp_pipeline.getStages() + model_pipeline.getStages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = final_pipeline.fit(train.union(validation), best_params)\n",
    "prediction = model.transform(test)\n",
    "f1 = evaluator.evaluate(prediction)\n",
    "f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
