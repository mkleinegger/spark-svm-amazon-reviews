{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Text Processing and Classification using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import MapType, IntegerType, DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, HashingTF, IDF, ChiSqSelector\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from operator import add\n",
    "import re\n",
    "\n",
    "spark: SparkSession = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/stopwords.txt') as f:\n",
    "    stopwords = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 75\n",
    "FILE_PATH = 'hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json' # devset\n",
    "# FILE_PATH = 'hdfs:///user/dic24_shared/amazon-reviews/full/reviewscombined.json' # full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1) RDDs\n",
    "\n",
    "Repeat the steps of Assignment 1, i.e. calculation of chi-square values and output of the sorted top terms per category, as well as the joined dictionary, using RDDs and transformations. Write the output to a file `output_rdd.txt`. Compare the generated `output_rdd.txt` with your generated `output.txt` from Assignment 1 and describe your observations briefly in the submission report (see Part 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.11 ms, sys: 2.1 ms, total: 4.2 ms\n",
      "Wall time: 788 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd = spark.read.json(FILE_PATH).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 µs, sys: 0 ns, total: 23 µs\n",
      "Wall time: 26.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def preprocessing(row):\n",
    "    category = row['category']\n",
    "\n",
    "    # lower text\n",
    "    # tokenises each line by using whitespaces, tabs, digits, and the characters ()[]{}.!?,;:+=-_\"'`~#@&*%€$§\\/ as delimiters\n",
    "    tokens = re.split(r'[^a-zA-Z<>^|]+', row['reviewText'].lower())\n",
    "\n",
    "    # remove stopwords\n",
    "    tokens = filter(lambda token: len(token) > 1 and (token not in stopwords), tokens)\n",
    "\n",
    "    # remove duplicates\n",
    "    tokens = set(tokens)\n",
    "\n",
    "    # count all documents in category\n",
    "    yield (category, None), 1\n",
    "\n",
    "    # count all documents in category containing token\n",
    "    for token in tokens:\n",
    "        yield (category, token), 1\n",
    "\n",
    "\n",
    "def token_to_key(row):\n",
    "    (category, token), count = row\n",
    "    return token, (category, count)\n",
    "\n",
    "\n",
    "def token_sum(row):\n",
    "    token, values = row\n",
    "    counts = {category: count for category, count in values}\n",
    "    n_t = sum(counts.values())\n",
    "\n",
    "    for category, count in counts.items():\n",
    "        yield category, (token, count, n_t)\n",
    "\n",
    "\n",
    "def chi_squared(row):\n",
    "    category, values = row\n",
    "\n",
    "    # dictionary of tokens with their counts and total number of documents\n",
    "    counts = {token: (count, n_t) for token, count, n_t in values}\n",
    "    \n",
    "    # total number of documents in category and dataset\n",
    "    n_c, n = counts.pop(None)\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for token, (a, n_t) in counts.items():\n",
    "        b = n_t - a\n",
    "        c = n_c - a\n",
    "        d = n - a - b - c\n",
    "\n",
    "        chi_squared = n * ((a * d - b * c) ** 2) / ((a + b) * (a + c) * (b + d) * (c + d))\n",
    "        result.append((chi_squared, token))\n",
    "\n",
    "    return category, sorted(result, key=lambda x: (-x[0], x[1]))[:K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.1 ms, sys: 13.8 ms, total: 46.9 ms\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "topk = rdd.flatMap(preprocessing) \\\n",
    "    .reduceByKey(add) \\\n",
    "    .map(token_to_key) \\\n",
    "    .groupByKey() \\\n",
    "    .flatMap(token_sum) \\\n",
    "    .groupByKey() \\\n",
    "    .map(chi_squared) \\\n",
    "    .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 1.62 ms, total: 12.8 ms\n",
      "Wall time: 740 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('../output_rdd.txt', 'w') as f:\n",
    "    tokens = set()\n",
    "\n",
    "    for category, values in topk.toLocalIterator():\n",
    "        tokens.update(map(lambda x: x[1], values))\n",
    "        value_strings = [f'{value[1]}:{value[0]}' for value in values]\n",
    "        print(' '.join([f'<{category}>'] + value_strings), file=f)\n",
    "\n",
    "    print(' '.join(sorted(tokens)), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2) Datasets/DataFrames: Spark ML and Pipelines\n",
    "\n",
    "Convert the review texts to a classic vector space representation with TFIDF-weighted features based on the Spark DataFrame/Dataset API by building a transformation [pipeline](https://spark.apache.org/docs/latest/ml-pipeline.html). The primary goal of this part is the preparation of the pipeline for Part 3 (see below). Note: although parts of this pipeline will be very similar to Assignment 1 or Part 1 above, do not expect to obtain identical results or have access to all intermediate outputs to compare the individual steps.\n",
    "\n",
    "Use built-in functions for [tokenization](https://spark.apache.org/docs/latest/ml-features.html#tokenizer) to unigrams at whitespaces, tabs, digits, and the delimiter characters ()\\[\\]{}.!?,;:+=-\\_\"'\\`~#@&\\*%€$§\\\\/, casefolding, [stopword removal](https://spark.apache.org/docs/latest/ml-features.html#stopwordsremover), [TF-IDF calculation](https://spark.apache.org/docs/latest/ml-features.html#tf-idf), and [chi square selection](https://spark.apache.org/docs/latest/ml-features.html#chisqselector) ) (using 2000 top terms overall). Write the terms selected this way to a file `output_ds.txt` and compare them with the terms selected in Assignment 1. Describe your observations briefly in the submission report (see Part 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol='reviewText', outputCol='rawTokens', pattern=r'[^a-zA-Z<>^|]+', toLowercase=True, minTokenLength=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordRemover = StopWordsRemover(inputCol='rawTokens', outputCol='tokens', stopWords=list(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "countVectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[\n",
    "    tokenizer,\n",
    "    stopwordRemover,\n",
    "    # hashingTF,\n",
    "    countVectorizer,\n",
    "    idf,\n",
    "    selector,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = model.stages[2].vocabulary\n",
    "selectedFeatures = model.stages[-1].selectedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared = model.transform(df).select('category', 'overall', 'selectedFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_vector_to_map = F.udf(lambda v: {i: float(v[i]) for i in v.indices.tolist()}, returnType=MapType(IntegerType(), DoubleType()))\n",
    "index_to_token = F.udf(lambda i: vocabulary[selectedFeatures[i]], returnType='string')\n",
    "\n",
    "topk = chi_squared.select('category', F.explode(sparse_vector_to_map('selectedFeatures')).alias('token', 'chi_squared'))\n",
    "topk = topk.groupBy('category', 'token').agg(F.mean('chi_squared').alias('chi_squared'))\n",
    "topk = topk.withColumn('token', index_to_token('token'))\n",
    "topk = topk.withColumn('rank', F.row_number().over(Window.partitionBy('category').orderBy(F.desc('chi_squared'), F.asc('token'))))\n",
    "topk = topk.filter(F.col('rank') <= K)\n",
    "topk = topk.withColumn('token_chisq', F.array('token', 'chi_squared'))\n",
    "topk = topk.groupBy('category').agg(F.collect_list('token_chisq').alias('topk'))\n",
    "topk = topk.sort('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output_ds.txt', 'w') as f:\n",
    "    tokens = set()\n",
    "\n",
    "    for row in topk.toLocalIterator():\n",
    "        tokens.update(map(lambda x: x[0], row['topk']))\n",
    "        value_strings = [f'{value[0]}:{value[1]}' for value in row['topk']]\n",
    "        print(' '.join([f'<{row[\"category\"]}>'] + value_strings), file=f)\n",
    "\n",
    "    print(' '.join(sorted(tokens)), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3) Text Classification\n",
    "\n",
    "In this part, you will train a text classifier from the features extracted in Part 2. The goal is to learn a model that can predict the product category from a review's text.\n",
    "\n",
    "To this end, extend the pipeline from Part 2 such that a **Support Vector Machine** classifier is trained. Since we are dealing with multi-class problems, make sure to put a strategy in place that allows binary classifiers to be applicable. Apply vector length normalization before feeding the feature vectors into the classifier (use [`Normalizer`](https://spark.apache.org/docs/latest/mllib-feature-extraction.html#normalizer) with L2 norm).\n",
    "\n",
    "Follow best practices for machine learning experiment design and investigate the effects of parameter settings using the functions provided by Spark:\n",
    "\n",
    "*   Split the review data into training, validation, and test set.\n",
    "    \n",
    "*   Make experiments reproducible.\n",
    "    \n",
    "*   Use a grid search for parameter optimization:\n",
    "    \n",
    "    *   Compare chi square overall top 2000 filtered features with another, heavier filtering with much less dimensionality (see Spark ML documentation for options).\n",
    "        \n",
    "    *   Compare different SVM settings by varying the regularization parameter (choose 3 different values), standardization of training features (2 values), and maximum number of iterations (2 values).\n",
    "        \n",
    "*   Use the [`MulticlassClassificationEvaluator`](https://spark.apache.org/docs/3.5.1/ml-tuning.html#model-selection-aka-hyperparameter-tuning) to estimate performance of your trained classifiers on the test set, using F1 measure as criterion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
