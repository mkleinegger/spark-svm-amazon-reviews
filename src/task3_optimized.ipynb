{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d10e58-393d-43b7-a5d7-97d4b356df17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1715326141961_1260\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1715326141961_1260)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.ml.feature.{StringIndexer, HashingTF, IDF, RegexTokenizer, StopWordsRemover, ChiSqSelector}\n",
       "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.Normalizer\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.util.DefaultParamsWritable\n",
       "import scala.io.Source.fromFile\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.feature.{StringIndexer,HashingTF,IDF,RegexTokenizer,StopWordsRemover,ChiSqSelector}\n",
    "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.Normalizer\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.util.DefaultParamsWritable\n",
    "import scala.io.Source.fromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d80ce1-34c0-4df8-86a7-44d2d9a2999f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "sc: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@62d710fd\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val sc = SparkSession.builder\n",
    ".appName(\"SVM Text Classification\")\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d8952-ca36-43e3-96cc-432ce0133182",
   "metadata": {},
   "source": [
    "## Set global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7a8ab46-31b7-44d1-98cf-47f628928a24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_stopwords: String = Exercise_2/data/stopwords.txt\n",
       "k: Int = 2000\n",
       "seed: Int = 42\n",
       "split_pattern: String = [^a-zA-Z<>^|]+\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// for execution using jupyter hub\n",
    "// val path_to_stopwords = \"../data/stopwords.txt\"\n",
    "\n",
    "// for execution us vs code\n",
    "val path_to_stopwords = \"Exercise_2/data/stopwords.txt\"\n",
    "\n",
    "val k = 2000\n",
    "val seed = 42\n",
    "val split_pattern = \"[^a-zA-Z<>^|]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c035ff-3849-4a57-ac67-549a000660e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b5cd5c-93c2-4f36-96aa-6b15aeaf7fe8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category: string (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "reviewsDF: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n",
       "stopwords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs, but, by, c, came, ca...\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsDF = sc\n",
    ".read.json(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\")\n",
    ".select(\"category\",\"reviewText\")\n",
    "\n",
    "val stopwords = fromFile(path_to_stopwords).getLines.toArray\n",
    "\n",
    "reviewsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39441aeb-ee41-4900-98c2-ed98632a6728",
   "metadata": {},
   "source": [
    "## Define preprocessing and feauture extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5f45bdc-dcac-4e43-8a3d-ad68a1130357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_707d0511f5e6, minTokenLength=1, gaps=true, pattern=[^a-zA-Z<>^|]+, toLowercase=true\n",
       "stopWordsFile: String = Exercise_2/data/stopwords.txt\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = StopWordsRemover: uid=stopWords_336d47a691b5, numStopWords=596, locale=en_US, caseSensitive=false\n",
       "hashingTF: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_21815b2b1ebc, binary=false, numFeatures=262144\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_6d536948173b\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_691cadd487ba\n",
       "selector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_0a9bb5fdd735\n",
       "normalizer: org.apache.spark.ml.feature.Normalizer = Normalizer: uid=normalizer_d667e6b...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// tokenize\n",
    "val tokenizer = new RegexTokenizer()\n",
    ".setInputCol(\"reviewText\")\n",
    ".setOutputCol(\"words\")\n",
    ".setPattern(split_pattern)\n",
    "\n",
    "// remove stopwords\n",
    "val stopWordsFile = path_to_stopwords\n",
    "val remover = new StopWordsRemover()\n",
    ".setInputCol(tokenizer.getOutputCol)\n",
    ".setOutputCol(\"filtered\")\n",
    ".setStopWords(stopwords)\n",
    "\n",
    "// turn words into numerical features\n",
    "val hashingTF = new HashingTF()\n",
    ".setInputCol(remover.getOutputCol)\n",
    ".setOutputCol(\"rawFeatures\")\n",
    "\n",
    "// scale feature\n",
    "val idf = new IDF()\n",
    ".setInputCol(hashingTF.getOutputCol)\n",
    ".setOutputCol(\"features\")\n",
    "\n",
    "// encode category to numerical label\n",
    "val indexer = new StringIndexer()\n",
    ".setInputCol(\"category\")\n",
    ".setOutputCol(\"label\")\n",
    "\n",
    "// ChiSqSelector\n",
    "val selector = new ChiSqSelector()\n",
    ".setNumTopFeatures(k)\n",
    ".setFeaturesCol(idf.getOutputCol)\n",
    ".setLabelCol(indexer.getOutputCol)\n",
    ".setOutputCol(\"selected_features\")\n",
    "\n",
    "// Normalize ChiSqSelector\n",
    "val normalizer = new Normalizer()\n",
    "  .setInputCol(selector.getOutputCol)\n",
    "  .setOutputCol(\"normalized_features\")\n",
    "  .setP(2.0) // L2 normalization\n",
    "\n",
    "val pipeline_feature_extraction = new Pipeline().setStages(Array(tokenizer, remover, hashingTF, idf, indexer, selector, normalizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24fc17-130b-434a-b9c8-b59430936790",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15572478-5d7e-4e58-8909-cfeeacd09b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = reviewsDF\n",
    ".sample(withReplacement = false, fraction = 0.01, seed = seed)\n",
    ".randomSplit(Array(0.8,0.2), seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b46495-466c-442d-bab2-b6bd8165e438",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Long = 705\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f43302-d0ea-49ba-9885-686942cb0e38",
   "metadata": {},
   "source": [
    "## Apply pipeline to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4077c0fe-33e6-49a4-8d5d-9cb4eb568b06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 43.52297329902649 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "preprocessed_train: org.apache.spark.sql.DataFrame = [label: double, normalized_features: vector]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "val preprocessed_train = pipeline_feature_extraction\n",
    ".fit(train)\n",
    ".transform(train)\n",
    ".select(\"label\", \"normalized_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66fca589-c7d5-48eb-824e-151d307e22a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28.62164330482483 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "preprocessed_test: org.apache.spark.sql.DataFrame = [label: double, normalized_features: vector]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "val preprocessed_test = pipeline_feature_extraction\n",
    ".fit(test)\n",
    ".transform(test)\n",
    ".select(\"label\", \"normalized_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e0ad34-f565-498a-83f6-99e8f52ceb2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label| normalized_features|\n",
      "+-----+--------------------+\n",
      "|  7.0|(2000,[68,1224,14...|\n",
      "|  7.0|(2000,[9,597,915,...|\n",
      "|  7.0|(2000,[301,317,58...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfa208b8-d01a-44f0-941d-912317a7255f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X_train: org.apache.spark.sql.DataFrame = [normalized_features: vector]\n",
       "y_train: org.apache.spark.sql.DataFrame = [label: double]\n",
       "X_test: org.apache.spark.sql.DataFrame = [normalized_features: vector]\n",
       "y_test: org.apache.spark.sql.DataFrame = [label: double]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val X_train = preprocessed_train.select(\"normalized_features\")\n",
    "val y_train = preprocessed_train.select(\"label\")\n",
    "val X_test = preprocessed_test.select(\"normalized_features\")\n",
    "val y_test = preprocessed_test.select(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a4c211-0887-4c78-aac1-b2005b36b0a1",
   "metadata": {},
   "source": [
    "## Persist preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "848214b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path_to_store: String = Exercise_2/data/\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path_to_store = \"Exercise_2/data/\"\n",
    "\n",
    "preprocessed_train\n",
    ".write\n",
    ".mode(\"overwrite\")\n",
    ".parquet(path_to_store + \"training_data.parquet\")\n",
    "\n",
    "preprocessed_test\n",
    ".write\n",
    ".mode(\"overwrite\")\n",
    ".parquet(path_to_store + \"test_data.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d7af9c",
   "metadata": {},
   "source": [
    "## Load parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f56b8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainDF: org.apache.spark.sql.DataFrame = [label: double, normalized_features: vector]\n",
       "testDF: org.apache.spark.sql.DataFrame = [label: double, normalized_features: vector]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainDF = sc.read.parquet(path_to_store + \"training_data.parquet\")\n",
    "val testDF = sc.read.parquet(path_to_store + \"training_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec6d91c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label| normalized_features|\n",
      "+-----+--------------------+\n",
      "|  7.0|(2000,[68,1224,14...|\n",
      "|  7.0|(2000,[9,597,915,...|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c8f7e-86a8-43c3-b1d4-0856846a9ba1",
   "metadata": {},
   "source": [
    "## Define estimator and evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3933e66-ca30-4bdd-ab5a-606b1796fe32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svm: org.apache.spark.ml.classification.LinearSVC = linearsvc_42c595a0e3d6\n",
       "ovr_svm: org.apache.spark.ml.classification.OneVsRest = oneVsRest_0fc10cef1bd0\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val svm = new LinearSVC()\n",
    ".setFeaturesCol(\"normalized_features\") //normalizer.getOutputCol\n",
    ".setLabelCol(\"label\") // indexer.getOutputCol\n",
    "\n",
    "val ovr_svm = new OneVsRest()\n",
    ".setClassifier(svm)\n",
    ".setFeaturesCol(\"normalized_features\")\n",
    ".setLabelCol(\"label\")\n",
    "\n",
    "//val pipeline_train_model = new Pipeline()\n",
    "//.setStages(ovr_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e125f1-895c-40f5-945c-6134fb67d352",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Grid Search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad930591-c389-429d-8542-e0db9234ac57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator_f1: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_6cb6f23fb8f3, metricName=f1, metricLabel=0.0, beta=1.0, eps=1.0E-15\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinearsvc_42c595a0e3d6-maxIter: 8,\n",
       "\tchiSqSelector_0a9bb5fdd735-numTopFeatures: 377,\n",
       "\tlinearsvc_42c595a0e3d6-regParam: 0.05,\n",
       "\tlinearsvc_42c595a0e3d6-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_42c595a0e3d6-maxIter: 8,\n",
       "\tchiSqSelector_0a9bb5fdd735-numTopFeatures: 377,\n",
       "\tlinearsvc_42c595a0e3d6-regParam: 0.05,\n",
       "\tlinearsvc_42c595a0e3d6-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_42c595a0e3d6-maxIter: 8,\n",
       "\tchiSqSelector_0a9bb5fdd735-numTopFeatures: 377,\n",
       "\tlinearsvc_42c595a0e3d6-regParam: 0.13,\n",
       "\tlinearsvc_42c595a0e3d6-standardization: false\n",
       "}, {\n",
       "\tlinearsv...\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator_f1 = new MulticlassClassificationEvaluator()\n",
    ".setMetricName(\"f1\")\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    ".addGrid(svm.maxIter, Array(8,34))\n",
    ".addGrid(svm.regParam, Array(0.05, 0.13, 0.55))\n",
    ".addGrid(selector.numTopFeatures, Array(377, 2000))\n",
    ".addGrid(svm.standardization, Array(false, true))\n",
    ".build()\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    ".setEstimator(ovr_svm)\n",
    ".setEvaluator(evaluator_f1)\n",
    ".setEstimatorParamMaps(paramGrid)\n",
    ".setSeed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7318d33f-aea5-4924-af74-975bf07acca9",
   "metadata": {},
   "source": [
    "## Perform Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92791874-5a3d-45fe-a7c3-f186ecce4c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "val grid_search_model = trainValidationSplit.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cafd62-d7f3-4dd2-8466-e351f575427d",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c74a869-f78e-4056-a0ba-89b67065c099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val predictons = grid_search_model.transform(testDF)\n",
    "val f1_score = evaluator.evaluate(predictons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
