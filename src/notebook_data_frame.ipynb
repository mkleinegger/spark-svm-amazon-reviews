{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Text Processing and Classification using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "24/05/13 17:07:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/13 17:07:44 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark: SparkSession = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "K = 75\n",
    "FILE_PATH = 'hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json' # devset\n",
    "# FILE_PATH = 'hdfs:///user/dic24_shared/amazon-reviews/full/reviewscombined.json' # full dataset\n",
    "\n",
    "df = spark.read.json(FILE_PATH)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/stopwords.txt') as f:\n",
    "    stopwords = set(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_col = F.array([F.lit(word) for word in stopwords])\n",
    "\n",
    "tokens = F.lower(F.col('reviewText'))\n",
    "tokens = F.split(tokens, r'[^a-zA-Z<>^|]+')\n",
    "tokens = F.filter(tokens, lambda token: F.length(token) > 1)\n",
    "tokens = F.array_except(tokens, stopwords_col)\n",
    "tokens = F.array_distinct(tokens)\n",
    "tokens = F.explode(tokens)\n",
    "\n",
    "tokenized = df.withColumn('token', tokens)\n",
    "tokenized = tokenized[['asin', 'reviewerID', 'category', 'token']]\n",
    "# tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized[['asin', 'reviewerID']].dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = tokenized.withColumn('t_c_d', 1 / F.count(F.expr('*')).over(Window.partitionBy('category', 'asin', 'reviewerID')))\n",
    "counts = counts.withColumn('t_d', 1 / F.count(F.expr('*')).over(Window.partitionBy('asin', 'reviewerID')))\n",
    "# counts = counts.groupBy(['category', 'token']).agg(F.count(F.expr('*')).alias('n_c_t'), F.sum('t_c_d').alias('t_c_d'))\n",
    "counts = counts.groupBy(['category', 'token']).agg(F.count(F.expr('*')).alias('n_c_t'), F.sum('t_c_d').alias('t_c_d'), F.sum('t_d').alias('t_d'))\n",
    "counts = counts.withColumn('n_c', F.round(F.sum('t_c_d').over(Window.partitionBy('category'))).cast('integer'))\n",
    "counts = counts.withColumn('n_t', F.sum('n_c_t').over(Window.partitionBy('token')))\n",
    "# counts = counts.withColumn('n', F.lit(df.count())) # TODO: check if it is faster with global window\n",
    "counts = counts.withColumn('n', F.round(F.sum('t_c_d').over(Window.partitionBy(F.lit(0)))).cast('integer'))\n",
    "counts = counts[['category', 'token', 'n_c_t', 'n_c', 'n_t', 'n']]\n",
    "# counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq = counts.withColumn('a', F.col('n_c_t'))\n",
    "chisq = chisq.withColumn('b', F.col('n_c') - F.col('a'))\n",
    "chisq = chisq.withColumn('c', F.col('n_t') - F.col('a'))\n",
    "chisq = chisq.withColumn('d', F.col('n') - F.col('a') - F.col('b') - F.col('c'))\n",
    "chisq = chisq.withColumn('chi_squared', F.col('n') * ((F.col('a') * F.col('d') - F.col('b') * F.col('c')) ** 2) / ((F.col('a') + F.col('b')) * (F.col('c') + F.col('d')) * (F.col('a') + F.col('c')) * (F.col('b') + F.col('d'))))\n",
    "chisq = chisq[['category', 'token', 'chi_squared']]\n",
    "# chisq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = chisq.withColumn('rank', F.row_number().over(Window.partitionBy('category').orderBy(F.desc('chi_squared'), F.asc('token'))))\n",
    "topk = topk.filter(F.col('rank') <= K)\n",
    "topk = topk.withColumn('token_chisq', F.array('token', 'chi_squared'))\n",
    "topk = topk.groupBy('category').agg(F.collect_list('token_chisq').alias('topk'))\n",
    "topk = topk.sort('category')\n",
    "# topk.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../output_rdd.txt', 'w') as f:\n",
    "    tokens = set()\n",
    "\n",
    "    for row in topk.toLocalIterator():\n",
    "        tokens.update(map(lambda x: x[0], row['topk']))\n",
    "        value_strings = [f'{value[0]}:{value[1]}' for value in row['topk']]\n",
    "        print(' '.join([f'<{row[\"category\"]}>'] + value_strings), file=f)\n",
    "\n",
    "    print(' '.join(sorted(tokens)), file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
