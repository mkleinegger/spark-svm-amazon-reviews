{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f7703a-8aa3-47c9-8461-670f297d9fb1",
   "metadata": {},
   "source": [
    "# Amazon-reviews predictions with Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e6d449-425b-4222-967e-d082d7692b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1715326141961_0199\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1715326141961_0199)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "sc: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@53422aa9\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val sc = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73583dd-e37b-4611-8f50-484cb7ba8bed",
   "metadata": {},
   "source": [
    "## Read data and transform to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "17ededeb-ae72-4a27-83a6-f3a17f4b5a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = sc.read.json(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\").select(\"category\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45d7f730-f982-419a-8a60-b02a07f2f636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizePattern: String = [^a-zA-Z<>^|]+\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizePattern = \"[^a-zA-Z<>^|]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "28fa0346-2df6-4f93-8577-13adf01fe6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopWords: List[String] = List(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs, but, by, c, came, camera, can, cannot, cant, car, case, cause, causes, cd, certain, certainly, changes, clearly,...\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopWords = List(\n",
    "  \"a\", \"aa\", \"able\", \"about\", \"above\", \"absorbs\", \"accord\", \"according\", \"accordingly\", \"across\",\n",
    "  \"actually\", \"after\", \"afterwards\", \"again\", \"against\", \"ain\", \"album\", \"all\", \"allow\", \"allows\",\n",
    "  \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\",\n",
    "  \"and\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\",\n",
    "  \"apart\", \"app\", \"appear\", \"appreciate\", \"appropriate\", \"are\", \"aren\", \"around\", \"as\", \"aside\", \"ask\",\n",
    "  \"asking\", \"associated\", \"at\", \"available\", \"away\", \"awfully\", \"b\", \"baby\", \"bb\", \"be\", \"became\",\n",
    "  \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"believe\",\n",
    "  \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bibs\", \"bike\", \"book\", \"books\",\n",
    "  \"both\", \"brief\", \"bulbs\", \"but\", \"by\", \"c\", \"came\", \"camera\", \"can\", \"cannot\", \"cant\", \"car\", \"case\",\n",
    "  \"cause\", \"causes\", \"cd\", \"certain\", \"certainly\", \"changes\", \"clearly\", \"co\", \"coffee\", \"com\", \"come\",\n",
    "  \"comes\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\",\n",
    "  \"corresponding\", \"could\", \"couldn\", \"course\", \"currently\", \"d\", \"definitely\", \"described\", \"despite\", \"did\",\n",
    "  \"didn\", \"different\", \"do\", \"does\", \"doesn\", \"dog\", \"dogs\", \"doing\", \"doll\", \"don\", \"done\", \"down\", \"downwards\",\n",
    "  \"during\", \"e\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\", \"elsewhere\", \"enough\", \"entirely\", \"especially\",\n",
    "  \"et\", \"etc\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\",\n",
    "  \"example\", \"except\", \"f\", \"far\", \"few\", \"fifth\", \"film\", \"first\", \"five\", \"flavor\", \"followed\", \"following\",\n",
    "  \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"four\", \"from\", \"fun\", \"further\", \"furthermore\", \"g\", \"game\",\n",
    "  \"get\", \"gets\", \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"greetings\", \"grill\",\n",
    "  \"guitar\", \"h\", \"had\", \"hadn\", \"hair\", \"happens\", \"hardly\", \"has\", \"hasn\", \"have\", \"haven\", \"having\", \"he\", \"hello\",\n",
    "  \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"hi\", \"him\", \"himself\",\n",
    "  \"his\", \"hither\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"i\", \"ie\", \"if\", \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\",\n",
    "  \"indeed\", \"indicate\", \"indicated\", \"indicates\", \"ink\", \"inner\", \"insofar\", \"install\", \"instead\", \"into\", \"inward\", \"is\", \"isn\",\n",
    "  \"it\", \"its\", \"itself\", \"j\", \"just\", \"k\", \"keep\", \"keeps\", \"kept\", \"kitchen\", \"knife\", \"know\", \"known\", \"knows\", \"l\", \"lamp\",\n",
    "  \"laptop\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\", \"lest\", \"let\", \"life\", \"like\", \"liked\", \"likely\",\n",
    "  \"little\", \"ll\", \"look\", \"looking\", \"looks\", \"ltd\", \"m\", \"mainly\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"meanwhile\", \"merely\",\n",
    "  \"might\", \"mon\", \"more\", \"moreover\", \"most\", \"mostly\", \"movie\", \"mower\", \"much\", \"must\", \"my\", \"myself\", \"n\", \"name\", \"namely\",\n",
    "  \"nd\", \"near\", \"nearly\", \"necessary\", \"need\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"nine\", \"no\", \"nobody\",\n",
    "  \"non\", \"none\", \"noone\", \"nor\", \"normally\", \"not\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"o\", \"obviously\", \"of\", \"off\", \"often\",\n",
    "  \"oh\", \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\",\n",
    "  \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"own\", \"p\", \"particular\", \"particularly\", \"per\", \"perhaps\", \"phone\", \"placed\", \"please\",\n",
    "  \"plus\", \"possible\", \"presumably\", \"printer\", \"probably\", \"product\", \"provides\", \"q\", \"que\", \"quite\", \"qv\", \"r\", \"rather\", \"rd\", \"re\", \"read\",\n",
    "  \"really\", \"reasonably\", \"regarding\", \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\", \"s\", \"said\", \"same\", \"saw\", \"say\",\n",
    "  \"saying\", \"says\", \"second\", \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\",\n",
    "  \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"shave\", \"she\", \"shoes\", \"should\", \"shouldn\", \"since\", \"six\", \"skin\", \"so\", \"some\", \"somebody\",\n",
    "  \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"song\", \"songs\", \"soon\", \"sorry\", \"specified\", \"specify\",\n",
    "  \"specifying\", \"still\", \"story\", \"strings\", \"stroller\", \"sub\", \"such\", \"sup\", \"sure\", \"t\", \"take\", \"taken\", \"taste\", \"tell\", \"tends\", \"th\",\n",
    "  \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\",\n",
    "  \"thereby\", \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\",\n",
    "  \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"took\", \"toward\", \"towards\", \"toy\", \"tried\", \"tries\",\n",
    "  \"truck\", \"truly\", \"try\", \"trying\", \"twice\", \"two\", \"u\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlikely\", \"until\", \"unto\", \"up\", \"upon\",\n",
    "  \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\", \"usually\", \"v\", \"value\", \"various\", \"ve\", \"very\", \"via\", \"viz\", \"vs\", \"want\", \"wants\", \"was\",\n",
    "  \"wasn\", \"way\", \"we\", \"wear\", \"welcome\", \"well\", \"went\", \"were\", \"weren\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\",\n",
    "  \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\",\n",
    "  \"why\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"won\", \"wonder\", \"would\", \"wouldn\", \"x\", \"y\", \"yes\", \"yet\", \"you\", \"your\", \"yours\",\n",
    "  \"yourself\", \"yourselves\", \"z\", \"zero\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195030e1-bb58-4186-b616-54d4767b8f20",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2d04b92f-8975-4983-a0d1-78eaa9a3c915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io.PrintWriter\n",
       "import org.apache.spark.rdd.RDD\n",
       "writeRDDToFile: (rdd: org.apache.spark.rdd.RDD[String], mergedDict: String, filePath: String)Unit\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "def writeRDDToFile(rdd: RDD[String], mergedDict: String, filePath: String) = {\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    rdd.collect().foreach(line => writer.println(line))\n",
    "    writer.println(mergedDict)\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a17fb-63a4-46e0-9748-e37714fb5364",
   "metadata": {},
   "source": [
    "## Calculate Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f5376711-f0d9-45df-8ad7-08d8659f759f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[146] at reduceByKey at <console>:46\n",
       "N: Long = 78829\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val counts = df.rdd.map(row => (row.getString(0), 1)).reduceByKey((a, b) => a + b)\n",
    "val N = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e902d564-b646-40cd-a72d-851b252cbdf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countsAsMap: scala.collection.immutable.Map[String,Int] = Map(Patio_Lawn_and_Garde -> 994, Movies_and_TV -> 4607, Electronic -> 7825, Office_Product -> 1243, Tools_and_Home_Improvement -> 1926, Kindle_Store -> 3205, Home_and_Kitche -> 4254, Digital_Music -> 836, Automotive -> 1374, Grocery_and_Gourmet_Food -> 1297, Baby -> 916, Book -> 22507, Clothing_Shoes_and_Jewelry -> 5749, Toys_and_Game -> 2253, Health_and_Personal_Care -> 2982, Sports_and_Outdoor -> 3269, Beauty -> 2023, CDs_and_Vinyl -> 3749, Musical_Instrument -> 500, Cell_Phones_and_Accessorie -> 3447, Apps_for_Android -> 2638, Pet_Supplie -> 1235)\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val countsAsMap = counts.collect.toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fdcd6433-bdba-4bf6-b8ee-93fe5fe879b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 18.433804988861084 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chiSquaredTermCategory: org.apache.spark.rdd.RDD[(String, List[(String, Double)])] = ShuffledRDD[168] at sortByKey at <console>:68\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val chiSquaredTermCategory = df.rdd.map(row => (row.getString(0), row.getString(1).toLowerCase().split(tokenizePattern).distinct))\n",
    "                         .map(row => (row._1, row._2.filter(w => w.length > 1 && !stopWords.contains(w))))\n",
    "                         .flatMap(row => row._2.map(term => (row._1, term)))\n",
    "                         .map(row => (row, 1)).reduceByKey((a, b) => a + b)\n",
    "                         .map(row => (row._1._2, (row._1._1, row._2))).groupByKey()\n",
    "                         .map(row => (row._1, (row._2.map(x => x._2).sum, row._2)))\n",
    "                         .flatMap(row => row._2._2.map(category => ((category._1), (row._1, category._2, row._2._1))))\n",
    "                         .map(row => {    \n",
    "                            val A = row._2._2\n",
    "                            val B = row._2._3 - A\n",
    "                            val C = countsAsMap(row._1) - A\n",
    "                            val D = N - A - B - C\n",
    "    \n",
    "                            val chiSquared = (N * math.pow((A * D) - (B * C), 2)) / ((A + B) * (A + C) * (B + D) * (C + D))\n",
    "                            (row._1, (row._2._1, chiSquared))\n",
    "                        })\n",
    "                        .groupByKey()\n",
    "                        .map(row => (row._1, row._2.toList.sortBy(x => -x._2).take(75)))\n",
    "                        .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "255bb627-46e3-4ce6-9c3a-f7f540b24bce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mergedDict: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[172] at distinct at <console>:46\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mergedDict = chiSquaredTermCategory.flatMap(row => row._2.map(term => term._1)).distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1cdd7146-1b9b-4bdd-93ee-9647bd343d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[173] at map at <console>:46\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val result = chiSquaredTermCategory.map(row => {\n",
    "    val key = row._1\n",
    "    val values = row._2.map { case (str, num) => s\"$str:$num\" }.mkString(\" \")\n",
    "    s\"<$key> $values\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "59b95cda-0e73-4bc4-b7e3-c40376b9183f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/e12041500/output_rdd.txt': No such file or directory\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/e12041500/output_rdd.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5aef949c-8da0-439f-a96f-bbecce713511",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.8346476554870605 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result.coalesce(1).saveAsTextFile(\"output_rdd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "abed58fc-7e32-4393-bb2a-c4cd009376db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.41611814498901367 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "writeRDDToFile(result, mergedDict.sortBy(x => x).reduce((a, b) => a + \" \" + b), \"./output_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ab42d-f2e5-4194-8d2f-5194f67a1b7a",
   "metadata": {},
   "source": [
    "## Datasets/DataFrames: Spark ML and Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f292e544-28b0-48f7-a444-c8c8915faeea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.RegexTokenizer\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_b13b5a1a2bbe, minTokenLength=1, gaps=false, pattern=[^a-zA-Z<>^|]+, toLowercase=true\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.RegexTokenizer\n",
    "\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setInputCol(\"reviewText\")\n",
    "  .setOutputCol(\"words\")\n",
    "  .setGaps(false)\n",
    "  .setPattern(tokenizePattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "008d4c6f-7867-47e0-a809-68b533e6336d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "32: error: overloaded method constructor StopWordsRemover with alternatives:",
     "output_type": "error",
     "traceback": [
      "<console>:32: error: overloaded method constructor StopWordsRemover with alternatives:",
      "  ()org.apache.spark.ml.feature.StopWordsRemover <and>",
      "  (uid: String)org.apache.spark.ml.feature.StopWordsRemover",
      " cannot be applied to (stopWords: List[String])",
      "       val remover = new StopWordsRemover(stopWords=stopWords)",
      "                     ^",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StopWordsRemover\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    " .setInputCol(\"words\")\n",
    " .setOutputCol(\"filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c328c249-d511-4f7f-8e00-c3a23ba9eaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "  .setNumFeatures(1000)\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"features\")\n",
    "val lr = new LogisticRegression()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.001)\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, hashingTF, lr))\n",
    "\n",
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(training)\n",
    "\n",
    "// Now we can optionally save the fitted pipeline to disk\n",
    "model.write.overwrite().save(\"/tmp/spark-logistic-regression-model\")\n",
    "\n",
    "// We can also save this unfit pipeline to disk\n",
    "pipeline.write.overwrite().save(\"/tmp/unfit-lr-model\")\n",
    "\n",
    "// And load it back in during production\n",
    "val sameModel = PipelineModel.load(\"/tmp/spark-logistic-regression-model\")\n",
    "\n",
    "// Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "val test = spark.createDataFrame(Seq(\n",
    "  (4L, \"spark i j k\"),\n",
    "  (5L, \"l m n\"),\n",
    "  (6L, \"spark hadoop spark\"),\n",
    "  (7L, \"apache hadoop\")\n",
    ")).toDF(\"id\", \"text\")\n",
    "\n",
    "// Make predictions on test documents.\n",
    "model.transform(test)\n",
    "  .select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "  .collect()\n",
    "  .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =>\n",
    "    println(s\"($id, $text) --> prob=$prob, prediction=$prediction\")\n",
    "  }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
