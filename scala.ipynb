{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f7703a-8aa3-47c9-8461-670f297d9fb1",
   "metadata": {},
   "source": [
    "# Amazon-reviews predictions with Spark ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06e6d449-425b-4222-967e-d082d7692b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "sc: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@1724680f\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val sc = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73583dd-e37b-4611-8f50-484cb7ba8bed",
   "metadata": {},
   "source": [
    "## Read data and transform to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61608eac-7522-4ac5-8f13-6ff8643f1507",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K: Int = 75\n",
       "file_path_stopwords: String = ./data/stopwords.txt\n",
       "file_path_reviews: String = hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\n",
       "tokenizePattern: String = [^a-zA-Z<>^|]+\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val K = 75\n",
    "val file_path_stopwords = \"./data/stopwords.txt\"\n",
    "val file_path_reviews = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "// val file_path_reviews = \"hdfs:///user/dic24_shared/amazon-reviews/full/reviewscombined.json\"\n",
    "\n",
    "val tokenizePattern = \"[^a-zA-Z<>^|]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17ededeb-ae72-4a27-83a6-f3a17f4b5a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 8.568068742752075 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val df = sc.read.json(file_path_reviews).select(\"category\", \"reviewText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28fa0346-2df6-4f93-8577-13adf01fe6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.io.Source.fromFile\n",
       "stopWords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs, but, by, c, came, camera, can, cannot, cant, car, case, cause, causes, ...\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source.fromFile\n",
    "\n",
    "val stopWords = fromFile(file_path_stopwords).getLines.toArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195030e1-bb58-4186-b616-54d4767b8f20",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d04b92f-8975-4983-a0d1-78eaa9a3c915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io.PrintWriter\n",
       "import org.apache.spark.rdd.RDD\n",
       "writeRDDToFile: (rdd: org.apache.spark.rdd.RDD[String], mergedDict: String, filePath: String)Unit\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.PrintWriter\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "def writeRDDToFile(rdd: RDD[String], mergedDict: String, filePath: String) = {\n",
    "    val writer = new PrintWriter(filePath)\n",
    "    rdd.collect().foreach(line => writer.println(line))\n",
    "    writer.println(mergedDict)\n",
    "    writer.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0a17fb-63a4-46e0-9748-e37714fb5364",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Calculate Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4d14419-d35e-4d0f-abd0-5c546e51d7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[8] at rdd at <console>:33\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f5376711-f0d9-45df-8ad7-08d8659f759f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 5.945173025131226 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "counts: scala.collection.Map[String,Long] = Map(Patio_Lawn_and_Garde -> 994, Movies_and_TV -> 4607, Electronic -> 7825, Office_Product -> 1243, Tools_and_Home_Improvement -> 1926, Kindle_Store -> 3205, Home_and_Kitche -> 4254, Digital_Music -> 836, Automotive -> 1374, Grocery_and_Gourmet_Food -> 1297, Baby -> 916, Book -> 22507, Clothing_Shoes_and_Jewelry -> 5749, Toys_and_Game -> 2253, Health_and_Personal_Care -> 2982, Sports_and_Outdoor -> 3269, Beauty -> 2023, CDs_and_Vinyl -> 3749, Musical_Instrument -> 500, Cell_Phones_and_Accessorie -> 3447, Apps_for_Android -> 2638, Pet_Supplie -> 1235)\n",
       "N: Long = 78829\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val counts = df.rdd.map(row => (row.getString(0), 1)).countByKey()\n",
    "val N = df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e902d564-b646-40cd-a72d-851b252cbdf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.3215475082397461 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "countsAsMap: scala.collection.immutable.Map[String,Long] = Map(Patio_Lawn_and_Garde -> 994, Movies_and_TV -> 4607, Electronic -> 7825, Office_Product -> 1243, Tools_and_Home_Improvement -> 1926, Kindle_Store -> 3205, Home_and_Kitche -> 4254, Digital_Music -> 836, Automotive -> 1374, Grocery_and_Gourmet_Food -> 1297, Baby -> 916, Book -> 22507, Clothing_Shoes_and_Jewelry -> 5749, Toys_and_Game -> 2253, Health_and_Personal_Care -> 2982, Sports_and_Outdoor -> 3269, Beauty -> 2023, CDs_and_Vinyl -> 3749, Musical_Instrument -> 500, Cell_Phones_and_Accessorie -> 3447, Apps_for_Android -> 2638, Pet_Supplie -> 1235)\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val countsAsMap = counts.toMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "93e4b94c-ce28-4aab-94cd-55be2193dbd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 30.290690183639526 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "preprocessedRDD: org.apache.spark.rdd.RDD[(String, Array[String])] = MapPartitionsRDD[20] at map at <console>:43\n",
       "termCategoryCounts: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[24] at map at <console>:49\n",
       "chiSquaredValues: org.apache.spark.rdd.RDD[(String, (String, Double))] = MapPartitionsRDD[26] at flatMapValues at <console>:53\n",
       "topTermsPerCategory: org.apache.spark.rdd.RDD[(String, List[(String, Double)])] = ShuffledRDD[32] at sortByKey at <console>:69\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val preprocessedRDD = rdd\n",
    "    .map(row => (row.getString(0), row.getString(1).toLowerCase().split(tokenizePattern).distinct))\n",
    "    .map(row => (row._1, row._2.filter(w => w.length > 1 && !stopWords.contains(w))))\n",
    "\n",
    "val termCategoryCounts = preprocessedRDD\n",
    "    .flatMapValues(terms => terms)\n",
    "    .map({ case (category, term) => ((category, term), 1) })\n",
    "    .reduceByKey(_ + _)\n",
    "    .map({ case ((category, term), count) => (term, (category, count)) })\n",
    "\n",
    "val chiSquaredValues = termCategoryCounts\n",
    "    .groupByKey()\n",
    "    .flatMapValues({ categoryCounts =>\n",
    "        val n_t = categoryCounts.map(row => row._2).sum\n",
    "        categoryCounts.map({ case (category, count) =>\n",
    "            val A = count\n",
    "            val B = n_t - A\n",
    "            val C = countsAsMap(category) - A\n",
    "            val D = N - A - B - C\n",
    "            val chiSquared = (N * math.pow((A * D) - (B * C), 2)) / ((A + B) * (A + C) * (B + D) * (C + D))\n",
    "            (category, chiSquared)\n",
    "        })\n",
    "      })\n",
    "   \n",
    "val topTermsPerCategory = chiSquaredValues\n",
    "    .map({ case (term, (category, chiSquared)) => (category, (term, chiSquared)) })\n",
    "    .groupByKey()\n",
    "    .mapValues(_.toList.sortBy(-_._2).take(K))\n",
    "    .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "255bb627-46e3-4ce6-9c3a-f7f540b24bce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.604891061782837 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mergedDict: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[36] at distinct at <console>:33\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val mergedDict = topTermsPerCategory.flatMap(row => row._2.map(term => term._1)).distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1cdd7146-1b9b-4bdd-93ee-9647bd343d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.2280044555664062 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[38] at map at <console>:33\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val result = topTermsPerCategory.map(row => {\n",
    "    val key = row._1\n",
    "    val values = row._2.map { case (str, num) => s\"$str:$num\" }.mkString(\" \")\n",
    "    s\"<$key> $values\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abed58fc-7e32-4393-bb2a-c4cd009376db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.9966464042663574 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "writeRDDToFile(result, mergedDict.sortBy(x => x).reduce(_ + \" \" + _), \"./output_rdd.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59b95cda-0e73-4bc4-b7e3-c40376b9183f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/e12041500/output_rdd.txt\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -rm -r hdfs://captain01.os.hpc.tuwien.ac.at:9000/user/e12041500/output_rdd.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5aef949c-8da0-439f-a96f-bbecce713511",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.3839399814605713 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result.coalesce(1).saveAsTextFile(\"output_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43300955-1cd7-4770-b6c9-e4f6f3263fdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Second approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "55802d0a-82a9-48c0-b863-b96eb1e212cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 39.51532602310181 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "filteredCategoryTerm: org.apache.spark.rdd.RDD[((String, String), Int)] = MapPartitionsRDD[83] at flatMap at <console>:45\n",
       "countTerms: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[85] at reduceByKey at <console>:49\n",
       "countCategoryTerm: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[86] at reduceByKey at <console>:52\n",
       "joinedCategoryTerm: org.apache.spark.rdd.RDD[(String, ((String, Int), Int))] = MapPartitionsRDD[90] at join at <console>:56\n",
       "chiSquaredTermCategory: org.apache.spark.rdd.RDD[(String, List[(String, Double)])] = ShuffledRDD[96] at sortByKey at <console>:70\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val filteredCategoryTerm = rdd\n",
    "    .map(row => (row.getString(0), row.getString(1).toLowerCase().split(tokenizePattern).distinct))\n",
    "    .map(row => (row._1, row._2.filter(w => w.length > 1 && !stopWords.contains(w))))\n",
    "    .flatMap(row => row._2.map(term => ((row._1, term), 1)))\n",
    "\n",
    "val countTerms = filteredCategoryTerm\n",
    "    .map(row => (row._1._2, 1))\n",
    "    .reduceByKey(_ + _)\n",
    "\n",
    "val countCategoryTerm = filteredCategoryTerm\n",
    "    .reduceByKey(_ + _)\n",
    "\n",
    "val joinedCategoryTerm = countCategoryTerm\n",
    "    .map(row => (row._1._2, (row._1._1, row._2)))\n",
    "    .join(countTerms)\n",
    "\n",
    "val chiSquaredTermCategory = joinedCategoryTerm\n",
    "    .map(row => {    \n",
    "        val A = row._2._1._2\n",
    "        val B = row._2._2 - A\n",
    "        val C = countsAsMap(row._2._1._1) - A\n",
    "        val D = N - A - B - C\n",
    "    \n",
    "        val chiSquared = (N * math.pow((A * D) - (B * C), 2)) / ((A + B) * (A + C) * (B + D) * (C + D))\n",
    "        (row._2._1._1, (row._1, chiSquared))\n",
    "    })\n",
    "    .groupByKey()\n",
    "    .map(row => (row._1, row._2.toList.sortBy(x => -x._2).take(K)))\n",
    "    .sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7d2cc8a-8125-42c0-abb9-b8abae492eff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.9137146472930908 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mergedDict: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[100] at distinct at <console>:33\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val mergedDict = chiSquaredTermCategory.flatMap(row => row._2.map(term => term._1)).distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d361bcf8-edd5-4189-bae8-87c77d778d08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.6810376644134521 seconds.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[101] at map at <console>:33\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "val result = chiSquaredTermCategory.map(row => {\n",
    "    val key = row._1\n",
    "    val values = row._2.map { case (str, num) => s\"$str:$num\" }.mkString(\" \")\n",
    "    s\"<$key> $values\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebb670bb-04cc-45b3-964c-209444a6a06a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.0665204524993896 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "writeRDDToFile(result, mergedDict.sortBy(x => x).reduce(_ + \" \" + _), \"./output2_rdd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ab42d-f2e5-4194-8d2f-5194f67a1b7a",
   "metadata": {},
   "source": [
    "## Datasets/DataFrames: Spark ML and Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88396d20-de4b-4903-bb18-b2c34c4a36ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K: Int = 2000\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val K = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac665195-33da-4940-ae61-2745c490eec3",
   "metadata": {},
   "source": [
    "Should the dataset first be grouped into category and all reviewTexts concatenated, to really get the top 2000 terms, or should we aggregate it afterwards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "48ae3185-f841-4cf2-9665-311311dfb4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "combinedDF: org.apache.spark.sql.DataFrame = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val combinedDF = df\n",
    "    .groupBy(\"category\")\n",
    "    .agg(concat_ws(\" \", collect_list(\"reviewText\")).alias(\"reviewText\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf1cfc-433a-4858-a1c1-58d54be9ec24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "combinedDF.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74ef30-43da-42a8-909f-801d904cdffe",
   "metadata": {},
   "source": [
    "First, create all the necessary transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4ecf763-118c-4101-82ba-ed8c66acade4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{StringIndexer, RegexTokenizer, StopWordsRemover}\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
       "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_675215802098\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = RegexTokenizer: uid=regexTok_b7a4e035ad6d, minTokenLength=1, gaps=false, pattern=[^a-zA-Z<>^|]+, toLowercase=true\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = StopWordsRemover: uid=stopWords_dc7274aa73af, numStopWords=596, locale=en_US, caseSensitive=false\n",
       "cvModel: org.apache.spark.ml.feature.CountVectorizer = cntVec_c75a7dbc9541\n",
       "hashingTF: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_67d39d7f75e2, binary=false, numFeatur...\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{StringIndexer, RegexTokenizer, StopWordsRemover}\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\n",
    "import org.apache.spark.ml.feature.{HashingTF, IDF}\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "    .setInputCol(\"category\")\n",
    "    .setOutputCol(\"category_index\")\n",
    "\n",
    "val tokenizer = new RegexTokenizer()\n",
    "    .setInputCol(\"reviewText\")\n",
    "    .setOutputCol(\"words\")\n",
    "    .setGaps(false)\n",
    "    .setPattern(tokenizePattern)\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "    .setInputCol(tokenizer.getOutputCol)\n",
    "    .setOutputCol(\"filtered\")\n",
    "    .setStopWords(stopWords)\n",
    "\n",
    "val cvModel = new CountVectorizer()\n",
    "    .setInputCol(remover.getOutputCol)\n",
    "    .setOutputCol(\"features\")\n",
    "    .setMinDF(1)\n",
    "\n",
    "val hashingTF = new HashingTF()\n",
    "    .setInputCol(remover.getOutputCol)\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "// Decide which frequencyCounter you want hashingTF vs cvModel?\n",
    "val idf = new IDF()\n",
    "    .setInputCol(hashingTF.getOutputCol)\n",
    "    .setOutputCol(\"weightedfeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35ba46-fd3a-45ba-b920-3cd4f6612b09",
   "metadata": {},
   "source": [
    "Afterward, we create the Chi^2-Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a4fdaab-8640-402d-b1f6-84d68a89dc10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.ChiSqSelector\n",
       "selector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_14fec18aabfa\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.ChiSqSelector\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    "  .setNumTopFeatures(K)\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(\"category_index\")\n",
    "  .setOutputCol(\"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1e633-8635-4bae-962e-d894a3d77aaf",
   "metadata": {},
   "source": [
    "Lastly, we create the pipeline to execute all the transformers and select the top K features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37fc437e-d8f4-48e7-ba40-c685356e7230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover, hashingTF, idf, indexer, selector))\n",
    "// todo: should we save the pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694186d-c794-405c-a19a-6c222d4bb77b",
   "metadata": {
    "tags": []
   },
   "source": [
    "After creation of the pipeline, we can now fit it to our data, we want to transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4126e42f-ce5a-494f-88c7-10849cad7c72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_ce35100f7e27\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = pipeline.fit(combinedDF)\n",
    "// todo: should we save the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f908c-5e7b-4a83-9294-c659bf2cd35e",
   "metadata": {},
   "source": [
    "Last but not least, we need to transform our data and display the format, which could be used going forward to the text classification task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d27ba7c-3c36-4dff-b9ac-cfb5e22bad1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rescaledData: org.apache.spark.sql.DataFrame = [category: string, selectedFeatures: vector]\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rescaledData = model.transform(combinedDF).select(\"category\", \"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "860efb42-29d4-48b5-9cd6-d06a7f599eee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            category|    selectedFeatures|\n",
      "+--------------------+--------------------+\n",
      "|Health_and_Person...|(2000,[4,6,10,12,...|\n",
      "|        Kindle_Store|(2000,[0,1,4,6,11...|\n",
      "|                Baby|(2000,[6,12,20,21...|\n",
      "|       Movies_and_TV|(2000,[1,2,4,6,7,...|\n",
      "|Clothing_Shoes_an...|(2000,[1,6,7,8,13...|\n",
      "|                Book|(2000,[0,1,2,3,4,...|\n",
      "|          Automotive|(2000,[6,7,16,19,...|\n",
      "|  Sports_and_Outdoor|(2000,[3,4,6,7,8,...|\n",
      "|    Apps_for_Android|(2000,[6,12,18,20...|\n",
      "|Tools_and_Home_Im...|(2000,[4,6,7,16,2...|\n",
      "|Cell_Phones_and_A...|(2000,[6,9,12,13,...|\n",
      "|       CDs_and_Vinyl|(2000,[1,2,4,5,6,...|\n",
      "|          Electronic|(2000,[0,1,5,6,7,...|\n",
      "|         Pet_Supplie|(2000,[6,20,34,38...|\n",
      "|Grocery_and_Gourm...|(2000,[4,6,12,20,...|\n",
      "|     Home_and_Kitche|(2000,[6,7,11,12,...|\n",
      "|       Toys_and_Game|(2000,[6,7,8,12,1...|\n",
      "|       Digital_Music|(2000,[1,12,18,20...|\n",
      "|  Musical_Instrument|(2000,[6,7,18,19,...|\n",
      "|Patio_Lawn_and_Garde|(2000,[6,12,18,20...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bcf444-c1a7-4f67-9e8d-d2c408feb2e3",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "266313ae-469c-4c4e-97bd-baa311c0ea5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seed: Int = 12041500\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 12041500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bf0e4-e657-4a8b-aeca-3e9482370d29",
   "metadata": {},
   "source": [
    "The next line of code is optional and can be used to down sample the dataframe to make model training faster, in case of high load on the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88b2a8e8-5d60-4d6b-97ec-fe302f325a28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampledDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampledDF = df.sample(withReplacement = false, fraction = 0.1, seed = 12041500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f02eb1-64e4-482f-8755-f27ce6e8186c",
   "metadata": {},
   "source": [
    "First, we split our data into training- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d145d3bc-45ca-451f-b9da-83c404a9bc9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category: string, reviewText: string]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = sampledDF.randomSplit(Array(0.8, 0.2), seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94727373-bffa-43c9-94cb-7510264f5949",
   "metadata": {},
   "source": [
    "Additionally, we create a Normalizer for our selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdf56e7d-c5b7-417b-9141-49520e3a2b8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Normalizer\n",
       "normalizer: org.apache.spark.ml.feature.Normalizer = Normalizer: uid=normalizer_0eef80a7078a, p=2.0\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val normalizer = new Normalizer()\n",
    "  .setInputCol(selector.getOutputCol)\n",
    "  .setOutputCol(\"normFeatures\")\n",
    "  .setP(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da936a-af29-4259-85e6-2746fbcc7e47",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then, we create the classifier. In our case we use a Linear Vector Machine. However, because we deal with a multiclass problem, we wrap it in a OneVsRest-classifier to bypass the limitation of Linear Vector Machines, which can only work with binary problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa503157-648c-4263-a8f9-cdd0b9eb4fd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
       "lsvc: org.apache.spark.ml.classification.LinearSVC = linearsvc_b3d835bac5ea\n",
       "classifier: org.apache.spark.ml.classification.OneVsRest = oneVsRest_96ad20cd751c\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
    "\n",
    "val lsvc = new LinearSVC()\n",
    "    // .setStandardization(false) // we use a normalizer ? Normalizer vs Standardization ?\n",
    "    .setFeaturesCol(normalizer.getOutputCol)\n",
    "    .setLabelCol(indexer.getOutputCol)\n",
    "\n",
    "val classifier = new OneVsRest()\n",
    "    .setClassifier(lsvc)\n",
    "    .setFeaturesCol(normalizer.getOutputCol)\n",
    "    .setLabelCol(indexer.getOutputCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957d8dbc-1ac1-4e43-aa75-adf462a74510",
   "metadata": {},
   "source": [
    "After, setting all things up, we create the pipeline, which uses all the prior created transformers and fits the transformed data to our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35d2f6db-c9f0-4212-b65f-32cc974bd99f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_eebf351ef7e0\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "    .setStages(Array(tokenizer, remover, hashingTF, idf, indexer, selector, normalizer, classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73a1348-9a02-4adc-8a53-058a5b475c29",
   "metadata": {},
   "source": [
    "Now, we only need to create our MulticlassClassificationEvaluator and ParamGridBuilder, which we use for hyperparameter tuning and evaluation of our model. We evaluate our model based on the F1-Score and our hyperparameter tuning happens based on the following params:\n",
    "- Compare chi square overall top 2000 filtered features with another, heavier filtering with much less dimensionality\n",
    "- Compare different SVM settings by \n",
    "    - varying the regularization parameter (choose 3 different values), \n",
    "    - standardization of training features (2 values),\n",
    "    - and maximum number of iterations (2 values).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b6330f5-734b-46a7-9537-ba9ccd192725",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "evaluater: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_5f9738fa46b4, metricName=weightedFMeasure, metricLabel=0.0, beta=1.0, eps=1.0E-15\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n",
    "\n",
    "val evaluater = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(indexer.getOutputCol)\n",
    "    .setMetricName(\"weightedFMeasure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae8539-ffc6-4d8d-be82-2d78649d9775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.ParamGridBuilder\n",
    "\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "    .addGrid(lsvc.maxIter, Array(10, 100))\n",
    "    .addGrid(lsvc.regParam, Array(0.01, 0.1, 0.5))\n",
    "    .addGrid(lsvc.standardization, Array(false, true))\n",
    "    .addGrid(selector.numTopFeatures, Array(200, 2000))\n",
    "    // .addGrid(normalizer.p, Array(1.0, 2.0)) ? instead of standardization\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb916f0-47b3-4b7f-ba42-cdef4f5d8f35",
   "metadata": {},
   "source": [
    "Now, we simply perform the grid-search on a train-validation split and evaluate the best hyperparams on our previously created evaluater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9621f352-b196-4e9c-a1ff-8a98dab38724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_47a148e1a6c8\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.TrainValidationSplit\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "    .setEstimator(pipeline)\n",
    "    .setEvaluator(evaluater)\n",
    "    .setEstimatorParamMaps(paramGrid)\n",
    "    .setTrainRatio(0.8)\n",
    "    .setSeed(seed)\n",
    "    .setParallelism(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a50112-73e8-4ce4-9b8c-a8d4854b74fe",
   "metadata": {},
   "source": [
    "Lastly, we fit the model, with the best hyperparameters to the data and perform predictions with it. Afterward, we evaluate the model based on the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b492a9-c8de-4fb7-bd36-39d7b1526420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val model = trainValidationSplit.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd607b-acf9-42fa-88bd-7e26984069c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9061ed4-6a74-4a93-afe5-d49809765612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val f1 = evaluater.evaluate(predictions)\n",
    "println(s\"F1-Score = ${f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf06e1d-95e8-4072-9597-ab693e0c5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "// todo: test, which one performs the best\n",
    "// model.params\n",
    "// model.explainParams()\n",
    "// model.extractParamMap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
